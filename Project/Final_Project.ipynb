{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db05b641-b477-4788-afc8-677dafd7b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Making sure it downloads models on my D drive, as no space in defualt file location\n",
    "os.environ['HF_HOME'] = 'D:\\\\Download\\\\UCSD\\\\cache'\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DebertaV2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from adapters import AdapterConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be59cec3-cf96-475a-bef8-489986d5d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Using Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9578774-1faf-4122-84bb-d47e6b8957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directories\n",
    "BASE_MODEL_DIR = \"./base_model\"\n",
    "LORA_MODEL_DIR = \"./lora_model\"\n",
    "ADAPTER_MODEL_DIR = \"./adapter_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6786e0aa-af95-4583-afe0-6aef67376e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 1B and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Using LLama 1B as base model\n",
    "\n",
    "# Couldn't train Llama because of lower mem GPUs so shifting to roberta\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ensure tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068915-0dc2-404e-b8a2-6cb8ff75c635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a59e493e-5941-4a6c-b968-6db0a2ba5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)  # Use full training dataset\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)    # Use full testing dataset\n",
    "\n",
    "# Veyr big dataset\n",
    "# Load a sentiment dataset (example: SST2)\n",
    "# ds = load_dataset(\"facebook/xnli\", \"all_languages\")\n",
    "# train_data = ds['train']\n",
    "# val_data = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99124809-bb4f-4c16-8f0b-2d3d2eab2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model, stage_name=\"Model\"):\n",
    "    print(f\"\\nTrainable Parameters in {stage_name}:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  - {name}: {param.numel()} params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738b4c-5eac-4474-b8a7-f1059757b2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e38754b-57ab-48ec-8ddf-58af4d37ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate periodically during training\n",
    "    #eval_steps=100,               # Frequency of evaluation (adjust as needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training for GPU\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    ")\n",
    "\n",
    "# Train base model\n",
    "trainer_base = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f194fb5-d8e7-434b-bf82-1eadc1809c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044c143-513d-48a6-b77c-27978ffacc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b655e5-9fc9-4cff-9684-2a3661d6bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_params(base_model, stage_name=\"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a387cc-974f-41cb-937d-4edf8387bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Base Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/4689 06:36 < 30:16:39, 0.04 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Resize model embeddings after adding new special tokens\u001b[39;00m\n\u001b[0;32m      3\u001b[0m base_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\accelerate\\accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "If \n",
    "print(\"\\nTraining Base Model...\")\n",
    "# Resize model embeddings after adding new special tokens\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a31c3a-d6d7-4d3f-93e6-3520a66719d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836e7b4-f721-4baa-bf44-848aba3b81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save base model\n",
    "tokenizer.save_pretrained(\"./base_model\")\n",
    "base_model.save_pretrained(\"./base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd97c7-ae67-47cc-8828-6f352f74a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "print(\"\\nEvaluating Base Model...\")\n",
    "base_results = trainer_base.evaluate()\n",
    "print(\"Base Model Results:\", base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181e522-f4c0-4926-bd38-6985d2a6b1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b1dd5-5fc1-4e67-b8ed-0fe3b999aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\", \n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "lora_model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "start_time = time.time()\n",
    "print(\"\\nTraining LoRA Model...\")\n",
    "trainer_lora.train()\n",
    "print(f\"LoRa trained in: {time.time() - start_time}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f322e-1953-4bd0-ade7-199f31a7190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA model\n",
    "tokenizer.save_pretrained(\"./lora_model\")\n",
    "lora_model.save_pretrained(\"./lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6226322-1a15-4026-abd0-f0573a802818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a95146-ca6f-45a2-8271-bb8a88a8c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LoRA model\n",
    "print(\"\\nEvaluating LoRA Model...\")\n",
    "lora_results = trainer_lora.evaluate()\n",
    "print(\"LoRA Model Results:\", lora_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d195363a-5a33-455a-aab0-1b30a06bffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaLayer, RobertaAttention, RobertaIntermediate, RobertaOutput\n",
    "\n",
    "# class CustomRobertaLayer(RobertaModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.attention = RobertaAttention(config)  # Multi-head attention\n",
    "#         self.intermediate = RobertaIntermediate(config)  # Feed-forward network\n",
    "#         self.output = RobertaOutput(config)  # Projection back\n",
    "#         self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)\n",
    "#         self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.up_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         hidden_states,\n",
    "#         attention_mask=None,\n",
    "#         head_mask=None,\n",
    "#         output_attentions=None,\n",
    "#         output_hidden_states=None, input_ids=None, token_type_ids=None, position_ids=None, \n",
    "#         inputs_embeds=None, labels=None, \n",
    "#     ):\n",
    "#         # 1. Self-attention\n",
    "#         attention_output = self.attention(\n",
    "#             hidden_states, attention_mask=attention_mask, head_mask=head_mask\n",
    "#         )\n",
    "        \n",
    "#         # 2. Downsample â†’ Activation â†’ Upsample\n",
    "#         downsampled = self.activation(self.down_layer(attention_output))\n",
    "#         upsampled = self.up_layer(downsampled)\n",
    "#         normalized = self.up_norm(upsampled + attention_output)  # Add residual connection\n",
    "\n",
    "#         # 3. Intermediate feed-forward network\n",
    "#         intermediate_output = self.intermediate(normalized)\n",
    "\n",
    "#         # 4. Final output projection and residual connection\n",
    "#         layer_output = self.output(intermediate_output, normalized)\n",
    "#         return layer_output\n",
    "\n",
    "# class CustomRobertaLayer(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.attention = RobertaAttention(config)\n",
    "#         self.intermediate = RobertaIntermediate(config)\n",
    "#         self.output = RobertaOutput(config)\n",
    "#         self.down_layer = nn.Linear(config.hidden_size, 512)\n",
    "#         self.up_layer = nn.Linear(512, config.hidden_size)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.up_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "#     def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "#         hidden_states = self.attention(hidden_states, attention_mask, head_mask)\n",
    "#         intermediate_output = self.intermediate(hidden_states)\n",
    "#         output = self.output(intermediate_output, hidden_states)\n",
    "#         output = self.down_layer(output)\n",
    "#         output = self.activation(output)\n",
    "#         output = self.up_layer(output)\n",
    "#         return self.up_norm(output)\n",
    "\n",
    "class CustomRobertaLayer(RobertaLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)  # Down-project\n",
    "        self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)  # Up-project\n",
    "        self.activation = nn.ReLU()  # You can use other activations like GELU\n",
    "        self.up_norm = nn.LayerNorm(config.hidden_size)  #Normalization layer \n",
    "\n",
    "    def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_value=None,\n",
    "    use_cache=False,\n",
    "    output_attentions=False,\n",
    "    **kwargs,\n",
    "    ):\n",
    "    # Ensure the attention mask is in the correct dtype\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # Match precision (e.g., float16)\n",
    "        \n",
    "        # Original attention operation\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "    \n",
    "        attention_output = attention_outputs[0]\n",
    "        print(f\"Attention Output Shape: {attention_output.shape}\")\n",
    "        \n",
    "        # Down-projection\n",
    "        down_projected = self.activation(self.down_layer(attention_output))\n",
    "        print(f\"Down Projected Shape: {down_projected.shape}\")\n",
    "        # Up-projection\n",
    "        up_projected = self.activation(self.up_layer(down_projected))\n",
    "        print(f\"Up Projected Shape: {up_projected.shape}\")\n",
    "        #normalization\n",
    "        normalized_output = self.up_norm(up_projected)\n",
    "        print(f\"Normalization Shape: {normalized_output.shape}\")\n",
    "        \n",
    "        # Add & Norm after FF layers\n",
    "        layer_output = self.output(hidden_states=normalized_output, input_tensor=attention_output)\n",
    "        return (layer_output,) + attention_outputs[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f593cc0d-7ef9-4ae2-b99d-ebf775b342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder, RobertaConfig, RobertaEmbeddings\n",
    "\n",
    "\n",
    "class CustomRobertaModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the encoder with the custom encoder\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = CustomRobertaEncoder(config)\n",
    "\n",
    "        # Add the classification head at the end\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, config.num_labels),\n",
    "        )\n",
    "\n",
    "        # Freeze existing layers and enable gradients only for the new layers\n",
    "        self.freeze_pretrained_layers()\n",
    "\n",
    "    def freeze_pretrained_layers(self):\n",
    "        # Freeze all layers except the new classifier and encoder's added layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"classifier\" in name or \"down_layer\" in name or \"up_layer\" in name or \"up_norm\" in name:\n",
    "                param.requires_grad = True  # Enable gradients for new layers\n",
    "            else:\n",
    "                param.requires_grad = False  # Freeze existing layers\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, \n",
    "        head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, labels=None, \n",
    "    ):\n",
    "\n",
    "        print(\"Calling super()\")\n",
    "        print(\"What are labels?: \", labels)\n",
    "        # Embeddings layer\n",
    "        embeddings_output = self.embeddings(input_ids, token_type_ids, position_ids)\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_outputs = self.encoder(embeddings_output, attention_mask)\n",
    "\n",
    "        # Extract [CLS] token for classification\n",
    "        cls_token_output = encoder_outputs[:, 0, :]\n",
    "        \n",
    "        # outputs = super().forward(\n",
    "        #     input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, \n",
    "        #     head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states\n",
    "        # )\n",
    "        # print(\"Super Done\")\n",
    "        # pooled_output = outputs[0][:, 0, :]        \n",
    "        print(f\"Encoder Output Shape: {cls_token_output.shape}\")\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        print(f\"Logits Shape: {logits.shape}\") \n",
    "\n",
    "        return (logits,) + outputs[2:]  # Return logits along with other outputs\n",
    "        \n",
    "class CustomRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([CustomRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff5136a-2737-40ef-9dba-b66f9c75d9f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the configuration\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacebookAI/roberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create the custom model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m CustomRobertaModel(config)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\configuration_utils.py:541\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    537\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[1;32m--> 541\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[0;32m    543\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\configuration_utils.py:570\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\utils\\hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: 'RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "# Load the configuration\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomRobertaModel(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_model = RobertaModel.from_pretrained(model_name, num_labels=2)\n",
    "custom_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "custom_model.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb534130-7ea0-457f-9545-2543091836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaModel,\n",
    "    RobertaEncoder,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaConfig,\n",
    ")\n",
    "\n",
    "# Custom Layer that adds Down-Up Projection and LayerNorm after FF layer\n",
    "class CustomRobertaLayer(RobertaLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)  # Down-projection\n",
    "        self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)    # Up-projection\n",
    "        self.activation = nn.GELU()                                              # Activation function\n",
    "        self.layer_norm_new = nn.LayerNorm(config.hidden_size)                       # LayerNorm after Up-projection\n",
    "        # intializing all as new layers\n",
    "        self.down_layer._is_new = True\n",
    "        self.up_layer._is_new = True\n",
    "        self.activation._is_new = True\n",
    "        self.layer_norm_new._is_new = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Ensure the attention mask matches the required dimensions\n",
    "        if attention_mask is not None:\n",
    "            # Expand dimensions for multi-head attention\n",
    "            attention_mask = attention_mask[:, None, None, :]  # Shape: [batch_size, 1, 1, seq_len]\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # Match precision (e.g., float16)\n",
    "\n",
    "\n",
    "        # Attention sub-layer\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output = attention_outputs[0]\n",
    "\n",
    "        # Feed-forward sub-layer\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(hidden_states=intermediate_output, input_tensor=attention_output)\n",
    "\n",
    "        # Down-projection, activation, up-projection, and LayerNorm\n",
    "        down_projected = self.activation(self.down_layer(layer_output))\n",
    "        up_projected = self.activation(self.up_layer(down_projected))\n",
    "        norm_output = self.layer_norm_new(up_projected + layer_output)  # Residual connection\n",
    "\n",
    "        return (norm_output,) + attention_outputs[1:]  # Return outputs\n",
    "\n",
    "# Custom Encoder\n",
    "class CustomRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([CustomRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "# Custom Model\n",
    "class CustomRobertaModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the encoder with the custom encoder\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = CustomRobertaEncoder(config)\n",
    "\n",
    "        # Add the classification head at the end\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, config.num_labels),\n",
    "        )\n",
    "        self.classifier._is_new = True\n",
    "\n",
    "        # Freeze existing layers if needed\n",
    "        self.freeze_pretrained_layers()\n",
    "\n",
    "    def freeze_pretrained_layers(self):\n",
    "        # Freeze all layers except the classifier and custom layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"classifier\" in name or \"down_layer\" in name or \"up_layer\" in name or \"layer_norm\" in name or \"attention\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # Embedding layer\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        # Extract the [CLS] token representation\n",
    "        cls_token_output = sequence_output[:, 0, :]\n",
    "\n",
    "       # Classification head\n",
    "        logits = self.classifier(cls_token_output)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Return loss if available, otherwise logits\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f617597-870e-4722-ba37-a16166ca068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CustomRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x CustomRobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (down_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (up_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (activation): GELU(approximate='none')\n",
       "        (layer_norm_new): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# Load configuration\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomRobertaModel(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_model = RobertaModel.from_pretrained(model_name)\n",
    "original_weights = pretrained_model.state_dict()\n",
    "\n",
    "custom_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear) and getattr(module, \"_is_new\", False):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "custom_model.apply(initialize_weights)\n",
    "\n",
    "# Compare weights\n",
    "# for name, param in custom_model.named_parameters():\n",
    "#     if any(keyword in name for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm\",]):\n",
    "#         print(\"Found:\", name, param)\n",
    "    # if name in original_weights:\n",
    "    #     if not torch.equal(param, original_weights[name]):\n",
    "    #         print(f\"Layer {name} weights were modified.\")\n",
    "    #     else:\n",
    "    #         print(f\"Layer {name, param} weights are unchanged.\")\n",
    "    # else:\n",
    "    #     print(\"Newly Added:\", name, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf5bb5-f000-4c06-8dff-d1b96fe98638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdde685-eb1c-40b3-a80a-e8fdd549608a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12c0b7-2700-4229-b8c7-a1c6bba5b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in custom_model.named_parameters():\n",
    "    if any(keyword in name for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm_new\",]):\n",
    "        print(\"New:\", name)\n",
    "    if all(keyword not in name for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm_new\",]):\n",
    "        print(\"Old:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1297337-4c18-4b46-b825-842e5730f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Evaluate periodically during training\n",
    "    #eval_steps=100,               # Frequency of evaluation (adjust as needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training for GPU\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    ")\n",
    "\n",
    "# Train base model\n",
    "trainer_base = Trainer(\n",
    "    model=custom_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29c7e9-1846-448c-9454-57d5d4940bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca646eba-cab8-420d-8901-afc236eaf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model, stage_name=\"Model\"):\n",
    "    print(f\"\\nTrainable Parameters in {stage_name}:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    print(f\"%\\\\age of trainable params: {(trainable_params/total_params) * 100}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  - {name}: {param.numel()} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8eaf7f0-aaa9-47c4-b96e-4e7da812ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Parameters in Adapter Model:\n",
      "Total Parameters: 381663234\n",
      "Trainable Parameters: 127114242\n",
      "%\\age of trainable params: 33.30534111650901\n",
      "  - encoder.layer.0.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.0.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.0.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.0.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.0.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.0.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.0.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.0.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.0.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.0.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.0.down_layer.weight: 524288 params\n",
      "  - encoder.layer.0.down_layer.bias: 512 params\n",
      "  - encoder.layer.0.up_layer.weight: 524288 params\n",
      "  - encoder.layer.0.up_layer.bias: 1024 params\n",
      "  - encoder.layer.0.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.0.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.1.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.1.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.1.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.1.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.1.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.1.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.1.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.1.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.1.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.1.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.1.down_layer.weight: 524288 params\n",
      "  - encoder.layer.1.down_layer.bias: 512 params\n",
      "  - encoder.layer.1.up_layer.weight: 524288 params\n",
      "  - encoder.layer.1.up_layer.bias: 1024 params\n",
      "  - encoder.layer.1.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.1.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.2.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.2.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.2.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.2.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.2.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.2.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.2.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.2.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.2.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.2.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.2.down_layer.weight: 524288 params\n",
      "  - encoder.layer.2.down_layer.bias: 512 params\n",
      "  - encoder.layer.2.up_layer.weight: 524288 params\n",
      "  - encoder.layer.2.up_layer.bias: 1024 params\n",
      "  - encoder.layer.2.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.2.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.3.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.3.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.3.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.3.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.3.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.3.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.3.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.3.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.3.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.3.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.3.down_layer.weight: 524288 params\n",
      "  - encoder.layer.3.down_layer.bias: 512 params\n",
      "  - encoder.layer.3.up_layer.weight: 524288 params\n",
      "  - encoder.layer.3.up_layer.bias: 1024 params\n",
      "  - encoder.layer.3.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.3.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.4.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.4.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.4.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.4.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.4.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.4.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.4.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.4.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.4.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.4.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.4.down_layer.weight: 524288 params\n",
      "  - encoder.layer.4.down_layer.bias: 512 params\n",
      "  - encoder.layer.4.up_layer.weight: 524288 params\n",
      "  - encoder.layer.4.up_layer.bias: 1024 params\n",
      "  - encoder.layer.4.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.4.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.5.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.5.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.5.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.5.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.5.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.5.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.5.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.5.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.5.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.5.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.5.down_layer.weight: 524288 params\n",
      "  - encoder.layer.5.down_layer.bias: 512 params\n",
      "  - encoder.layer.5.up_layer.weight: 524288 params\n",
      "  - encoder.layer.5.up_layer.bias: 1024 params\n",
      "  - encoder.layer.5.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.5.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.6.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.6.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.6.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.6.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.6.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.6.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.6.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.6.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.6.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.6.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.6.down_layer.weight: 524288 params\n",
      "  - encoder.layer.6.down_layer.bias: 512 params\n",
      "  - encoder.layer.6.up_layer.weight: 524288 params\n",
      "  - encoder.layer.6.up_layer.bias: 1024 params\n",
      "  - encoder.layer.6.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.6.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.7.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.7.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.7.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.7.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.7.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.7.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.7.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.7.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.7.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.7.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.7.down_layer.weight: 524288 params\n",
      "  - encoder.layer.7.down_layer.bias: 512 params\n",
      "  - encoder.layer.7.up_layer.weight: 524288 params\n",
      "  - encoder.layer.7.up_layer.bias: 1024 params\n",
      "  - encoder.layer.7.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.7.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.8.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.8.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.8.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.8.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.8.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.8.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.8.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.8.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.8.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.8.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.8.down_layer.weight: 524288 params\n",
      "  - encoder.layer.8.down_layer.bias: 512 params\n",
      "  - encoder.layer.8.up_layer.weight: 524288 params\n",
      "  - encoder.layer.8.up_layer.bias: 1024 params\n",
      "  - encoder.layer.8.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.8.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.9.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.9.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.9.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.9.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.9.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.9.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.9.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.9.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.9.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.9.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.9.down_layer.weight: 524288 params\n",
      "  - encoder.layer.9.down_layer.bias: 512 params\n",
      "  - encoder.layer.9.up_layer.weight: 524288 params\n",
      "  - encoder.layer.9.up_layer.bias: 1024 params\n",
      "  - encoder.layer.9.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.9.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.10.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.10.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.10.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.10.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.10.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.10.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.10.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.10.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.10.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.10.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.10.down_layer.weight: 524288 params\n",
      "  - encoder.layer.10.down_layer.bias: 512 params\n",
      "  - encoder.layer.10.up_layer.weight: 524288 params\n",
      "  - encoder.layer.10.up_layer.bias: 1024 params\n",
      "  - encoder.layer.10.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.10.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.11.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.11.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.11.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.11.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.11.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.11.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.11.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.11.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.11.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.11.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.11.down_layer.weight: 524288 params\n",
      "  - encoder.layer.11.down_layer.bias: 512 params\n",
      "  - encoder.layer.11.up_layer.weight: 524288 params\n",
      "  - encoder.layer.11.up_layer.bias: 1024 params\n",
      "  - encoder.layer.11.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.11.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.12.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.12.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.12.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.12.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.12.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.12.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.12.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.12.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.12.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.12.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.12.down_layer.weight: 524288 params\n",
      "  - encoder.layer.12.down_layer.bias: 512 params\n",
      "  - encoder.layer.12.up_layer.weight: 524288 params\n",
      "  - encoder.layer.12.up_layer.bias: 1024 params\n",
      "  - encoder.layer.12.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.12.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.13.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.13.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.13.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.13.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.13.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.13.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.13.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.13.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.13.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.13.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.13.down_layer.weight: 524288 params\n",
      "  - encoder.layer.13.down_layer.bias: 512 params\n",
      "  - encoder.layer.13.up_layer.weight: 524288 params\n",
      "  - encoder.layer.13.up_layer.bias: 1024 params\n",
      "  - encoder.layer.13.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.13.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.14.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.14.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.14.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.14.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.14.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.14.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.14.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.14.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.14.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.14.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.14.down_layer.weight: 524288 params\n",
      "  - encoder.layer.14.down_layer.bias: 512 params\n",
      "  - encoder.layer.14.up_layer.weight: 524288 params\n",
      "  - encoder.layer.14.up_layer.bias: 1024 params\n",
      "  - encoder.layer.14.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.14.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.15.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.15.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.15.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.15.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.15.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.15.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.15.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.15.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.15.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.15.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.15.down_layer.weight: 524288 params\n",
      "  - encoder.layer.15.down_layer.bias: 512 params\n",
      "  - encoder.layer.15.up_layer.weight: 524288 params\n",
      "  - encoder.layer.15.up_layer.bias: 1024 params\n",
      "  - encoder.layer.15.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.15.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.16.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.16.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.16.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.16.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.16.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.16.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.16.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.16.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.16.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.16.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.16.down_layer.weight: 524288 params\n",
      "  - encoder.layer.16.down_layer.bias: 512 params\n",
      "  - encoder.layer.16.up_layer.weight: 524288 params\n",
      "  - encoder.layer.16.up_layer.bias: 1024 params\n",
      "  - encoder.layer.16.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.16.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.17.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.17.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.17.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.17.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.17.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.17.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.17.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.17.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.17.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.17.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.17.down_layer.weight: 524288 params\n",
      "  - encoder.layer.17.down_layer.bias: 512 params\n",
      "  - encoder.layer.17.up_layer.weight: 524288 params\n",
      "  - encoder.layer.17.up_layer.bias: 1024 params\n",
      "  - encoder.layer.17.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.17.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.18.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.18.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.18.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.18.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.18.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.18.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.18.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.18.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.18.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.18.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.18.down_layer.weight: 524288 params\n",
      "  - encoder.layer.18.down_layer.bias: 512 params\n",
      "  - encoder.layer.18.up_layer.weight: 524288 params\n",
      "  - encoder.layer.18.up_layer.bias: 1024 params\n",
      "  - encoder.layer.18.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.18.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.19.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.19.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.19.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.19.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.19.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.19.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.19.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.19.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.19.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.19.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.19.down_layer.weight: 524288 params\n",
      "  - encoder.layer.19.down_layer.bias: 512 params\n",
      "  - encoder.layer.19.up_layer.weight: 524288 params\n",
      "  - encoder.layer.19.up_layer.bias: 1024 params\n",
      "  - encoder.layer.19.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.19.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.20.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.20.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.20.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.20.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.20.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.20.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.20.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.20.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.20.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.20.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.20.down_layer.weight: 524288 params\n",
      "  - encoder.layer.20.down_layer.bias: 512 params\n",
      "  - encoder.layer.20.up_layer.weight: 524288 params\n",
      "  - encoder.layer.20.up_layer.bias: 1024 params\n",
      "  - encoder.layer.20.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.20.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.21.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.21.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.21.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.21.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.21.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.21.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.21.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.21.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.21.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.21.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.21.down_layer.weight: 524288 params\n",
      "  - encoder.layer.21.down_layer.bias: 512 params\n",
      "  - encoder.layer.21.up_layer.weight: 524288 params\n",
      "  - encoder.layer.21.up_layer.bias: 1024 params\n",
      "  - encoder.layer.21.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.21.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.22.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.22.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.22.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.22.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.22.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.22.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.22.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.22.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.22.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.22.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.22.down_layer.weight: 524288 params\n",
      "  - encoder.layer.22.down_layer.bias: 512 params\n",
      "  - encoder.layer.22.up_layer.weight: 524288 params\n",
      "  - encoder.layer.22.up_layer.bias: 1024 params\n",
      "  - encoder.layer.22.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.22.layer_norm_new.bias: 1024 params\n",
      "  - encoder.layer.23.attention.self.query.weight: 1048576 params\n",
      "  - encoder.layer.23.attention.self.query.bias: 1024 params\n",
      "  - encoder.layer.23.attention.self.key.weight: 1048576 params\n",
      "  - encoder.layer.23.attention.self.key.bias: 1024 params\n",
      "  - encoder.layer.23.attention.self.value.weight: 1048576 params\n",
      "  - encoder.layer.23.attention.self.value.bias: 1024 params\n",
      "  - encoder.layer.23.attention.output.dense.weight: 1048576 params\n",
      "  - encoder.layer.23.attention.output.dense.bias: 1024 params\n",
      "  - encoder.layer.23.attention.output.LayerNorm.weight: 1024 params\n",
      "  - encoder.layer.23.attention.output.LayerNorm.bias: 1024 params\n",
      "  - encoder.layer.23.down_layer.weight: 524288 params\n",
      "  - encoder.layer.23.down_layer.bias: 512 params\n",
      "  - encoder.layer.23.up_layer.weight: 524288 params\n",
      "  - encoder.layer.23.up_layer.bias: 1024 params\n",
      "  - encoder.layer.23.layer_norm_new.weight: 1024 params\n",
      "  - encoder.layer.23.layer_norm_new.bias: 1024 params\n",
      "  - classifier.0.weight: 1048576 params\n",
      "  - classifier.0.bias: 1024 params\n",
      "  - classifier.2.weight: 2048 params\n",
      "  - classifier.2.bias: 2 params\n"
     ]
    }
   ],
   "source": [
    "print_trainable_params(custom_model, stage_name=\"Adapter Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a0201b-f123-4f38-a933-3cb8a1049b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Custom Adapter Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1895' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1895/4689 1:25:24 < 2:06:03, 0.37 it/s, Epoch 1.21/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695700</td>\n",
       "      <td>0.694092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Custom Adapter Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Resize model embeddings after adding new special tokens\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# custom_model.resize_token_embeddings(len(tokenizer))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "custom_model.to(device)\n",
    "print(\"\\nTraining Custom Adapter Model...\")\n",
    "# Resize model embeddings after adding new special tokens\n",
    "# custom_model.resize_token_embeddings(len(tokenizer))\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd4db22a-cadc-4172-9e9b-1de52f9be2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06891806088924982"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26303490/381663234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea296e78-5e51-4a77-ab05-5addf8c1e5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Parameters in Base Model:\n",
      "Total Parameters: 381663234\n",
      "Trainable Parameters: 26303490\n",
      "  - encoder.layer.0.down_layer.weight: 524288 params\n",
      "  - encoder.layer.0.down_layer.bias: 512 params\n",
      "  - encoder.layer.0.up_layer.weight: 524288 params\n",
      "  - encoder.layer.0.up_layer.bias: 1024 params\n",
      "  - encoder.layer.0.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.0.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.1.down_layer.weight: 524288 params\n",
      "  - encoder.layer.1.down_layer.bias: 512 params\n",
      "  - encoder.layer.1.up_layer.weight: 524288 params\n",
      "  - encoder.layer.1.up_layer.bias: 1024 params\n",
      "  - encoder.layer.1.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.1.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.2.down_layer.weight: 524288 params\n",
      "  - encoder.layer.2.down_layer.bias: 512 params\n",
      "  - encoder.layer.2.up_layer.weight: 524288 params\n",
      "  - encoder.layer.2.up_layer.bias: 1024 params\n",
      "  - encoder.layer.2.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.2.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.3.down_layer.weight: 524288 params\n",
      "  - encoder.layer.3.down_layer.bias: 512 params\n",
      "  - encoder.layer.3.up_layer.weight: 524288 params\n",
      "  - encoder.layer.3.up_layer.bias: 1024 params\n",
      "  - encoder.layer.3.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.3.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.4.down_layer.weight: 524288 params\n",
      "  - encoder.layer.4.down_layer.bias: 512 params\n",
      "  - encoder.layer.4.up_layer.weight: 524288 params\n",
      "  - encoder.layer.4.up_layer.bias: 1024 params\n",
      "  - encoder.layer.4.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.4.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.5.down_layer.weight: 524288 params\n",
      "  - encoder.layer.5.down_layer.bias: 512 params\n",
      "  - encoder.layer.5.up_layer.weight: 524288 params\n",
      "  - encoder.layer.5.up_layer.bias: 1024 params\n",
      "  - encoder.layer.5.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.5.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.6.down_layer.weight: 524288 params\n",
      "  - encoder.layer.6.down_layer.bias: 512 params\n",
      "  - encoder.layer.6.up_layer.weight: 524288 params\n",
      "  - encoder.layer.6.up_layer.bias: 1024 params\n",
      "  - encoder.layer.6.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.6.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.7.down_layer.weight: 524288 params\n",
      "  - encoder.layer.7.down_layer.bias: 512 params\n",
      "  - encoder.layer.7.up_layer.weight: 524288 params\n",
      "  - encoder.layer.7.up_layer.bias: 1024 params\n",
      "  - encoder.layer.7.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.7.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.8.down_layer.weight: 524288 params\n",
      "  - encoder.layer.8.down_layer.bias: 512 params\n",
      "  - encoder.layer.8.up_layer.weight: 524288 params\n",
      "  - encoder.layer.8.up_layer.bias: 1024 params\n",
      "  - encoder.layer.8.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.8.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.9.down_layer.weight: 524288 params\n",
      "  - encoder.layer.9.down_layer.bias: 512 params\n",
      "  - encoder.layer.9.up_layer.weight: 524288 params\n",
      "  - encoder.layer.9.up_layer.bias: 1024 params\n",
      "  - encoder.layer.9.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.9.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.10.down_layer.weight: 524288 params\n",
      "  - encoder.layer.10.down_layer.bias: 512 params\n",
      "  - encoder.layer.10.up_layer.weight: 524288 params\n",
      "  - encoder.layer.10.up_layer.bias: 1024 params\n",
      "  - encoder.layer.10.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.10.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.11.down_layer.weight: 524288 params\n",
      "  - encoder.layer.11.down_layer.bias: 512 params\n",
      "  - encoder.layer.11.up_layer.weight: 524288 params\n",
      "  - encoder.layer.11.up_layer.bias: 1024 params\n",
      "  - encoder.layer.11.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.11.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.12.down_layer.weight: 524288 params\n",
      "  - encoder.layer.12.down_layer.bias: 512 params\n",
      "  - encoder.layer.12.up_layer.weight: 524288 params\n",
      "  - encoder.layer.12.up_layer.bias: 1024 params\n",
      "  - encoder.layer.12.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.12.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.13.down_layer.weight: 524288 params\n",
      "  - encoder.layer.13.down_layer.bias: 512 params\n",
      "  - encoder.layer.13.up_layer.weight: 524288 params\n",
      "  - encoder.layer.13.up_layer.bias: 1024 params\n",
      "  - encoder.layer.13.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.13.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.14.down_layer.weight: 524288 params\n",
      "  - encoder.layer.14.down_layer.bias: 512 params\n",
      "  - encoder.layer.14.up_layer.weight: 524288 params\n",
      "  - encoder.layer.14.up_layer.bias: 1024 params\n",
      "  - encoder.layer.14.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.14.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.15.down_layer.weight: 524288 params\n",
      "  - encoder.layer.15.down_layer.bias: 512 params\n",
      "  - encoder.layer.15.up_layer.weight: 524288 params\n",
      "  - encoder.layer.15.up_layer.bias: 1024 params\n",
      "  - encoder.layer.15.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.15.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.16.down_layer.weight: 524288 params\n",
      "  - encoder.layer.16.down_layer.bias: 512 params\n",
      "  - encoder.layer.16.up_layer.weight: 524288 params\n",
      "  - encoder.layer.16.up_layer.bias: 1024 params\n",
      "  - encoder.layer.16.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.16.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.17.down_layer.weight: 524288 params\n",
      "  - encoder.layer.17.down_layer.bias: 512 params\n",
      "  - encoder.layer.17.up_layer.weight: 524288 params\n",
      "  - encoder.layer.17.up_layer.bias: 1024 params\n",
      "  - encoder.layer.17.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.17.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.18.down_layer.weight: 524288 params\n",
      "  - encoder.layer.18.down_layer.bias: 512 params\n",
      "  - encoder.layer.18.up_layer.weight: 524288 params\n",
      "  - encoder.layer.18.up_layer.bias: 1024 params\n",
      "  - encoder.layer.18.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.18.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.19.down_layer.weight: 524288 params\n",
      "  - encoder.layer.19.down_layer.bias: 512 params\n",
      "  - encoder.layer.19.up_layer.weight: 524288 params\n",
      "  - encoder.layer.19.up_layer.bias: 1024 params\n",
      "  - encoder.layer.19.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.19.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.20.down_layer.weight: 524288 params\n",
      "  - encoder.layer.20.down_layer.bias: 512 params\n",
      "  - encoder.layer.20.up_layer.weight: 524288 params\n",
      "  - encoder.layer.20.up_layer.bias: 1024 params\n",
      "  - encoder.layer.20.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.20.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.21.down_layer.weight: 524288 params\n",
      "  - encoder.layer.21.down_layer.bias: 512 params\n",
      "  - encoder.layer.21.up_layer.weight: 524288 params\n",
      "  - encoder.layer.21.up_layer.bias: 1024 params\n",
      "  - encoder.layer.21.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.21.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.22.down_layer.weight: 524288 params\n",
      "  - encoder.layer.22.down_layer.bias: 512 params\n",
      "  - encoder.layer.22.up_layer.weight: 524288 params\n",
      "  - encoder.layer.22.up_layer.bias: 1024 params\n",
      "  - encoder.layer.22.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.22.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.23.down_layer.weight: 524288 params\n",
      "  - encoder.layer.23.down_layer.bias: 512 params\n",
      "  - encoder.layer.23.up_layer.weight: 524288 params\n",
      "  - encoder.layer.23.up_layer.bias: 1024 params\n",
      "  - encoder.layer.23.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.23.layer_norm.bias: 1024 params\n",
      "  - classifier.0.weight: 1048576 params\n",
      "  - classifier.0.bias: 1024 params\n",
      "  - classifier.2.weight: 2048 params\n",
      "  - classifier.2.bias: 2 params\n"
     ]
    }
   ],
   "source": [
    "print_trainable_params(custom_model, stage_name=\"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad630c-6c8b-49c0-b3be-9bd460c1639f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat_nlp",
   "language": "python",
   "name": "stat_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
