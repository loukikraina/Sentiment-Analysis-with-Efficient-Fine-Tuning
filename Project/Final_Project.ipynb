{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db05b641-b477-4788-afc8-677dafd7b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Making sure it downloads models on my D drive, as no space in defualt file location\n",
    "os.environ['HF_HOME'] = 'D:\\\\Download\\\\UCSD\\\\cache'\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DebertaV2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from adapters import AdapterConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be59cec3-cf96-475a-bef8-489986d5d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Using Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9578774-1faf-4122-84bb-d47e6b8957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directories\n",
    "BASE_MODEL_DIR = \"./base_model\"\n",
    "LORA_MODEL_DIR = \"./lora_model\"\n",
    "ADAPTER_MODEL_DIR = \"./adapter_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6786e0aa-af95-4583-afe0-6aef67376e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 1B and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Using LLama 1B as base model\n",
    "\n",
    "# Couldn't train Llama because of lower mem GPUs so shifting to roberta\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ensure tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068915-0dc2-404e-b8a2-6cb8ff75c635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a59e493e-5941-4a6c-b968-6db0a2ba5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)  # Use full training dataset\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)    # Use full testing dataset\n",
    "\n",
    "# Veyr big dataset\n",
    "# Load a sentiment dataset (example: SST2)\n",
    "# ds = load_dataset(\"facebook/xnli\", \"all_languages\")\n",
    "# train_data = ds['train']\n",
    "# val_data = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99124809-bb4f-4c16-8f0b-2d3d2eab2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model, stage_name=\"Model\"):\n",
    "    print(f\"\\nTrainable Parameters in {stage_name}:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  - {name}: {param.numel()} params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738b4c-5eac-4474-b8a7-f1059757b2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e38754b-57ab-48ec-8ddf-58af4d37ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate periodically during training\n",
    "    #eval_steps=100,               # Frequency of evaluation (adjust as needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training for GPU\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    ")\n",
    "\n",
    "# Train base model\n",
    "trainer_base = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f194fb5-d8e7-434b-bf82-1eadc1809c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044c143-513d-48a6-b77c-27978ffacc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b655e5-9fc9-4cff-9684-2a3661d6bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_params(base_model, stage_name=\"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a387cc-974f-41cb-937d-4edf8387bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Base Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/4689 06:36 < 30:16:39, 0.04 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Resize model embeddings after adding new special tokens\u001b[39;00m\n\u001b[0;32m      3\u001b[0m base_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\accelerate\\accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "If \n",
    "print(\"\\nTraining Base Model...\")\n",
    "# Resize model embeddings after adding new special tokens\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a31c3a-d6d7-4d3f-93e6-3520a66719d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836e7b4-f721-4baa-bf44-848aba3b81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save base model\n",
    "tokenizer.save_pretrained(\"./base_model\")\n",
    "base_model.save_pretrained(\"./base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd97c7-ae67-47cc-8828-6f352f74a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "print(\"\\nEvaluating Base Model...\")\n",
    "base_results = trainer_base.evaluate()\n",
    "print(\"Base Model Results:\", base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181e522-f4c0-4926-bd38-6985d2a6b1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b1dd5-5fc1-4e67-b8ed-0fe3b999aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\", \n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "lora_model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "start_time = time.time()\n",
    "print(\"\\nTraining LoRA Model...\")\n",
    "trainer_lora.train()\n",
    "print(f\"LoRa trained in: {time.time() - start_time}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f322e-1953-4bd0-ade7-199f31a7190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA model\n",
    "tokenizer.save_pretrained(\"./lora_model\")\n",
    "lora_model.save_pretrained(\"./lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6226322-1a15-4026-abd0-f0573a802818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a95146-ca6f-45a2-8271-bb8a88a8c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LoRA model\n",
    "print(\"\\nEvaluating LoRA Model...\")\n",
    "lora_results = trainer_lora.evaluate()\n",
    "print(\"LoRA Model Results:\", lora_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d195363a-5a33-455a-aab0-1b30a06bffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaLayer, RobertaAttention, RobertaIntermediate, RobertaOutput\n",
    "\n",
    "# class CustomRobertaLayer(RobertaModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.attention = RobertaAttention(config)  # Multi-head attention\n",
    "#         self.intermediate = RobertaIntermediate(config)  # Feed-forward network\n",
    "#         self.output = RobertaOutput(config)  # Projection back\n",
    "#         self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)\n",
    "#         self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.up_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         hidden_states,\n",
    "#         attention_mask=None,\n",
    "#         head_mask=None,\n",
    "#         output_attentions=None,\n",
    "#         output_hidden_states=None, input_ids=None, token_type_ids=None, position_ids=None, \n",
    "#         inputs_embeds=None, labels=None, \n",
    "#     ):\n",
    "#         # 1. Self-attention\n",
    "#         attention_output = self.attention(\n",
    "#             hidden_states, attention_mask=attention_mask, head_mask=head_mask\n",
    "#         )\n",
    "        \n",
    "#         # 2. Downsample â†’ Activation â†’ Upsample\n",
    "#         downsampled = self.activation(self.down_layer(attention_output))\n",
    "#         upsampled = self.up_layer(downsampled)\n",
    "#         normalized = self.up_norm(upsampled + attention_output)  # Add residual connection\n",
    "\n",
    "#         # 3. Intermediate feed-forward network\n",
    "#         intermediate_output = self.intermediate(normalized)\n",
    "\n",
    "#         # 4. Final output projection and residual connection\n",
    "#         layer_output = self.output(intermediate_output, normalized)\n",
    "#         return layer_output\n",
    "\n",
    "# class CustomRobertaLayer(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.attention = RobertaAttention(config)\n",
    "#         self.intermediate = RobertaIntermediate(config)\n",
    "#         self.output = RobertaOutput(config)\n",
    "#         self.down_layer = nn.Linear(config.hidden_size, 512)\n",
    "#         self.up_layer = nn.Linear(512, config.hidden_size)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.up_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "#     def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "#         hidden_states = self.attention(hidden_states, attention_mask, head_mask)\n",
    "#         intermediate_output = self.intermediate(hidden_states)\n",
    "#         output = self.output(intermediate_output, hidden_states)\n",
    "#         output = self.down_layer(output)\n",
    "#         output = self.activation(output)\n",
    "#         output = self.up_layer(output)\n",
    "#         return self.up_norm(output)\n",
    "\n",
    "class CustomRobertaLayer(RobertaLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)  # Down-project\n",
    "        self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)  # Up-project\n",
    "        self.activation = nn.ReLU()  # You can use other activations like GELU\n",
    "        self.up_norm = nn.LayerNorm(config.hidden_size)  #Normalization layer \n",
    "\n",
    "    def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_value=None,\n",
    "    use_cache=False,\n",
    "    output_attentions=False,\n",
    "    **kwargs,\n",
    "    ):\n",
    "    # Ensure the attention mask is in the correct dtype\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # Match precision (e.g., float16)\n",
    "        \n",
    "        # Original attention operation\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "    \n",
    "        attention_output = attention_outputs[0]\n",
    "        print(f\"Attention Output Shape: {attention_output.shape}\")\n",
    "        \n",
    "        # Down-projection\n",
    "        down_projected = self.activation(self.down_layer(attention_output))\n",
    "        print(f\"Down Projected Shape: {down_projected.shape}\")\n",
    "        # Up-projection\n",
    "        up_projected = self.activation(self.up_layer(down_projected))\n",
    "        print(f\"Up Projected Shape: {up_projected.shape}\")\n",
    "        #normalization\n",
    "        normalized_output = self.up_norm(up_projected)\n",
    "        print(f\"Normalization Shape: {normalized_output.shape}\")\n",
    "        \n",
    "        # Add & Norm after FF layers\n",
    "        layer_output = self.output(hidden_states=normalized_output, input_tensor=attention_output)\n",
    "        return (layer_output,) + attention_outputs[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f593cc0d-7ef9-4ae2-b99d-ebf775b342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder, RobertaConfig, RobertaEmbeddings\n",
    "\n",
    "\n",
    "class CustomRobertaModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the encoder with the custom encoder\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = CustomRobertaEncoder(config)\n",
    "\n",
    "        # Add the classification head at the end\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, config.num_labels),\n",
    "        )\n",
    "\n",
    "        # Freeze existing layers and enable gradients only for the new layers\n",
    "        self.freeze_pretrained_layers()\n",
    "\n",
    "    def freeze_pretrained_layers(self):\n",
    "        # Freeze all layers except the new classifier and encoder's added layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"classifier\" in name or \"down_layer\" in name or \"up_layer\" in name or \"up_norm\" in name:\n",
    "                param.requires_grad = True  # Enable gradients for new layers\n",
    "            else:\n",
    "                param.requires_grad = False  # Freeze existing layers\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, \n",
    "        head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, labels=None, \n",
    "    ):\n",
    "\n",
    "        print(\"Calling super()\")\n",
    "        print(\"What are labels?: \", labels)\n",
    "        # Embeddings layer\n",
    "        embeddings_output = self.embeddings(input_ids, token_type_ids, position_ids)\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_outputs = self.encoder(embeddings_output, attention_mask)\n",
    "\n",
    "        # Extract [CLS] token for classification\n",
    "        cls_token_output = encoder_outputs[:, 0, :]\n",
    "        \n",
    "        # outputs = super().forward(\n",
    "        #     input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, \n",
    "        #     head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states\n",
    "        # )\n",
    "        # print(\"Super Done\")\n",
    "        # pooled_output = outputs[0][:, 0, :]        \n",
    "        print(f\"Encoder Output Shape: {cls_token_output.shape}\")\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        print(f\"Logits Shape: {logits.shape}\") \n",
    "\n",
    "        return (logits,) + outputs[2:]  # Return logits along with other outputs\n",
    "        \n",
    "class CustomRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([CustomRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff5136a-2737-40ef-9dba-b66f9c75d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CustomRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x CustomRobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (down_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (up_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (activation): ReLU()\n",
       "        (up_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "# Load the configuration\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomRobertaModel(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_model = RobertaModel.from_pretrained(model_name, num_labels=2)\n",
    "custom_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "custom_model.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb534130-7ea0-457f-9545-2543091836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaModel,\n",
    "    RobertaEncoder,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaConfig,\n",
    ")\n",
    "\n",
    "# Custom Layer that adds Down-Up Projection and LayerNorm after FF layer\n",
    "class CustomRobertaLayer(RobertaLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)  # Down-projection\n",
    "        self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)    # Up-projection\n",
    "        self.activation = nn.ReLU()                                              # Activation function\n",
    "        self.layer_norm_new = nn.LayerNorm(config.hidden_size)                       # LayerNorm after Up-projection\n",
    "        # intializing all as new layers\n",
    "        self.down_layer._is_new = True\n",
    "        self.up_layer._is_new = True\n",
    "        self.activation._is_new = True\n",
    "        self.layer_norm_new._is_new = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Ensure the attention mask matches the required dimensions\n",
    "        if attention_mask is not None:\n",
    "            # Expand dimensions for multi-head attention\n",
    "            attention_mask = attention_mask[:, None, None, :]  # Shape: [batch_size, 1, 1, seq_len]\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # Match precision (e.g., float16)\n",
    "\n",
    "\n",
    "        # Attention sub-layer\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output = attention_outputs[0]\n",
    "\n",
    "        # Feed-forward sub-layer\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(hidden_states=intermediate_output, input_tensor=attention_output)\n",
    "\n",
    "        # Down-projection, activation, up-projection, and LayerNorm\n",
    "        down_projected = self.activation(self.down_layer(layer_output))\n",
    "        up_projected = self.activation(self.up_layer(down_projected))\n",
    "        norm_output = self.layer_norm_new(up_projected + layer_output)  # Residual connection\n",
    "\n",
    "        return (norm_output,) + attention_outputs[1:]  # Return outputs\n",
    "\n",
    "# Custom Encoder\n",
    "class CustomRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([CustomRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "# Custom Model\n",
    "class CustomRobertaModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the encoder with the custom encoder\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = CustomRobertaEncoder(config)\n",
    "\n",
    "        # Add the classification head at the end\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, config.num_labels),\n",
    "        )\n",
    "        self.classifier._is_new = True\n",
    "\n",
    "        # Freeze existing layers if needed\n",
    "        self.freeze_pretrained_layers()\n",
    "\n",
    "    def freeze_pretrained_layers(self):\n",
    "        # Freeze all layers except the classifier and custom layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"classifier\" in name or \"down_layer\" in name or \"up_layer\" in name or \"layer_norm\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # Embedding layer\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        # Extract the [CLS] token representation\n",
    "        cls_token_output = sequence_output[:, 0, :]\n",
    "\n",
    "       # Classification head\n",
    "        logits = self.classifier(cls_token_output)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Return loss if available, otherwise logits\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f617597-870e-4722-ba37-a16166ca068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: encoder.layer.0.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0320,  0.0090, -0.0493,  ...,  0.0186,  0.0459, -0.0127],\n",
      "        [-0.0592,  0.0212,  0.0170,  ..., -0.0181, -0.0251, -0.0375],\n",
      "        [-0.0496, -0.0348,  0.0044,  ..., -0.0325, -0.0543, -0.0345],\n",
      "        ...,\n",
      "        [-0.0485, -0.0443,  0.0314,  ..., -0.0545, -0.0387,  0.0126],\n",
      "        [-0.0586,  0.0558, -0.0169,  ..., -0.0381,  0.0244,  0.0500],\n",
      "        [ 0.0610, -0.0195,  0.0125,  ...,  0.0501,  0.0268,  0.0381]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.0.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.0.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0561,  0.0519, -0.0278,  ..., -0.0564, -0.0543,  0.0106],\n",
      "        [-0.0076,  0.0089, -0.0505,  ..., -0.0250,  0.0067, -0.0306],\n",
      "        [-0.0024, -0.0514,  0.0306,  ...,  0.0020, -0.0576,  0.0308],\n",
      "        ...,\n",
      "        [ 0.0592,  0.0286, -0.0549,  ..., -0.0613, -0.0379,  0.0473],\n",
      "        [-0.0195, -0.0533, -0.0406,  ..., -0.0306,  0.0348,  0.0310],\n",
      "        [ 0.0524,  0.0443, -0.0300,  ...,  0.0489,  0.0006, -0.0295]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.0.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.0.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.0.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.1.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0366,  0.0493,  0.0614,  ...,  0.0270, -0.0278,  0.0586],\n",
      "        [-0.0291, -0.0543,  0.0333,  ...,  0.0151, -0.0198, -0.0032],\n",
      "        [ 0.0575,  0.0487, -0.0156,  ...,  0.0373, -0.0624, -0.0541],\n",
      "        ...,\n",
      "        [-0.0138, -0.0040,  0.0079,  ..., -0.0112, -0.0135, -0.0348],\n",
      "        [ 0.0335, -0.0535, -0.0132,  ...,  0.0078,  0.0437, -0.0208],\n",
      "        [-0.0310,  0.0624,  0.0538,  ...,  0.0536, -0.0605,  0.0364]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.1.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.1.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0042, -0.0021,  0.0373,  ...,  0.0611, -0.0227, -0.0365],\n",
      "        [-0.0218, -0.0052,  0.0009,  ..., -0.0594, -0.0257, -0.0485],\n",
      "        [ 0.0232, -0.0127, -0.0034,  ..., -0.0481, -0.0134, -0.0478],\n",
      "        ...,\n",
      "        [ 0.0289,  0.0533,  0.0277,  ...,  0.0148, -0.0049, -0.0222],\n",
      "        [-0.0278, -0.0062, -0.0375,  ...,  0.0214,  0.0019, -0.0491],\n",
      "        [ 0.0245,  0.0406,  0.0319,  ..., -0.0115, -0.0558,  0.0481]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.1.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.1.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.1.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.2.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0530, -0.0619, -0.0045,  ..., -0.0415, -0.0616,  0.0332],\n",
      "        [-0.0011,  0.0507,  0.0019,  ...,  0.0405,  0.0604, -0.0482],\n",
      "        [-0.0256, -0.0029,  0.0029,  ...,  0.0209, -0.0133, -0.0131],\n",
      "        ...,\n",
      "        [-0.0198, -0.0060,  0.0204,  ..., -0.0160, -0.0366,  0.0488],\n",
      "        [ 0.0576,  0.0247, -0.0588,  ...,  0.0268, -0.0291, -0.0427],\n",
      "        [-0.0446,  0.0334, -0.0161,  ...,  0.0531,  0.0407,  0.0048]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.2.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.2.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0125,  0.0229,  0.0440,  ...,  0.0346, -0.0430,  0.0314],\n",
      "        [ 0.0619, -0.0024,  0.0040,  ..., -0.0243, -0.0602, -0.0114],\n",
      "        [-0.0390,  0.0200,  0.0335,  ...,  0.0342, -0.0429, -0.0025],\n",
      "        ...,\n",
      "        [ 0.0401,  0.0039, -0.0542,  ...,  0.0552,  0.0033,  0.0205],\n",
      "        [ 0.0042, -0.0330, -0.0305,  ...,  0.0253,  0.0322,  0.0593],\n",
      "        [ 0.0532, -0.0604,  0.0048,  ..., -0.0033,  0.0591, -0.0502]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.2.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.2.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.2.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.3.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0009,  0.0254, -0.0376,  ..., -0.0108, -0.0295, -0.0199],\n",
      "        [-0.0568, -0.0086,  0.0623,  ...,  0.0335,  0.0500, -0.0538],\n",
      "        [ 0.0483, -0.0401, -0.0513,  ..., -0.0118, -0.0150, -0.0188],\n",
      "        ...,\n",
      "        [-0.0053, -0.0292, -0.0540,  ...,  0.0142, -0.0405,  0.0528],\n",
      "        [-0.0330, -0.0600, -0.0130,  ...,  0.0091, -0.0023,  0.0505],\n",
      "        [ 0.0167, -0.0253,  0.0255,  ...,  0.0353, -0.0405,  0.0244]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.3.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.3.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0084, -0.0070,  0.0347,  ..., -0.0092,  0.0039, -0.0283],\n",
      "        [ 0.0409,  0.0458,  0.0420,  ..., -0.0167, -0.0129,  0.0421],\n",
      "        [-0.0289,  0.0283,  0.0320,  ..., -0.0178,  0.0025, -0.0169],\n",
      "        ...,\n",
      "        [ 0.0193,  0.0603, -0.0005,  ...,  0.0371,  0.0046, -0.0553],\n",
      "        [-0.0089, -0.0296,  0.0236,  ...,  0.0074,  0.0571, -0.0054],\n",
      "        [ 0.0385,  0.0543, -0.0150,  ...,  0.0503, -0.0066,  0.0006]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.3.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.3.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.3.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.4.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0258,  0.0157, -0.0489,  ..., -0.0073,  0.0155,  0.0152],\n",
      "        [ 0.0103, -0.0444, -0.0257,  ...,  0.0612,  0.0370, -0.0183],\n",
      "        [-0.0572, -0.0241, -0.0509,  ..., -0.0191, -0.0540, -0.0061],\n",
      "        ...,\n",
      "        [ 0.0525, -0.0208, -0.0078,  ...,  0.0600, -0.0299, -0.0132],\n",
      "        [ 0.0502, -0.0076,  0.0178,  ..., -0.0348,  0.0567,  0.0529],\n",
      "        [ 0.0330, -0.0365, -0.0614,  ..., -0.0142, -0.0193,  0.0376]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.4.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.4.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0203, -0.0615, -0.0393,  ..., -0.0343, -0.0365,  0.0278],\n",
      "        [ 0.0099,  0.0429, -0.0082,  ...,  0.0057, -0.0334,  0.0127],\n",
      "        [-0.0118,  0.0133,  0.0315,  ..., -0.0463,  0.0226, -0.0060],\n",
      "        ...,\n",
      "        [ 0.0213, -0.0137,  0.0224,  ...,  0.0240,  0.0412, -0.0286],\n",
      "        [ 0.0148, -0.0216, -0.0052,  ...,  0.0585,  0.0188,  0.0184],\n",
      "        [ 0.0509, -0.0103, -0.0269,  ...,  0.0593,  0.0116, -0.0295]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.4.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.4.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.4.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.5.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0485, -0.0565,  0.0593,  ..., -0.0535, -0.0568,  0.0522],\n",
      "        [ 0.0066, -0.0320,  0.0337,  ..., -0.0523, -0.0454, -0.0252],\n",
      "        [ 0.0268, -0.0204,  0.0107,  ..., -0.0066, -0.0229,  0.0130],\n",
      "        ...,\n",
      "        [-0.0163, -0.0392,  0.0132,  ..., -0.0304,  0.0190, -0.0330],\n",
      "        [-0.0125,  0.0493,  0.0508,  ..., -0.0366, -0.0370,  0.0084],\n",
      "        [-0.0154, -0.0382, -0.0501,  ...,  0.0197, -0.0073,  0.0319]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.5.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.5.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0435,  0.0382,  0.0229,  ...,  0.0582, -0.0112,  0.0124],\n",
      "        [ 0.0191, -0.0030,  0.0613,  ..., -0.0018,  0.0520, -0.0583],\n",
      "        [-0.0029, -0.0056,  0.0432,  ..., -0.0358,  0.0141, -0.0209],\n",
      "        ...,\n",
      "        [ 0.0246, -0.0169,  0.0620,  ...,  0.0472,  0.0347,  0.0415],\n",
      "        [-0.0410,  0.0034, -0.0062,  ..., -0.0184, -0.0090, -0.0334],\n",
      "        [-0.0109, -0.0294, -0.0534,  ...,  0.0038,  0.0487,  0.0301]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.5.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.5.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.5.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.6.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0525,  0.0512, -0.0158,  ...,  0.0469,  0.0326,  0.0603],\n",
      "        [-0.0064,  0.0465,  0.0566,  ..., -0.0152,  0.0152,  0.0167],\n",
      "        [-0.0445, -0.0229,  0.0080,  ...,  0.0479, -0.0356, -0.0149],\n",
      "        ...,\n",
      "        [-0.0501, -0.0135,  0.0105,  ...,  0.0298,  0.0385, -0.0407],\n",
      "        [-0.0422, -0.0151,  0.0490,  ...,  0.0112, -0.0168,  0.0193],\n",
      "        [-0.0370,  0.0275,  0.0183,  ...,  0.0602,  0.0254,  0.0196]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.6.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.6.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0097,  0.0081,  0.0119,  ...,  0.0106,  0.0436,  0.0173],\n",
      "        [ 0.0618,  0.0213, -0.0350,  ..., -0.0344,  0.0586,  0.0068],\n",
      "        [-0.0068, -0.0316, -0.0611,  ..., -0.0084,  0.0116,  0.0076],\n",
      "        ...,\n",
      "        [-0.0174,  0.0049,  0.0250,  ..., -0.0375, -0.0487,  0.0286],\n",
      "        [ 0.0117,  0.0340, -0.0608,  ...,  0.0030,  0.0600,  0.0369],\n",
      "        [-0.0269, -0.0115, -0.0521,  ..., -0.0343, -0.0291, -0.0073]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.6.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.6.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.6.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.7.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0248,  0.0622,  0.0451,  ...,  0.0560,  0.0080, -0.0176],\n",
      "        [-0.0353,  0.0280,  0.0069,  ..., -0.0259,  0.0231, -0.0521],\n",
      "        [ 0.0436, -0.0425, -0.0315,  ..., -0.0472, -0.0247,  0.0137],\n",
      "        ...,\n",
      "        [ 0.0043,  0.0459,  0.0206,  ...,  0.0448, -0.0382,  0.0570],\n",
      "        [ 0.0584,  0.0176,  0.0392,  ..., -0.0090, -0.0455,  0.0505],\n",
      "        [ 0.0111,  0.0236,  0.0495,  ...,  0.0508, -0.0109,  0.0269]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.7.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.7.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0153,  0.0483,  0.0224,  ..., -0.0402, -0.0568,  0.0388],\n",
      "        [-0.0448, -0.0254,  0.0444,  ...,  0.0483, -0.0204,  0.0552],\n",
      "        [ 0.0411, -0.0156,  0.0564,  ..., -0.0008,  0.0117, -0.0114],\n",
      "        ...,\n",
      "        [-0.0198,  0.0388,  0.0427,  ...,  0.0253,  0.0248, -0.0410],\n",
      "        [-0.0351, -0.0050,  0.0143,  ..., -0.0624, -0.0167,  0.0554],\n",
      "        [-0.0489, -0.0387,  0.0488,  ...,  0.0192, -0.0275,  0.0247]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.7.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.7.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.7.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.8.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0234,  0.0142, -0.0473,  ..., -0.0086, -0.0176, -0.0310],\n",
      "        [ 0.0588,  0.0462,  0.0147,  ...,  0.0152, -0.0082,  0.0324],\n",
      "        [-0.0408,  0.0513, -0.0404,  ..., -0.0293,  0.0408,  0.0070],\n",
      "        ...,\n",
      "        [ 0.0417,  0.0152,  0.0361,  ...,  0.0512, -0.0471, -0.0291],\n",
      "        [ 0.0601,  0.0527, -0.0172,  ...,  0.0338,  0.0025,  0.0465],\n",
      "        [ 0.0350, -0.0535,  0.0495,  ..., -0.0275, -0.0004,  0.0236]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.8.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.8.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0447, -0.0137,  0.0183,  ..., -0.0549,  0.0207, -0.0259],\n",
      "        [ 0.0193, -0.0164, -0.0203,  ..., -0.0287, -0.0338,  0.0455],\n",
      "        [-0.0097,  0.0589,  0.0516,  ...,  0.0283, -0.0470,  0.0278],\n",
      "        ...,\n",
      "        [-0.0497,  0.0316,  0.0430,  ..., -0.0468, -0.0522, -0.0269],\n",
      "        [ 0.0153, -0.0267,  0.0327,  ..., -0.0151, -0.0120,  0.0224],\n",
      "        [ 0.0484, -0.0481, -0.0381,  ..., -0.0059,  0.0453, -0.0054]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.8.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.8.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.8.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.9.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0046, -0.0006,  0.0354,  ...,  0.0123,  0.0420, -0.0146],\n",
      "        [ 0.0625,  0.0582,  0.0355,  ..., -0.0243,  0.0161,  0.0073],\n",
      "        [-0.0595, -0.0490,  0.0199,  ..., -0.0578,  0.0431, -0.0139],\n",
      "        ...,\n",
      "        [ 0.0171, -0.0504,  0.0458,  ...,  0.0161,  0.0414,  0.0140],\n",
      "        [ 0.0428,  0.0604,  0.0346,  ..., -0.0093, -0.0082,  0.0339],\n",
      "        [-0.0398, -0.0289, -0.0439,  ...,  0.0580,  0.0310,  0.0221]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.9.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.9.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0131,  0.0487,  0.0307,  ..., -0.0244, -0.0303, -0.0546],\n",
      "        [ 0.0599,  0.0524,  0.0553,  ...,  0.0509, -0.0123, -0.0334],\n",
      "        [ 0.0076, -0.0121,  0.0235,  ..., -0.0098,  0.0046,  0.0257],\n",
      "        ...,\n",
      "        [ 0.0211, -0.0542, -0.0059,  ..., -0.0185, -0.0257, -0.0284],\n",
      "        [-0.0591, -0.0169, -0.0273,  ...,  0.0560, -0.0082, -0.0472],\n",
      "        [ 0.0441, -0.0369,  0.0339,  ..., -0.0057, -0.0401, -0.0383]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.9.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.9.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.9.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.10.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0442, -0.0493,  0.0338,  ..., -0.0247, -0.0158, -0.0545],\n",
      "        [ 0.0106, -0.0483, -0.0584,  ...,  0.0132, -0.0321, -0.0615],\n",
      "        [ 0.0547,  0.0357, -0.0139,  ..., -0.0480,  0.0014,  0.0118],\n",
      "        ...,\n",
      "        [-0.0482,  0.0301, -0.0330,  ..., -0.0481, -0.0403, -0.0433],\n",
      "        [-0.0494, -0.0391, -0.0050,  ...,  0.0119,  0.0115,  0.0295],\n",
      "        [ 0.0618,  0.0164, -0.0515,  ...,  0.0078,  0.0349,  0.0594]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.10.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.10.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0482, -0.0051, -0.0619,  ...,  0.0477, -0.0558,  0.0290],\n",
      "        [-0.0545,  0.0347, -0.0216,  ...,  0.0122, -0.0478,  0.0360],\n",
      "        [ 0.0276, -0.0030,  0.0393,  ...,  0.0428, -0.0159,  0.0610],\n",
      "        ...,\n",
      "        [ 0.0530, -0.0248,  0.0013,  ..., -0.0574, -0.0450,  0.0034],\n",
      "        [-0.0014, -0.0466, -0.0442,  ...,  0.0421,  0.0620, -0.0612],\n",
      "        [ 0.0516, -0.0269,  0.0525,  ...,  0.0341,  0.0537, -0.0005]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.10.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.10.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.10.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.11.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0232, -0.0240, -0.0134,  ...,  0.0246,  0.0607,  0.0281],\n",
      "        [ 0.0082,  0.0599, -0.0347,  ..., -0.0048, -0.0247, -0.0122],\n",
      "        [-0.0240, -0.0222, -0.0486,  ...,  0.0495,  0.0082,  0.0488],\n",
      "        ...,\n",
      "        [ 0.0369,  0.0620,  0.0594,  ...,  0.0483, -0.0024,  0.0296],\n",
      "        [-0.0435,  0.0476,  0.0608,  ..., -0.0220,  0.0263,  0.0298],\n",
      "        [ 0.0598,  0.0246, -0.0595,  ...,  0.0056, -0.0124,  0.0031]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.11.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.11.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0488, -0.0013,  0.0227,  ...,  0.0057, -0.0271, -0.0359],\n",
      "        [ 0.0467, -0.0204, -0.0321,  ..., -0.0419, -0.0450,  0.0483],\n",
      "        [ 0.0315, -0.0292, -0.0608,  ...,  0.0362, -0.0356, -0.0135],\n",
      "        ...,\n",
      "        [ 0.0192,  0.0231, -0.0538,  ..., -0.0440,  0.0062,  0.0588],\n",
      "        [-0.0319, -0.0047, -0.0337,  ...,  0.0076, -0.0120,  0.0076],\n",
      "        [-0.0555, -0.0165, -0.0059,  ...,  0.0578,  0.0123,  0.0528]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.11.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.11.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.11.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.12.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0237,  0.0307, -0.0016,  ..., -0.0084, -0.0137,  0.0021],\n",
      "        [ 0.0047,  0.0053,  0.0011,  ...,  0.0139,  0.0440,  0.0133],\n",
      "        [ 0.0315, -0.0548, -0.0499,  ...,  0.0293, -0.0105, -0.0313],\n",
      "        ...,\n",
      "        [ 0.0322,  0.0157, -0.0278,  ..., -0.0157, -0.0150,  0.0434],\n",
      "        [ 0.0215,  0.0443, -0.0258,  ..., -0.0172, -0.0111, -0.0114],\n",
      "        [ 0.0602,  0.0284, -0.0092,  ...,  0.0588, -0.0124, -0.0365]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.12.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.12.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0525,  0.0403,  0.0591,  ...,  0.0071,  0.0077,  0.0384],\n",
      "        [-0.0129,  0.0300,  0.0040,  ...,  0.0505, -0.0358, -0.0354],\n",
      "        [-0.0537, -0.0007,  0.0607,  ...,  0.0161, -0.0247,  0.0164],\n",
      "        ...,\n",
      "        [-0.0210,  0.0283, -0.0602,  ...,  0.0311,  0.0217, -0.0450],\n",
      "        [-0.0402,  0.0609,  0.0109,  ...,  0.0345, -0.0078,  0.0166],\n",
      "        [-0.0329, -0.0255,  0.0560,  ...,  0.0015, -0.0562, -0.0571]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.12.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.12.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.12.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.13.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0066, -0.0182,  0.0444,  ..., -0.0494, -0.0126, -0.0425],\n",
      "        [ 0.0256,  0.0449,  0.0614,  ..., -0.0039, -0.0462, -0.0303],\n",
      "        [ 0.0417,  0.0456,  0.0091,  ...,  0.0616,  0.0474, -0.0050],\n",
      "        ...,\n",
      "        [-0.0524, -0.0158, -0.0577,  ...,  0.0166,  0.0010, -0.0090],\n",
      "        [ 0.0219, -0.0092, -0.0421,  ...,  0.0335, -0.0127,  0.0451],\n",
      "        [ 0.0030, -0.0139, -0.0328,  ..., -0.0297, -0.0316,  0.0373]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.13.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.13.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0015, -0.0564,  0.0337,  ..., -0.0028, -0.0449, -0.0065],\n",
      "        [ 0.0366,  0.0453, -0.0610,  ..., -0.0161,  0.0260, -0.0046],\n",
      "        [-0.0174,  0.0013,  0.0124,  ...,  0.0294, -0.0224,  0.0007],\n",
      "        ...,\n",
      "        [-0.0266,  0.0126,  0.0114,  ...,  0.0256, -0.0065, -0.0380],\n",
      "        [-0.0400, -0.0143, -0.0375,  ..., -0.0599,  0.0191, -0.0290],\n",
      "        [ 0.0620,  0.0507, -0.0178,  ..., -0.0512, -0.0070,  0.0088]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.13.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.13.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.13.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.14.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0282,  0.0226,  0.0499,  ...,  0.0608,  0.0022,  0.0449],\n",
      "        [-0.0183,  0.0601, -0.0517,  ..., -0.0302,  0.0427,  0.0073],\n",
      "        [ 0.0513,  0.0112, -0.0188,  ..., -0.0493,  0.0594, -0.0128],\n",
      "        ...,\n",
      "        [-0.0289, -0.0534, -0.0106,  ...,  0.0102,  0.0562,  0.0067],\n",
      "        [-0.0342, -0.0470, -0.0113,  ...,  0.0606,  0.0574, -0.0453],\n",
      "        [ 0.0225, -0.0436,  0.0428,  ..., -0.0535, -0.0586, -0.0151]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.14.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.14.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0408, -0.0106, -0.0547,  ...,  0.0520,  0.0245, -0.0016],\n",
      "        [ 0.0265, -0.0063, -0.0224,  ..., -0.0360,  0.0039,  0.0038],\n",
      "        [ 0.0162, -0.0454,  0.0090,  ...,  0.0267,  0.0317, -0.0058],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0499, -0.0156,  ..., -0.0452, -0.0047,  0.0245],\n",
      "        [ 0.0369,  0.0420, -0.0201,  ...,  0.0607, -0.0529, -0.0236],\n",
      "        [ 0.0533,  0.0260, -0.0122,  ..., -0.0015, -0.0614, -0.0115]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.14.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.14.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.14.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.15.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0610,  0.0431, -0.0368,  ..., -0.0081,  0.0119, -0.0153],\n",
      "        [ 0.0182,  0.0135,  0.0421,  ..., -0.0043, -0.0032, -0.0053],\n",
      "        [ 0.0608,  0.0378, -0.0434,  ..., -0.0541,  0.0217,  0.0180],\n",
      "        ...,\n",
      "        [-0.0594, -0.0503,  0.0139,  ..., -0.0411,  0.0050, -0.0366],\n",
      "        [ 0.0462, -0.0439,  0.0332,  ..., -0.0596, -0.0207,  0.0300],\n",
      "        [-0.0391,  0.0103,  0.0482,  ...,  0.0355, -0.0130,  0.0345]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.15.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.15.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0563,  0.0563, -0.0536,  ..., -0.0392,  0.0588,  0.0493],\n",
      "        [-0.0151, -0.0620, -0.0142,  ..., -0.0088,  0.0483,  0.0617],\n",
      "        [ 0.0143,  0.0119,  0.0375,  ..., -0.0235, -0.0185, -0.0280],\n",
      "        ...,\n",
      "        [-0.0563,  0.0483,  0.0332,  ...,  0.0597,  0.0534,  0.0046],\n",
      "        [-0.0013,  0.0257,  0.0382,  ..., -0.0161, -0.0345,  0.0407],\n",
      "        [-0.0060,  0.0379, -0.0107,  ..., -0.0571, -0.0475, -0.0303]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.15.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.15.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.15.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.16.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0093,  0.0468,  0.0102,  ...,  0.0403, -0.0113,  0.0161],\n",
      "        [-0.0525, -0.0298, -0.0415,  ..., -0.0296,  0.0437, -0.0555],\n",
      "        [ 0.0151, -0.0566, -0.0365,  ...,  0.0112, -0.0262, -0.0334],\n",
      "        ...,\n",
      "        [-0.0359, -0.0464, -0.0498,  ...,  0.0431,  0.0441, -0.0051],\n",
      "        [ 0.0353, -0.0501,  0.0496,  ..., -0.0326,  0.0318, -0.0274],\n",
      "        [ 0.0282,  0.0319,  0.0102,  ...,  0.0093,  0.0495, -0.0060]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.16.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.16.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0387,  0.0475, -0.0312,  ..., -0.0589, -0.0600, -0.0418],\n",
      "        [ 0.0287,  0.0275,  0.0278,  ...,  0.0479, -0.0148,  0.0265],\n",
      "        [ 0.0572, -0.0116, -0.0494,  ...,  0.0511,  0.0536,  0.0084],\n",
      "        ...,\n",
      "        [ 0.0470,  0.0303,  0.0521,  ...,  0.0031,  0.0153, -0.0547],\n",
      "        [ 0.0503, -0.0592,  0.0360,  ...,  0.0368,  0.0025,  0.0015],\n",
      "        [-0.0178,  0.0593, -0.0301,  ...,  0.0550,  0.0034, -0.0252]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.16.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.16.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.16.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.17.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0088,  0.0080,  0.0326,  ..., -0.0479, -0.0325, -0.0157],\n",
      "        [-0.0238, -0.0131,  0.0331,  ...,  0.0200, -0.0133,  0.0295],\n",
      "        [-0.0039, -0.0362,  0.0342,  ..., -0.0208, -0.0548,  0.0349],\n",
      "        ...,\n",
      "        [-0.0057, -0.0445,  0.0459,  ...,  0.0586, -0.0368, -0.0537],\n",
      "        [-0.0532, -0.0306, -0.0235,  ...,  0.0109,  0.0536, -0.0586],\n",
      "        [ 0.0001,  0.0129,  0.0239,  ...,  0.0304, -0.0288,  0.0565]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.17.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.17.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0083, -0.0151,  0.0322,  ..., -0.0441, -0.0410, -0.0538],\n",
      "        [-0.0302,  0.0060, -0.0153,  ...,  0.0437, -0.0146,  0.0548],\n",
      "        [-0.0560,  0.0248, -0.0015,  ...,  0.0129, -0.0615,  0.0573],\n",
      "        ...,\n",
      "        [-0.0187, -0.0446, -0.0020,  ..., -0.0260,  0.0447, -0.0241],\n",
      "        [-0.0199, -0.0171, -0.0098,  ...,  0.0279,  0.0437, -0.0469],\n",
      "        [-0.0572, -0.0381, -0.0396,  ..., -0.0005,  0.0390,  0.0372]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.17.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.17.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.17.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.18.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0103, -0.0246, -0.0175,  ..., -0.0139, -0.0497,  0.0404],\n",
      "        [-0.0214, -0.0027, -0.0480,  ..., -0.0586, -0.0185, -0.0026],\n",
      "        [-0.0366,  0.0496, -0.0425,  ..., -0.0333, -0.0593,  0.0494],\n",
      "        ...,\n",
      "        [-0.0546,  0.0453, -0.0124,  ..., -0.0007,  0.0128,  0.0441],\n",
      "        [ 0.0109,  0.0064,  0.0411,  ..., -0.0590, -0.0177,  0.0012],\n",
      "        [-0.0604,  0.0103,  0.0388,  ...,  0.0424,  0.0237,  0.0359]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.18.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.18.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0082, -0.0567,  0.0592,  ..., -0.0608, -0.0275, -0.0107],\n",
      "        [-0.0199, -0.0348, -0.0157,  ...,  0.0180,  0.0555,  0.0011],\n",
      "        [ 0.0383, -0.0601, -0.0465,  ..., -0.0216, -0.0365, -0.0150],\n",
      "        ...,\n",
      "        [-0.0483,  0.0451, -0.0435,  ..., -0.0341, -0.0297,  0.0339],\n",
      "        [-0.0147, -0.0129, -0.0310,  ..., -0.0211,  0.0514,  0.0266],\n",
      "        [-0.0085, -0.0462,  0.0621,  ..., -0.0270,  0.0574,  0.0367]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.18.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.18.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.18.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.19.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0310,  0.0371, -0.0444,  ...,  0.0325, -0.0595,  0.0294],\n",
      "        [ 0.0411,  0.0215, -0.0534,  ..., -0.0039,  0.0296,  0.0512],\n",
      "        [-0.0176, -0.0503,  0.0605,  ..., -0.0090,  0.0165, -0.0052],\n",
      "        ...,\n",
      "        [-0.0265, -0.0374,  0.0184,  ..., -0.0416, -0.0346, -0.0488],\n",
      "        [-0.0143,  0.0082, -0.0241,  ...,  0.0332, -0.0050, -0.0088],\n",
      "        [-0.0435, -0.0140, -0.0392,  ..., -0.0074, -0.0346,  0.0618]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.19.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.19.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0231, -0.0172, -0.0162,  ...,  0.0114,  0.0187, -0.0605],\n",
      "        [-0.0198, -0.0087, -0.0030,  ...,  0.0118,  0.0194,  0.0270],\n",
      "        [ 0.0100, -0.0567, -0.0286,  ...,  0.0148, -0.0156, -0.0591],\n",
      "        ...,\n",
      "        [ 0.0301, -0.0155,  0.0602,  ...,  0.0521,  0.0214,  0.0276],\n",
      "        [-0.0366, -0.0349,  0.0376,  ...,  0.0213,  0.0378, -0.0052],\n",
      "        [ 0.0035, -0.0398,  0.0161,  ...,  0.0341, -0.0161, -0.0456]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.19.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.19.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.19.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.20.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0317,  0.0195,  0.0469,  ...,  0.0392,  0.0491, -0.0211],\n",
      "        [ 0.0249, -0.0012, -0.0310,  ...,  0.0108, -0.0049,  0.0280],\n",
      "        [ 0.0437, -0.0551, -0.0563,  ...,  0.0417,  0.0058,  0.0587],\n",
      "        ...,\n",
      "        [ 0.0564,  0.0091, -0.0504,  ...,  0.0594, -0.0048,  0.0579],\n",
      "        [ 0.0053,  0.0105,  0.0582,  ...,  0.0300, -0.0612,  0.0274],\n",
      "        [ 0.0328,  0.0492,  0.0124,  ..., -0.0364,  0.0069,  0.0190]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.20.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.20.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0244,  0.0362,  0.0380,  ...,  0.0116, -0.0458,  0.0292],\n",
      "        [ 0.0030, -0.0530, -0.0414,  ..., -0.0340, -0.0370, -0.0042],\n",
      "        [ 0.0125,  0.0431,  0.0091,  ...,  0.0483, -0.0618,  0.0450],\n",
      "        ...,\n",
      "        [-0.0358,  0.0353, -0.0464,  ..., -0.0428, -0.0209,  0.0518],\n",
      "        [-0.0409,  0.0326,  0.0254,  ..., -0.0034, -0.0384, -0.0286],\n",
      "        [ 0.0381,  0.0072, -0.0334,  ..., -0.0086,  0.0071, -0.0535]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.20.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.20.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.20.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.21.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0044,  0.0237,  0.0014,  ...,  0.0326,  0.0396,  0.0005],\n",
      "        [ 0.0059, -0.0145,  0.0579,  ..., -0.0134, -0.0189,  0.0466],\n",
      "        [ 0.0587, -0.0296, -0.0552,  ..., -0.0201, -0.0149, -0.0033],\n",
      "        ...,\n",
      "        [ 0.0421, -0.0304, -0.0546,  ...,  0.0385, -0.0041, -0.0065],\n",
      "        [-0.0565, -0.0163, -0.0256,  ...,  0.0588,  0.0564, -0.0085],\n",
      "        [-0.0348,  0.0010, -0.0164,  ..., -0.0103, -0.0451, -0.0339]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.21.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.21.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0446,  0.0461,  0.0280,  ...,  0.0181, -0.0566,  0.0466],\n",
      "        [-0.0195, -0.0598, -0.0421,  ..., -0.0130,  0.0408,  0.0129],\n",
      "        [ 0.0546, -0.0567, -0.0241,  ...,  0.0462,  0.0485,  0.0116],\n",
      "        ...,\n",
      "        [-0.0478, -0.0184,  0.0475,  ...,  0.0204,  0.0332,  0.0465],\n",
      "        [-0.0591, -0.0203, -0.0407,  ...,  0.0043, -0.0586,  0.0233],\n",
      "        [ 0.0260,  0.0376,  0.0362,  ...,  0.0583, -0.0289, -0.0485]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.21.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.21.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.21.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.22.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0610,  0.0056, -0.0370,  ...,  0.0439,  0.0150,  0.0151],\n",
      "        [ 0.0575,  0.0074, -0.0215,  ...,  0.0184,  0.0545,  0.0074],\n",
      "        [ 0.0493,  0.0260, -0.0445,  ..., -0.0399, -0.0389, -0.0562],\n",
      "        ...,\n",
      "        [-0.0476, -0.0009,  0.0225,  ..., -0.0448,  0.0430, -0.0173],\n",
      "        [ 0.0431,  0.0304, -0.0462,  ...,  0.0085,  0.0070,  0.0293],\n",
      "        [ 0.0169,  0.0173,  0.0452,  ..., -0.0208,  0.0402, -0.0326]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.22.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.22.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0505, -0.0546, -0.0368,  ...,  0.0557,  0.0307,  0.0054],\n",
      "        [ 0.0218, -0.0268, -0.0152,  ..., -0.0427,  0.0290,  0.0107],\n",
      "        [-0.0036, -0.0468,  0.0314,  ..., -0.0548, -0.0057, -0.0287],\n",
      "        ...,\n",
      "        [ 0.0256,  0.0414,  0.0539,  ..., -0.0138, -0.0025,  0.0012],\n",
      "        [ 0.0228, -0.0205, -0.0394,  ..., -0.0609,  0.0474, -0.0186],\n",
      "        [ 0.0554, -0.0169,  0.0036,  ...,  0.0461,  0.0385, -0.0067]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.22.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.22.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.22.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.23.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0237,  0.0221, -0.0206,  ...,  0.0356, -0.0394,  0.0212],\n",
      "        [ 0.0073,  0.0308, -0.0609,  ...,  0.0183, -0.0003,  0.0264],\n",
      "        [-0.0205, -0.0430,  0.0262,  ...,  0.0605, -0.0523,  0.0236],\n",
      "        ...,\n",
      "        [ 0.0078,  0.0340, -0.0524,  ..., -0.0591,  0.0620,  0.0059],\n",
      "        [-0.0581, -0.0550, -0.0442,  ..., -0.0178, -0.0549,  0.0275],\n",
      "        [-0.0310,  0.0078,  0.0188,  ...,  0.0222, -0.0335, -0.0197]],\n",
      "       requires_grad=True)\n",
      "Found: encoder.layer.23.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.23.up_layer.weight Parameter containing:\n",
      "tensor([[ 5.5068e-02,  2.3651e-02,  3.0950e-02,  ...,  4.6733e-02,\n",
      "         -6.7639e-04,  4.3510e-02],\n",
      "        [ 1.8990e-04,  2.5202e-02, -2.2792e-02,  ...,  3.5340e-02,\n",
      "          1.1517e-03,  4.7475e-05],\n",
      "        [-1.9861e-03, -3.3747e-02,  5.2966e-02,  ...,  1.8917e-02,\n",
      "          6.1463e-02,  5.8510e-02],\n",
      "        ...,\n",
      "        [-5.1673e-02,  6.0791e-02,  5.0282e-02,  ...,  1.6379e-02,\n",
      "         -5.6059e-02, -6.0859e-02],\n",
      "        [ 5.6710e-02, -5.4406e-02, -1.1719e-02,  ..., -3.1995e-03,\n",
      "          4.9687e-02,  5.3884e-02],\n",
      "        [-1.6980e-02, -2.2287e-03, -6.1641e-02,  ..., -5.0798e-02,\n",
      "          4.7705e-02, -2.8067e-02]], requires_grad=True)\n",
      "Found: encoder.layer.23.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: encoder.layer.23.layer_norm_new.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Found: encoder.layer.23.layer_norm_new.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Found: classifier.0.weight Parameter containing:\n",
      "tensor([[ 0.0227,  0.0125,  0.0289,  ...,  0.0064, -0.0251,  0.0073],\n",
      "        [ 0.0009,  0.0053, -0.0231,  ..., -0.0035, -0.0138, -0.0155],\n",
      "        [ 0.0127, -0.0073, -0.0139,  ..., -0.0312,  0.0119, -0.0238],\n",
      "        ...,\n",
      "        [-0.0275,  0.0004, -0.0108,  ...,  0.0049, -0.0032,  0.0189],\n",
      "        [ 0.0157,  0.0270, -0.0225,  ..., -0.0014,  0.0237, -0.0117],\n",
      "        [-0.0173,  0.0012,  0.0032,  ...,  0.0071, -0.0105, -0.0312]],\n",
      "       requires_grad=True)\n",
      "Found: classifier.0.bias Parameter containing:\n",
      "tensor([ 0.0246, -0.0175,  0.0195,  ..., -0.0208,  0.0061, -0.0085],\n",
      "       requires_grad=True)\n",
      "Found: classifier.2.weight Parameter containing:\n",
      "tensor([[-0.0240, -0.0101, -0.0226,  ...,  0.0306,  0.0015,  0.0020],\n",
      "        [-0.0266,  0.0233, -0.0007,  ...,  0.0163, -0.0113,  0.0257]],\n",
      "       requires_grad=True)\n",
      "Found: classifier.2.bias Parameter containing:\n",
      "tensor([0.0260, 0.0028], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# Load configuration\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomRobertaModel(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_model = RobertaModel.from_pretrained(model_name)\n",
    "original_weights = pretrained_model.state_dict()\n",
    "\n",
    "custom_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear) and getattr(module, \"_is_new\", False):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "custom_model.apply(initialize_weights)\n",
    "\n",
    "# Compare weights\n",
    "for name, param in custom_model.named_parameters():\n",
    "    if any(keyword in name for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm\",]):\n",
    "        print(\"Found:\", name, param)\n",
    "    # if name in original_weights:\n",
    "    #     if not torch.equal(param, original_weights[name]):\n",
    "    #         print(f\"Layer {name} weights were modified.\")\n",
    "    #     else:\n",
    "    #         print(f\"Layer {name, param} weights are unchanged.\")\n",
    "    # else:\n",
    "    #     print(\"Newly Added:\", name, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b12c0b7-2700-4229-b8c7-a1c6bba5b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old: embeddings.word_embeddings.weight\n",
      "Old: embeddings.position_embeddings.weight\n",
      "Old: embeddings.token_type_embeddings.weight\n",
      "Old: embeddings.LayerNorm.weight\n",
      "Old: embeddings.LayerNorm.bias\n",
      "Old: encoder.layer.0.attention.self.query.weight\n",
      "Old: encoder.layer.0.attention.self.query.bias\n",
      "Old: encoder.layer.0.attention.self.key.weight\n",
      "Old: encoder.layer.0.attention.self.key.bias\n",
      "Old: encoder.layer.0.attention.self.value.weight\n",
      "Old: encoder.layer.0.attention.self.value.bias\n",
      "Old: encoder.layer.0.attention.output.dense.weight\n",
      "Old: encoder.layer.0.attention.output.dense.bias\n",
      "Old: encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.0.intermediate.dense.weight\n",
      "Old: encoder.layer.0.intermediate.dense.bias\n",
      "Old: encoder.layer.0.output.dense.weight\n",
      "Old: encoder.layer.0.output.dense.bias\n",
      "Old: encoder.layer.0.output.LayerNorm.weight\n",
      "Old: encoder.layer.0.output.LayerNorm.bias\n",
      "New: encoder.layer.0.down_layer.weight\n",
      "New: encoder.layer.0.down_layer.bias\n",
      "New: encoder.layer.0.up_layer.weight\n",
      "New: encoder.layer.0.up_layer.bias\n",
      "New: encoder.layer.0.layer_norm_new.weight\n",
      "New: encoder.layer.0.layer_norm_new.bias\n",
      "Old: encoder.layer.1.attention.self.query.weight\n",
      "Old: encoder.layer.1.attention.self.query.bias\n",
      "Old: encoder.layer.1.attention.self.key.weight\n",
      "Old: encoder.layer.1.attention.self.key.bias\n",
      "Old: encoder.layer.1.attention.self.value.weight\n",
      "Old: encoder.layer.1.attention.self.value.bias\n",
      "Old: encoder.layer.1.attention.output.dense.weight\n",
      "Old: encoder.layer.1.attention.output.dense.bias\n",
      "Old: encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.1.intermediate.dense.weight\n",
      "Old: encoder.layer.1.intermediate.dense.bias\n",
      "Old: encoder.layer.1.output.dense.weight\n",
      "Old: encoder.layer.1.output.dense.bias\n",
      "Old: encoder.layer.1.output.LayerNorm.weight\n",
      "Old: encoder.layer.1.output.LayerNorm.bias\n",
      "New: encoder.layer.1.down_layer.weight\n",
      "New: encoder.layer.1.down_layer.bias\n",
      "New: encoder.layer.1.up_layer.weight\n",
      "New: encoder.layer.1.up_layer.bias\n",
      "New: encoder.layer.1.layer_norm_new.weight\n",
      "New: encoder.layer.1.layer_norm_new.bias\n",
      "Old: encoder.layer.2.attention.self.query.weight\n",
      "Old: encoder.layer.2.attention.self.query.bias\n",
      "Old: encoder.layer.2.attention.self.key.weight\n",
      "Old: encoder.layer.2.attention.self.key.bias\n",
      "Old: encoder.layer.2.attention.self.value.weight\n",
      "Old: encoder.layer.2.attention.self.value.bias\n",
      "Old: encoder.layer.2.attention.output.dense.weight\n",
      "Old: encoder.layer.2.attention.output.dense.bias\n",
      "Old: encoder.layer.2.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.2.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.2.intermediate.dense.weight\n",
      "Old: encoder.layer.2.intermediate.dense.bias\n",
      "Old: encoder.layer.2.output.dense.weight\n",
      "Old: encoder.layer.2.output.dense.bias\n",
      "Old: encoder.layer.2.output.LayerNorm.weight\n",
      "Old: encoder.layer.2.output.LayerNorm.bias\n",
      "New: encoder.layer.2.down_layer.weight\n",
      "New: encoder.layer.2.down_layer.bias\n",
      "New: encoder.layer.2.up_layer.weight\n",
      "New: encoder.layer.2.up_layer.bias\n",
      "New: encoder.layer.2.layer_norm_new.weight\n",
      "New: encoder.layer.2.layer_norm_new.bias\n",
      "Old: encoder.layer.3.attention.self.query.weight\n",
      "Old: encoder.layer.3.attention.self.query.bias\n",
      "Old: encoder.layer.3.attention.self.key.weight\n",
      "Old: encoder.layer.3.attention.self.key.bias\n",
      "Old: encoder.layer.3.attention.self.value.weight\n",
      "Old: encoder.layer.3.attention.self.value.bias\n",
      "Old: encoder.layer.3.attention.output.dense.weight\n",
      "Old: encoder.layer.3.attention.output.dense.bias\n",
      "Old: encoder.layer.3.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.3.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.3.intermediate.dense.weight\n",
      "Old: encoder.layer.3.intermediate.dense.bias\n",
      "Old: encoder.layer.3.output.dense.weight\n",
      "Old: encoder.layer.3.output.dense.bias\n",
      "Old: encoder.layer.3.output.LayerNorm.weight\n",
      "Old: encoder.layer.3.output.LayerNorm.bias\n",
      "New: encoder.layer.3.down_layer.weight\n",
      "New: encoder.layer.3.down_layer.bias\n",
      "New: encoder.layer.3.up_layer.weight\n",
      "New: encoder.layer.3.up_layer.bias\n",
      "New: encoder.layer.3.layer_norm_new.weight\n",
      "New: encoder.layer.3.layer_norm_new.bias\n",
      "Old: encoder.layer.4.attention.self.query.weight\n",
      "Old: encoder.layer.4.attention.self.query.bias\n",
      "Old: encoder.layer.4.attention.self.key.weight\n",
      "Old: encoder.layer.4.attention.self.key.bias\n",
      "Old: encoder.layer.4.attention.self.value.weight\n",
      "Old: encoder.layer.4.attention.self.value.bias\n",
      "Old: encoder.layer.4.attention.output.dense.weight\n",
      "Old: encoder.layer.4.attention.output.dense.bias\n",
      "Old: encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.4.intermediate.dense.weight\n",
      "Old: encoder.layer.4.intermediate.dense.bias\n",
      "Old: encoder.layer.4.output.dense.weight\n",
      "Old: encoder.layer.4.output.dense.bias\n",
      "Old: encoder.layer.4.output.LayerNorm.weight\n",
      "Old: encoder.layer.4.output.LayerNorm.bias\n",
      "New: encoder.layer.4.down_layer.weight\n",
      "New: encoder.layer.4.down_layer.bias\n",
      "New: encoder.layer.4.up_layer.weight\n",
      "New: encoder.layer.4.up_layer.bias\n",
      "New: encoder.layer.4.layer_norm_new.weight\n",
      "New: encoder.layer.4.layer_norm_new.bias\n",
      "Old: encoder.layer.5.attention.self.query.weight\n",
      "Old: encoder.layer.5.attention.self.query.bias\n",
      "Old: encoder.layer.5.attention.self.key.weight\n",
      "Old: encoder.layer.5.attention.self.key.bias\n",
      "Old: encoder.layer.5.attention.self.value.weight\n",
      "Old: encoder.layer.5.attention.self.value.bias\n",
      "Old: encoder.layer.5.attention.output.dense.weight\n",
      "Old: encoder.layer.5.attention.output.dense.bias\n",
      "Old: encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.5.intermediate.dense.weight\n",
      "Old: encoder.layer.5.intermediate.dense.bias\n",
      "Old: encoder.layer.5.output.dense.weight\n",
      "Old: encoder.layer.5.output.dense.bias\n",
      "Old: encoder.layer.5.output.LayerNorm.weight\n",
      "Old: encoder.layer.5.output.LayerNorm.bias\n",
      "New: encoder.layer.5.down_layer.weight\n",
      "New: encoder.layer.5.down_layer.bias\n",
      "New: encoder.layer.5.up_layer.weight\n",
      "New: encoder.layer.5.up_layer.bias\n",
      "New: encoder.layer.5.layer_norm_new.weight\n",
      "New: encoder.layer.5.layer_norm_new.bias\n",
      "Old: encoder.layer.6.attention.self.query.weight\n",
      "Old: encoder.layer.6.attention.self.query.bias\n",
      "Old: encoder.layer.6.attention.self.key.weight\n",
      "Old: encoder.layer.6.attention.self.key.bias\n",
      "Old: encoder.layer.6.attention.self.value.weight\n",
      "Old: encoder.layer.6.attention.self.value.bias\n",
      "Old: encoder.layer.6.attention.output.dense.weight\n",
      "Old: encoder.layer.6.attention.output.dense.bias\n",
      "Old: encoder.layer.6.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.6.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.6.intermediate.dense.weight\n",
      "Old: encoder.layer.6.intermediate.dense.bias\n",
      "Old: encoder.layer.6.output.dense.weight\n",
      "Old: encoder.layer.6.output.dense.bias\n",
      "Old: encoder.layer.6.output.LayerNorm.weight\n",
      "Old: encoder.layer.6.output.LayerNorm.bias\n",
      "New: encoder.layer.6.down_layer.weight\n",
      "New: encoder.layer.6.down_layer.bias\n",
      "New: encoder.layer.6.up_layer.weight\n",
      "New: encoder.layer.6.up_layer.bias\n",
      "New: encoder.layer.6.layer_norm_new.weight\n",
      "New: encoder.layer.6.layer_norm_new.bias\n",
      "Old: encoder.layer.7.attention.self.query.weight\n",
      "Old: encoder.layer.7.attention.self.query.bias\n",
      "Old: encoder.layer.7.attention.self.key.weight\n",
      "Old: encoder.layer.7.attention.self.key.bias\n",
      "Old: encoder.layer.7.attention.self.value.weight\n",
      "Old: encoder.layer.7.attention.self.value.bias\n",
      "Old: encoder.layer.7.attention.output.dense.weight\n",
      "Old: encoder.layer.7.attention.output.dense.bias\n",
      "Old: encoder.layer.7.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.7.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.7.intermediate.dense.weight\n",
      "Old: encoder.layer.7.intermediate.dense.bias\n",
      "Old: encoder.layer.7.output.dense.weight\n",
      "Old: encoder.layer.7.output.dense.bias\n",
      "Old: encoder.layer.7.output.LayerNorm.weight\n",
      "Old: encoder.layer.7.output.LayerNorm.bias\n",
      "New: encoder.layer.7.down_layer.weight\n",
      "New: encoder.layer.7.down_layer.bias\n",
      "New: encoder.layer.7.up_layer.weight\n",
      "New: encoder.layer.7.up_layer.bias\n",
      "New: encoder.layer.7.layer_norm_new.weight\n",
      "New: encoder.layer.7.layer_norm_new.bias\n",
      "Old: encoder.layer.8.attention.self.query.weight\n",
      "Old: encoder.layer.8.attention.self.query.bias\n",
      "Old: encoder.layer.8.attention.self.key.weight\n",
      "Old: encoder.layer.8.attention.self.key.bias\n",
      "Old: encoder.layer.8.attention.self.value.weight\n",
      "Old: encoder.layer.8.attention.self.value.bias\n",
      "Old: encoder.layer.8.attention.output.dense.weight\n",
      "Old: encoder.layer.8.attention.output.dense.bias\n",
      "Old: encoder.layer.8.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.8.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.8.intermediate.dense.weight\n",
      "Old: encoder.layer.8.intermediate.dense.bias\n",
      "Old: encoder.layer.8.output.dense.weight\n",
      "Old: encoder.layer.8.output.dense.bias\n",
      "Old: encoder.layer.8.output.LayerNorm.weight\n",
      "Old: encoder.layer.8.output.LayerNorm.bias\n",
      "New: encoder.layer.8.down_layer.weight\n",
      "New: encoder.layer.8.down_layer.bias\n",
      "New: encoder.layer.8.up_layer.weight\n",
      "New: encoder.layer.8.up_layer.bias\n",
      "New: encoder.layer.8.layer_norm_new.weight\n",
      "New: encoder.layer.8.layer_norm_new.bias\n",
      "Old: encoder.layer.9.attention.self.query.weight\n",
      "Old: encoder.layer.9.attention.self.query.bias\n",
      "Old: encoder.layer.9.attention.self.key.weight\n",
      "Old: encoder.layer.9.attention.self.key.bias\n",
      "Old: encoder.layer.9.attention.self.value.weight\n",
      "Old: encoder.layer.9.attention.self.value.bias\n",
      "Old: encoder.layer.9.attention.output.dense.weight\n",
      "Old: encoder.layer.9.attention.output.dense.bias\n",
      "Old: encoder.layer.9.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.9.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.9.intermediate.dense.weight\n",
      "Old: encoder.layer.9.intermediate.dense.bias\n",
      "Old: encoder.layer.9.output.dense.weight\n",
      "Old: encoder.layer.9.output.dense.bias\n",
      "Old: encoder.layer.9.output.LayerNorm.weight\n",
      "Old: encoder.layer.9.output.LayerNorm.bias\n",
      "New: encoder.layer.9.down_layer.weight\n",
      "New: encoder.layer.9.down_layer.bias\n",
      "New: encoder.layer.9.up_layer.weight\n",
      "New: encoder.layer.9.up_layer.bias\n",
      "New: encoder.layer.9.layer_norm_new.weight\n",
      "New: encoder.layer.9.layer_norm_new.bias\n",
      "Old: encoder.layer.10.attention.self.query.weight\n",
      "Old: encoder.layer.10.attention.self.query.bias\n",
      "Old: encoder.layer.10.attention.self.key.weight\n",
      "Old: encoder.layer.10.attention.self.key.bias\n",
      "Old: encoder.layer.10.attention.self.value.weight\n",
      "Old: encoder.layer.10.attention.self.value.bias\n",
      "Old: encoder.layer.10.attention.output.dense.weight\n",
      "Old: encoder.layer.10.attention.output.dense.bias\n",
      "Old: encoder.layer.10.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.10.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.10.intermediate.dense.weight\n",
      "Old: encoder.layer.10.intermediate.dense.bias\n",
      "Old: encoder.layer.10.output.dense.weight\n",
      "Old: encoder.layer.10.output.dense.bias\n",
      "Old: encoder.layer.10.output.LayerNorm.weight\n",
      "Old: encoder.layer.10.output.LayerNorm.bias\n",
      "New: encoder.layer.10.down_layer.weight\n",
      "New: encoder.layer.10.down_layer.bias\n",
      "New: encoder.layer.10.up_layer.weight\n",
      "New: encoder.layer.10.up_layer.bias\n",
      "New: encoder.layer.10.layer_norm_new.weight\n",
      "New: encoder.layer.10.layer_norm_new.bias\n",
      "Old: encoder.layer.11.attention.self.query.weight\n",
      "Old: encoder.layer.11.attention.self.query.bias\n",
      "Old: encoder.layer.11.attention.self.key.weight\n",
      "Old: encoder.layer.11.attention.self.key.bias\n",
      "Old: encoder.layer.11.attention.self.value.weight\n",
      "Old: encoder.layer.11.attention.self.value.bias\n",
      "Old: encoder.layer.11.attention.output.dense.weight\n",
      "Old: encoder.layer.11.attention.output.dense.bias\n",
      "Old: encoder.layer.11.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.11.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.11.intermediate.dense.weight\n",
      "Old: encoder.layer.11.intermediate.dense.bias\n",
      "Old: encoder.layer.11.output.dense.weight\n",
      "Old: encoder.layer.11.output.dense.bias\n",
      "Old: encoder.layer.11.output.LayerNorm.weight\n",
      "Old: encoder.layer.11.output.LayerNorm.bias\n",
      "New: encoder.layer.11.down_layer.weight\n",
      "New: encoder.layer.11.down_layer.bias\n",
      "New: encoder.layer.11.up_layer.weight\n",
      "New: encoder.layer.11.up_layer.bias\n",
      "New: encoder.layer.11.layer_norm_new.weight\n",
      "New: encoder.layer.11.layer_norm_new.bias\n",
      "Old: encoder.layer.12.attention.self.query.weight\n",
      "Old: encoder.layer.12.attention.self.query.bias\n",
      "Old: encoder.layer.12.attention.self.key.weight\n",
      "Old: encoder.layer.12.attention.self.key.bias\n",
      "Old: encoder.layer.12.attention.self.value.weight\n",
      "Old: encoder.layer.12.attention.self.value.bias\n",
      "Old: encoder.layer.12.attention.output.dense.weight\n",
      "Old: encoder.layer.12.attention.output.dense.bias\n",
      "Old: encoder.layer.12.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.12.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.12.intermediate.dense.weight\n",
      "Old: encoder.layer.12.intermediate.dense.bias\n",
      "Old: encoder.layer.12.output.dense.weight\n",
      "Old: encoder.layer.12.output.dense.bias\n",
      "Old: encoder.layer.12.output.LayerNorm.weight\n",
      "Old: encoder.layer.12.output.LayerNorm.bias\n",
      "New: encoder.layer.12.down_layer.weight\n",
      "New: encoder.layer.12.down_layer.bias\n",
      "New: encoder.layer.12.up_layer.weight\n",
      "New: encoder.layer.12.up_layer.bias\n",
      "New: encoder.layer.12.layer_norm_new.weight\n",
      "New: encoder.layer.12.layer_norm_new.bias\n",
      "Old: encoder.layer.13.attention.self.query.weight\n",
      "Old: encoder.layer.13.attention.self.query.bias\n",
      "Old: encoder.layer.13.attention.self.key.weight\n",
      "Old: encoder.layer.13.attention.self.key.bias\n",
      "Old: encoder.layer.13.attention.self.value.weight\n",
      "Old: encoder.layer.13.attention.self.value.bias\n",
      "Old: encoder.layer.13.attention.output.dense.weight\n",
      "Old: encoder.layer.13.attention.output.dense.bias\n",
      "Old: encoder.layer.13.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.13.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.13.intermediate.dense.weight\n",
      "Old: encoder.layer.13.intermediate.dense.bias\n",
      "Old: encoder.layer.13.output.dense.weight\n",
      "Old: encoder.layer.13.output.dense.bias\n",
      "Old: encoder.layer.13.output.LayerNorm.weight\n",
      "Old: encoder.layer.13.output.LayerNorm.bias\n",
      "New: encoder.layer.13.down_layer.weight\n",
      "New: encoder.layer.13.down_layer.bias\n",
      "New: encoder.layer.13.up_layer.weight\n",
      "New: encoder.layer.13.up_layer.bias\n",
      "New: encoder.layer.13.layer_norm_new.weight\n",
      "New: encoder.layer.13.layer_norm_new.bias\n",
      "Old: encoder.layer.14.attention.self.query.weight\n",
      "Old: encoder.layer.14.attention.self.query.bias\n",
      "Old: encoder.layer.14.attention.self.key.weight\n",
      "Old: encoder.layer.14.attention.self.key.bias\n",
      "Old: encoder.layer.14.attention.self.value.weight\n",
      "Old: encoder.layer.14.attention.self.value.bias\n",
      "Old: encoder.layer.14.attention.output.dense.weight\n",
      "Old: encoder.layer.14.attention.output.dense.bias\n",
      "Old: encoder.layer.14.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.14.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.14.intermediate.dense.weight\n",
      "Old: encoder.layer.14.intermediate.dense.bias\n",
      "Old: encoder.layer.14.output.dense.weight\n",
      "Old: encoder.layer.14.output.dense.bias\n",
      "Old: encoder.layer.14.output.LayerNorm.weight\n",
      "Old: encoder.layer.14.output.LayerNorm.bias\n",
      "New: encoder.layer.14.down_layer.weight\n",
      "New: encoder.layer.14.down_layer.bias\n",
      "New: encoder.layer.14.up_layer.weight\n",
      "New: encoder.layer.14.up_layer.bias\n",
      "New: encoder.layer.14.layer_norm_new.weight\n",
      "New: encoder.layer.14.layer_norm_new.bias\n",
      "Old: encoder.layer.15.attention.self.query.weight\n",
      "Old: encoder.layer.15.attention.self.query.bias\n",
      "Old: encoder.layer.15.attention.self.key.weight\n",
      "Old: encoder.layer.15.attention.self.key.bias\n",
      "Old: encoder.layer.15.attention.self.value.weight\n",
      "Old: encoder.layer.15.attention.self.value.bias\n",
      "Old: encoder.layer.15.attention.output.dense.weight\n",
      "Old: encoder.layer.15.attention.output.dense.bias\n",
      "Old: encoder.layer.15.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.15.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.15.intermediate.dense.weight\n",
      "Old: encoder.layer.15.intermediate.dense.bias\n",
      "Old: encoder.layer.15.output.dense.weight\n",
      "Old: encoder.layer.15.output.dense.bias\n",
      "Old: encoder.layer.15.output.LayerNorm.weight\n",
      "Old: encoder.layer.15.output.LayerNorm.bias\n",
      "New: encoder.layer.15.down_layer.weight\n",
      "New: encoder.layer.15.down_layer.bias\n",
      "New: encoder.layer.15.up_layer.weight\n",
      "New: encoder.layer.15.up_layer.bias\n",
      "New: encoder.layer.15.layer_norm_new.weight\n",
      "New: encoder.layer.15.layer_norm_new.bias\n",
      "Old: encoder.layer.16.attention.self.query.weight\n",
      "Old: encoder.layer.16.attention.self.query.bias\n",
      "Old: encoder.layer.16.attention.self.key.weight\n",
      "Old: encoder.layer.16.attention.self.key.bias\n",
      "Old: encoder.layer.16.attention.self.value.weight\n",
      "Old: encoder.layer.16.attention.self.value.bias\n",
      "Old: encoder.layer.16.attention.output.dense.weight\n",
      "Old: encoder.layer.16.attention.output.dense.bias\n",
      "Old: encoder.layer.16.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.16.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.16.intermediate.dense.weight\n",
      "Old: encoder.layer.16.intermediate.dense.bias\n",
      "Old: encoder.layer.16.output.dense.weight\n",
      "Old: encoder.layer.16.output.dense.bias\n",
      "Old: encoder.layer.16.output.LayerNorm.weight\n",
      "Old: encoder.layer.16.output.LayerNorm.bias\n",
      "New: encoder.layer.16.down_layer.weight\n",
      "New: encoder.layer.16.down_layer.bias\n",
      "New: encoder.layer.16.up_layer.weight\n",
      "New: encoder.layer.16.up_layer.bias\n",
      "New: encoder.layer.16.layer_norm_new.weight\n",
      "New: encoder.layer.16.layer_norm_new.bias\n",
      "Old: encoder.layer.17.attention.self.query.weight\n",
      "Old: encoder.layer.17.attention.self.query.bias\n",
      "Old: encoder.layer.17.attention.self.key.weight\n",
      "Old: encoder.layer.17.attention.self.key.bias\n",
      "Old: encoder.layer.17.attention.self.value.weight\n",
      "Old: encoder.layer.17.attention.self.value.bias\n",
      "Old: encoder.layer.17.attention.output.dense.weight\n",
      "Old: encoder.layer.17.attention.output.dense.bias\n",
      "Old: encoder.layer.17.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.17.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.17.intermediate.dense.weight\n",
      "Old: encoder.layer.17.intermediate.dense.bias\n",
      "Old: encoder.layer.17.output.dense.weight\n",
      "Old: encoder.layer.17.output.dense.bias\n",
      "Old: encoder.layer.17.output.LayerNorm.weight\n",
      "Old: encoder.layer.17.output.LayerNorm.bias\n",
      "New: encoder.layer.17.down_layer.weight\n",
      "New: encoder.layer.17.down_layer.bias\n",
      "New: encoder.layer.17.up_layer.weight\n",
      "New: encoder.layer.17.up_layer.bias\n",
      "New: encoder.layer.17.layer_norm_new.weight\n",
      "New: encoder.layer.17.layer_norm_new.bias\n",
      "Old: encoder.layer.18.attention.self.query.weight\n",
      "Old: encoder.layer.18.attention.self.query.bias\n",
      "Old: encoder.layer.18.attention.self.key.weight\n",
      "Old: encoder.layer.18.attention.self.key.bias\n",
      "Old: encoder.layer.18.attention.self.value.weight\n",
      "Old: encoder.layer.18.attention.self.value.bias\n",
      "Old: encoder.layer.18.attention.output.dense.weight\n",
      "Old: encoder.layer.18.attention.output.dense.bias\n",
      "Old: encoder.layer.18.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.18.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.18.intermediate.dense.weight\n",
      "Old: encoder.layer.18.intermediate.dense.bias\n",
      "Old: encoder.layer.18.output.dense.weight\n",
      "Old: encoder.layer.18.output.dense.bias\n",
      "Old: encoder.layer.18.output.LayerNorm.weight\n",
      "Old: encoder.layer.18.output.LayerNorm.bias\n",
      "New: encoder.layer.18.down_layer.weight\n",
      "New: encoder.layer.18.down_layer.bias\n",
      "New: encoder.layer.18.up_layer.weight\n",
      "New: encoder.layer.18.up_layer.bias\n",
      "New: encoder.layer.18.layer_norm_new.weight\n",
      "New: encoder.layer.18.layer_norm_new.bias\n",
      "Old: encoder.layer.19.attention.self.query.weight\n",
      "Old: encoder.layer.19.attention.self.query.bias\n",
      "Old: encoder.layer.19.attention.self.key.weight\n",
      "Old: encoder.layer.19.attention.self.key.bias\n",
      "Old: encoder.layer.19.attention.self.value.weight\n",
      "Old: encoder.layer.19.attention.self.value.bias\n",
      "Old: encoder.layer.19.attention.output.dense.weight\n",
      "Old: encoder.layer.19.attention.output.dense.bias\n",
      "Old: encoder.layer.19.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.19.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.19.intermediate.dense.weight\n",
      "Old: encoder.layer.19.intermediate.dense.bias\n",
      "Old: encoder.layer.19.output.dense.weight\n",
      "Old: encoder.layer.19.output.dense.bias\n",
      "Old: encoder.layer.19.output.LayerNorm.weight\n",
      "Old: encoder.layer.19.output.LayerNorm.bias\n",
      "New: encoder.layer.19.down_layer.weight\n",
      "New: encoder.layer.19.down_layer.bias\n",
      "New: encoder.layer.19.up_layer.weight\n",
      "New: encoder.layer.19.up_layer.bias\n",
      "New: encoder.layer.19.layer_norm_new.weight\n",
      "New: encoder.layer.19.layer_norm_new.bias\n",
      "Old: encoder.layer.20.attention.self.query.weight\n",
      "Old: encoder.layer.20.attention.self.query.bias\n",
      "Old: encoder.layer.20.attention.self.key.weight\n",
      "Old: encoder.layer.20.attention.self.key.bias\n",
      "Old: encoder.layer.20.attention.self.value.weight\n",
      "Old: encoder.layer.20.attention.self.value.bias\n",
      "Old: encoder.layer.20.attention.output.dense.weight\n",
      "Old: encoder.layer.20.attention.output.dense.bias\n",
      "Old: encoder.layer.20.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.20.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.20.intermediate.dense.weight\n",
      "Old: encoder.layer.20.intermediate.dense.bias\n",
      "Old: encoder.layer.20.output.dense.weight\n",
      "Old: encoder.layer.20.output.dense.bias\n",
      "Old: encoder.layer.20.output.LayerNorm.weight\n",
      "Old: encoder.layer.20.output.LayerNorm.bias\n",
      "New: encoder.layer.20.down_layer.weight\n",
      "New: encoder.layer.20.down_layer.bias\n",
      "New: encoder.layer.20.up_layer.weight\n",
      "New: encoder.layer.20.up_layer.bias\n",
      "New: encoder.layer.20.layer_norm_new.weight\n",
      "New: encoder.layer.20.layer_norm_new.bias\n",
      "Old: encoder.layer.21.attention.self.query.weight\n",
      "Old: encoder.layer.21.attention.self.query.bias\n",
      "Old: encoder.layer.21.attention.self.key.weight\n",
      "Old: encoder.layer.21.attention.self.key.bias\n",
      "Old: encoder.layer.21.attention.self.value.weight\n",
      "Old: encoder.layer.21.attention.self.value.bias\n",
      "Old: encoder.layer.21.attention.output.dense.weight\n",
      "Old: encoder.layer.21.attention.output.dense.bias\n",
      "Old: encoder.layer.21.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.21.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.21.intermediate.dense.weight\n",
      "Old: encoder.layer.21.intermediate.dense.bias\n",
      "Old: encoder.layer.21.output.dense.weight\n",
      "Old: encoder.layer.21.output.dense.bias\n",
      "Old: encoder.layer.21.output.LayerNorm.weight\n",
      "Old: encoder.layer.21.output.LayerNorm.bias\n",
      "New: encoder.layer.21.down_layer.weight\n",
      "New: encoder.layer.21.down_layer.bias\n",
      "New: encoder.layer.21.up_layer.weight\n",
      "New: encoder.layer.21.up_layer.bias\n",
      "New: encoder.layer.21.layer_norm_new.weight\n",
      "New: encoder.layer.21.layer_norm_new.bias\n",
      "Old: encoder.layer.22.attention.self.query.weight\n",
      "Old: encoder.layer.22.attention.self.query.bias\n",
      "Old: encoder.layer.22.attention.self.key.weight\n",
      "Old: encoder.layer.22.attention.self.key.bias\n",
      "Old: encoder.layer.22.attention.self.value.weight\n",
      "Old: encoder.layer.22.attention.self.value.bias\n",
      "Old: encoder.layer.22.attention.output.dense.weight\n",
      "Old: encoder.layer.22.attention.output.dense.bias\n",
      "Old: encoder.layer.22.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.22.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.22.intermediate.dense.weight\n",
      "Old: encoder.layer.22.intermediate.dense.bias\n",
      "Old: encoder.layer.22.output.dense.weight\n",
      "Old: encoder.layer.22.output.dense.bias\n",
      "Old: encoder.layer.22.output.LayerNorm.weight\n",
      "Old: encoder.layer.22.output.LayerNorm.bias\n",
      "New: encoder.layer.22.down_layer.weight\n",
      "New: encoder.layer.22.down_layer.bias\n",
      "New: encoder.layer.22.up_layer.weight\n",
      "New: encoder.layer.22.up_layer.bias\n",
      "New: encoder.layer.22.layer_norm_new.weight\n",
      "New: encoder.layer.22.layer_norm_new.bias\n",
      "Old: encoder.layer.23.attention.self.query.weight\n",
      "Old: encoder.layer.23.attention.self.query.bias\n",
      "Old: encoder.layer.23.attention.self.key.weight\n",
      "Old: encoder.layer.23.attention.self.key.bias\n",
      "Old: encoder.layer.23.attention.self.value.weight\n",
      "Old: encoder.layer.23.attention.self.value.bias\n",
      "Old: encoder.layer.23.attention.output.dense.weight\n",
      "Old: encoder.layer.23.attention.output.dense.bias\n",
      "Old: encoder.layer.23.attention.output.LayerNorm.weight\n",
      "Old: encoder.layer.23.attention.output.LayerNorm.bias\n",
      "Old: encoder.layer.23.intermediate.dense.weight\n",
      "Old: encoder.layer.23.intermediate.dense.bias\n",
      "Old: encoder.layer.23.output.dense.weight\n",
      "Old: encoder.layer.23.output.dense.bias\n",
      "Old: encoder.layer.23.output.LayerNorm.weight\n",
      "Old: encoder.layer.23.output.LayerNorm.bias\n",
      "New: encoder.layer.23.down_layer.weight\n",
      "New: encoder.layer.23.down_layer.bias\n",
      "New: encoder.layer.23.up_layer.weight\n",
      "New: encoder.layer.23.up_layer.bias\n",
      "New: encoder.layer.23.layer_norm_new.weight\n",
      "New: encoder.layer.23.layer_norm_new.bias\n",
      "Old: pooler.dense.weight\n",
      "Old: pooler.dense.bias\n",
      "New: classifier.0.weight\n",
      "New: classifier.0.bias\n",
      "New: classifier.2.weight\n",
      "New: classifier.2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in custom_model.named_parameters():\n",
    "    if any(keyword in name for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm_new\",]):\n",
    "        print(\"New:\", name)\n",
    "    if all(keyword not in name for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm_new\",]):\n",
    "        print(\"Old:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1297337-4c18-4b46-b825-842e5730f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Evaluate periodically during training\n",
    "    #eval_steps=100,               # Frequency of evaluation (adjust as needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training for GPU\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    ")\n",
    "\n",
    "# Train base model\n",
    "trainer_base = Trainer(\n",
    "    model=custom_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a0201b-f123-4f38-a933-3cb8a1049b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Custom Adapter Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1895' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1895/4689 1:25:24 < 2:06:03, 0.37 it/s, Epoch 1.21/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695700</td>\n",
       "      <td>0.694092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Custom Adapter Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Resize model embeddings after adding new special tokens\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# custom_model.resize_token_embeddings(len(tokenizer))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "custom_model.to(device)\n",
    "print(\"\\nTraining Custom Adapter Model...\")\n",
    "# Resize model embeddings after adding new special tokens\n",
    "# custom_model.resize_token_embeddings(len(tokenizer))\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd4db22a-cadc-4172-9e9b-1de52f9be2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06891806088924982"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26303490/381663234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea296e78-5e51-4a77-ab05-5addf8c1e5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Parameters in Base Model:\n",
      "Total Parameters: 381663234\n",
      "Trainable Parameters: 26303490\n",
      "  - encoder.layer.0.down_layer.weight: 524288 params\n",
      "  - encoder.layer.0.down_layer.bias: 512 params\n",
      "  - encoder.layer.0.up_layer.weight: 524288 params\n",
      "  - encoder.layer.0.up_layer.bias: 1024 params\n",
      "  - encoder.layer.0.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.0.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.1.down_layer.weight: 524288 params\n",
      "  - encoder.layer.1.down_layer.bias: 512 params\n",
      "  - encoder.layer.1.up_layer.weight: 524288 params\n",
      "  - encoder.layer.1.up_layer.bias: 1024 params\n",
      "  - encoder.layer.1.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.1.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.2.down_layer.weight: 524288 params\n",
      "  - encoder.layer.2.down_layer.bias: 512 params\n",
      "  - encoder.layer.2.up_layer.weight: 524288 params\n",
      "  - encoder.layer.2.up_layer.bias: 1024 params\n",
      "  - encoder.layer.2.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.2.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.3.down_layer.weight: 524288 params\n",
      "  - encoder.layer.3.down_layer.bias: 512 params\n",
      "  - encoder.layer.3.up_layer.weight: 524288 params\n",
      "  - encoder.layer.3.up_layer.bias: 1024 params\n",
      "  - encoder.layer.3.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.3.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.4.down_layer.weight: 524288 params\n",
      "  - encoder.layer.4.down_layer.bias: 512 params\n",
      "  - encoder.layer.4.up_layer.weight: 524288 params\n",
      "  - encoder.layer.4.up_layer.bias: 1024 params\n",
      "  - encoder.layer.4.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.4.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.5.down_layer.weight: 524288 params\n",
      "  - encoder.layer.5.down_layer.bias: 512 params\n",
      "  - encoder.layer.5.up_layer.weight: 524288 params\n",
      "  - encoder.layer.5.up_layer.bias: 1024 params\n",
      "  - encoder.layer.5.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.5.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.6.down_layer.weight: 524288 params\n",
      "  - encoder.layer.6.down_layer.bias: 512 params\n",
      "  - encoder.layer.6.up_layer.weight: 524288 params\n",
      "  - encoder.layer.6.up_layer.bias: 1024 params\n",
      "  - encoder.layer.6.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.6.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.7.down_layer.weight: 524288 params\n",
      "  - encoder.layer.7.down_layer.bias: 512 params\n",
      "  - encoder.layer.7.up_layer.weight: 524288 params\n",
      "  - encoder.layer.7.up_layer.bias: 1024 params\n",
      "  - encoder.layer.7.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.7.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.8.down_layer.weight: 524288 params\n",
      "  - encoder.layer.8.down_layer.bias: 512 params\n",
      "  - encoder.layer.8.up_layer.weight: 524288 params\n",
      "  - encoder.layer.8.up_layer.bias: 1024 params\n",
      "  - encoder.layer.8.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.8.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.9.down_layer.weight: 524288 params\n",
      "  - encoder.layer.9.down_layer.bias: 512 params\n",
      "  - encoder.layer.9.up_layer.weight: 524288 params\n",
      "  - encoder.layer.9.up_layer.bias: 1024 params\n",
      "  - encoder.layer.9.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.9.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.10.down_layer.weight: 524288 params\n",
      "  - encoder.layer.10.down_layer.bias: 512 params\n",
      "  - encoder.layer.10.up_layer.weight: 524288 params\n",
      "  - encoder.layer.10.up_layer.bias: 1024 params\n",
      "  - encoder.layer.10.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.10.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.11.down_layer.weight: 524288 params\n",
      "  - encoder.layer.11.down_layer.bias: 512 params\n",
      "  - encoder.layer.11.up_layer.weight: 524288 params\n",
      "  - encoder.layer.11.up_layer.bias: 1024 params\n",
      "  - encoder.layer.11.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.11.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.12.down_layer.weight: 524288 params\n",
      "  - encoder.layer.12.down_layer.bias: 512 params\n",
      "  - encoder.layer.12.up_layer.weight: 524288 params\n",
      "  - encoder.layer.12.up_layer.bias: 1024 params\n",
      "  - encoder.layer.12.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.12.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.13.down_layer.weight: 524288 params\n",
      "  - encoder.layer.13.down_layer.bias: 512 params\n",
      "  - encoder.layer.13.up_layer.weight: 524288 params\n",
      "  - encoder.layer.13.up_layer.bias: 1024 params\n",
      "  - encoder.layer.13.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.13.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.14.down_layer.weight: 524288 params\n",
      "  - encoder.layer.14.down_layer.bias: 512 params\n",
      "  - encoder.layer.14.up_layer.weight: 524288 params\n",
      "  - encoder.layer.14.up_layer.bias: 1024 params\n",
      "  - encoder.layer.14.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.14.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.15.down_layer.weight: 524288 params\n",
      "  - encoder.layer.15.down_layer.bias: 512 params\n",
      "  - encoder.layer.15.up_layer.weight: 524288 params\n",
      "  - encoder.layer.15.up_layer.bias: 1024 params\n",
      "  - encoder.layer.15.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.15.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.16.down_layer.weight: 524288 params\n",
      "  - encoder.layer.16.down_layer.bias: 512 params\n",
      "  - encoder.layer.16.up_layer.weight: 524288 params\n",
      "  - encoder.layer.16.up_layer.bias: 1024 params\n",
      "  - encoder.layer.16.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.16.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.17.down_layer.weight: 524288 params\n",
      "  - encoder.layer.17.down_layer.bias: 512 params\n",
      "  - encoder.layer.17.up_layer.weight: 524288 params\n",
      "  - encoder.layer.17.up_layer.bias: 1024 params\n",
      "  - encoder.layer.17.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.17.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.18.down_layer.weight: 524288 params\n",
      "  - encoder.layer.18.down_layer.bias: 512 params\n",
      "  - encoder.layer.18.up_layer.weight: 524288 params\n",
      "  - encoder.layer.18.up_layer.bias: 1024 params\n",
      "  - encoder.layer.18.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.18.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.19.down_layer.weight: 524288 params\n",
      "  - encoder.layer.19.down_layer.bias: 512 params\n",
      "  - encoder.layer.19.up_layer.weight: 524288 params\n",
      "  - encoder.layer.19.up_layer.bias: 1024 params\n",
      "  - encoder.layer.19.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.19.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.20.down_layer.weight: 524288 params\n",
      "  - encoder.layer.20.down_layer.bias: 512 params\n",
      "  - encoder.layer.20.up_layer.weight: 524288 params\n",
      "  - encoder.layer.20.up_layer.bias: 1024 params\n",
      "  - encoder.layer.20.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.20.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.21.down_layer.weight: 524288 params\n",
      "  - encoder.layer.21.down_layer.bias: 512 params\n",
      "  - encoder.layer.21.up_layer.weight: 524288 params\n",
      "  - encoder.layer.21.up_layer.bias: 1024 params\n",
      "  - encoder.layer.21.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.21.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.22.down_layer.weight: 524288 params\n",
      "  - encoder.layer.22.down_layer.bias: 512 params\n",
      "  - encoder.layer.22.up_layer.weight: 524288 params\n",
      "  - encoder.layer.22.up_layer.bias: 1024 params\n",
      "  - encoder.layer.22.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.22.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.23.down_layer.weight: 524288 params\n",
      "  - encoder.layer.23.down_layer.bias: 512 params\n",
      "  - encoder.layer.23.up_layer.weight: 524288 params\n",
      "  - encoder.layer.23.up_layer.bias: 1024 params\n",
      "  - encoder.layer.23.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.23.layer_norm.bias: 1024 params\n",
      "  - classifier.0.weight: 1048576 params\n",
      "  - classifier.0.bias: 1024 params\n",
      "  - classifier.2.weight: 2048 params\n",
      "  - classifier.2.bias: 2 params\n"
     ]
    }
   ],
   "source": [
    "print_trainable_params(custom_model, stage_name=\"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad630c-6c8b-49c0-b3be-9bd460c1639f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat_nlp",
   "language": "python",
   "name": "stat_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
