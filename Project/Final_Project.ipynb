{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db05b641-b477-4788-afc8-677dafd7b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Making sure it downloads models on my D drive, as no space in defualt file location\n",
    "os.environ['HF_HOME'] = 'D:\\\\Download\\\\UCSD\\\\cache'\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DebertaV2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from adapters import AdapterConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be59cec3-cf96-475a-bef8-489986d5d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Using Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9578774-1faf-4122-84bb-d47e6b8957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directories\n",
    "BASE_MODEL_DIR = \"./base_model\"\n",
    "LORA_MODEL_DIR = \"./lora_model\"\n",
    "ADAPTER_MODEL_DIR = \"./adapter_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6786e0aa-af95-4583-afe0-6aef67376e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44810a5568cf4bc0b840baaf47ddfe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc2cbaffb494d79b24818aa7e2e9cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c49e77cc7fe4c1cb54077fc2c92691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cedc269a3942f1b90e92e2f31dbe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0517c5e5378a49a8b1ff43b48a3a3a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caad8eacd55d47f69ecce16d080898e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 1B and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Using LLama 1B as base model\n",
    "\n",
    "# Couldn't train Llama because of lower mem GPUs so shifting to roberta\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ensure tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068915-0dc2-404e-b8a2-6cb8ff75c635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59e493e-5941-4a6c-b968-6db0a2ba5272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faa381a6d5d4ec9bfc8c2880cec1be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f6f5acaa4d4d3aa67d2a4e207cf268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0d3b9c3a384e7183caa2e4f2d352b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)  # Use full training dataset\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)    # Use full testing dataset\n",
    "\n",
    "# Veyr big dataset\n",
    "# Load a sentiment dataset (example: SST2)\n",
    "# ds = load_dataset(\"facebook/xnli\", \"all_languages\")\n",
    "# train_data = ds['train']\n",
    "# val_data = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99124809-bb4f-4c16-8f0b-2d3d2eab2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model, stage_name=\"Model\"):\n",
    "    print(f\"\\nTrainable Parameters in {stage_name}:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  - {name}: {param.numel()} params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738b4c-5eac-4474-b8a7-f1059757b2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e38754b-57ab-48ec-8ddf-58af4d37ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate periodically during training\n",
    "    #eval_steps=100,               # Frequency of evaluation (adjust as needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training for GPU\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    ")\n",
    "\n",
    "# Train base model\n",
    "trainer_base = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f194fb5-d8e7-434b-bf82-1eadc1809c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044c143-513d-48a6-b77c-27978ffacc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b655e5-9fc9-4cff-9684-2a3661d6bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Parameters in Base Model:\n",
      "Total Parameters: 355361794\n",
      "Trainable Parameters: 355361794\n",
      "  - roberta.embeddings.word_embeddings.weight: 51471360 params\n",
      "  - roberta.embeddings.position_embeddings.weight: 526336 params\n",
      "  - roberta.embeddings.token_type_embeddings.weight: 1024 params\n",
      "  - roberta.embeddings.LayerNorm.weight: 1024 params\n",
      "  - roberta.embeddings.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.0.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.0.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.0.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.0.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.0.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.0.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.0.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.0.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.0.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.0.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.0.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.0.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.0.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.0.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.0.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.0.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.1.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.1.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.1.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.1.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.1.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.1.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.1.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.1.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.1.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.1.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.1.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.1.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.1.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.1.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.1.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.1.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.2.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.2.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.2.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.2.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.2.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.2.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.2.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.2.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.2.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.2.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.2.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.2.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.2.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.2.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.2.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.2.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.3.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.3.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.3.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.3.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.3.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.3.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.3.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.3.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.3.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.3.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.3.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.3.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.3.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.3.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.3.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.3.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.4.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.4.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.4.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.4.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.4.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.4.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.4.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.4.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.4.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.4.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.4.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.4.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.4.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.4.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.4.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.4.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.5.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.5.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.5.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.5.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.5.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.5.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.5.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.5.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.5.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.5.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.5.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.5.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.5.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.5.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.5.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.5.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.6.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.6.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.6.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.6.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.6.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.6.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.6.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.6.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.6.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.6.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.6.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.6.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.6.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.6.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.6.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.6.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.7.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.7.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.7.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.7.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.7.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.7.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.7.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.7.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.7.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.7.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.7.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.7.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.7.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.7.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.7.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.7.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.8.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.8.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.8.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.8.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.8.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.8.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.8.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.8.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.8.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.8.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.8.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.8.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.8.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.8.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.8.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.8.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.9.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.9.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.9.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.9.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.9.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.9.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.9.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.9.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.9.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.9.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.9.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.9.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.9.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.9.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.9.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.9.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.10.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.10.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.10.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.10.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.10.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.10.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.10.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.10.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.10.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.10.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.10.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.10.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.10.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.10.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.10.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.10.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.11.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.11.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.11.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.11.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.11.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.11.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.11.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.11.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.11.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.11.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.11.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.11.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.11.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.11.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.11.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.11.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.12.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.12.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.12.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.12.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.12.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.12.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.12.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.12.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.12.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.12.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.12.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.12.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.12.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.12.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.12.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.12.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.13.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.13.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.13.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.13.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.13.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.13.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.13.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.13.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.13.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.13.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.13.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.13.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.13.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.13.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.13.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.13.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.14.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.14.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.14.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.14.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.14.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.14.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.14.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.14.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.14.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.14.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.14.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.14.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.14.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.14.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.14.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.14.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.15.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.15.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.15.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.15.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.15.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.15.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.15.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.15.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.15.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.15.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.15.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.15.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.15.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.15.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.15.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.15.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.16.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.16.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.16.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.16.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.16.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.16.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.16.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.16.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.16.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.16.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.16.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.16.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.16.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.16.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.16.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.16.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.17.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.17.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.17.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.17.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.17.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.17.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.17.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.17.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.17.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.17.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.17.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.17.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.17.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.17.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.17.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.17.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.18.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.18.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.18.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.18.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.18.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.18.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.18.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.18.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.18.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.18.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.18.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.18.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.18.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.18.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.18.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.18.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.19.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.19.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.19.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.19.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.19.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.19.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.19.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.19.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.19.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.19.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.19.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.19.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.19.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.19.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.19.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.19.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.20.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.20.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.20.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.20.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.20.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.20.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.20.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.20.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.20.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.20.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.20.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.20.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.20.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.20.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.20.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.20.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.21.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.21.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.21.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.21.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.21.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.21.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.21.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.21.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.21.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.21.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.21.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.21.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.21.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.21.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.21.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.21.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.22.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.22.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.22.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.22.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.22.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.22.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.22.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.22.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.22.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.22.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.22.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.22.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.22.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.22.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.22.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.22.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.23.attention.self.query.weight: 1048576 params\n",
      "  - roberta.encoder.layer.23.attention.self.query.bias: 1024 params\n",
      "  - roberta.encoder.layer.23.attention.self.key.weight: 1048576 params\n",
      "  - roberta.encoder.layer.23.attention.self.key.bias: 1024 params\n",
      "  - roberta.encoder.layer.23.attention.self.value.weight: 1048576 params\n",
      "  - roberta.encoder.layer.23.attention.self.value.bias: 1024 params\n",
      "  - roberta.encoder.layer.23.attention.output.dense.weight: 1048576 params\n",
      "  - roberta.encoder.layer.23.attention.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.23.attention.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.23.attention.output.LayerNorm.bias: 1024 params\n",
      "  - roberta.encoder.layer.23.intermediate.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.23.intermediate.dense.bias: 4096 params\n",
      "  - roberta.encoder.layer.23.output.dense.weight: 4194304 params\n",
      "  - roberta.encoder.layer.23.output.dense.bias: 1024 params\n",
      "  - roberta.encoder.layer.23.output.LayerNorm.weight: 1024 params\n",
      "  - roberta.encoder.layer.23.output.LayerNorm.bias: 1024 params\n",
      "  - classifier.dense.weight: 1048576 params\n",
      "  - classifier.dense.bias: 1024 params\n",
      "  - classifier.out_proj.weight: 2048 params\n",
      "  - classifier.out_proj.bias: 2 params\n"
     ]
    }
   ],
   "source": [
    "print_trainable_params(base_model, stage_name=\"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a387cc-974f-41cb-937d-4edf8387bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Base Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/4689 06:36 < 30:16:39, 0.04 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Resize model embeddings after adding new special tokens\u001b[39;00m\n\u001b[0;32m      3\u001b[0m base_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\accelerate\\accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "If \n",
    "print(\"\\nTraining Base Model...\")\n",
    "# Resize model embeddings after adding new special tokens\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a31c3a-d6d7-4d3f-93e6-3520a66719d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836e7b4-f721-4baa-bf44-848aba3b81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save base model\n",
    "tokenizer.save_pretrained(\"./base_model\")\n",
    "base_model.save_pretrained(\"./base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd97c7-ae67-47cc-8828-6f352f74a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "print(\"\\nEvaluating Base Model...\")\n",
    "base_results = trainer_base.evaluate()\n",
    "print(\"Base Model Results:\", base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181e522-f4c0-4926-bd38-6985d2a6b1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b1dd5-5fc1-4e67-b8ed-0fe3b999aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\", \n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "lora_model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "start_time = time.time()\n",
    "print(\"\\nTraining LoRA Model...\")\n",
    "trainer_lora.train()\n",
    "print(f\"LoRa trained in: {time.time() - start_time}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f322e-1953-4bd0-ade7-199f31a7190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA model\n",
    "tokenizer.save_pretrained(\"./lora_model\")\n",
    "lora_model.save_pretrained(\"./lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a95146-ca6f-45a2-8271-bb8a88a8c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LoRA model\n",
    "print(\"\\nEvaluating LoRA Model...\")\n",
    "lora_results = trainer_lora.evaluate()\n",
    "print(\"LoRA Model Results:\", lora_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195363a-5a33-455a-aab0-1b30a06bffc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat_nlp",
   "language": "python",
   "name": "stat_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
