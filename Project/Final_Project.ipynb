{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db05b641-b477-4788-afc8-677dafd7b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Making sure it downloads models on my D drive, as no space in defualt file location\n",
    "os.environ['HF_HOME'] = 'D:\\\\Download\\\\UCSD\\\\cache'\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DebertaV2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from adapters import AdapterConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be59cec3-cf96-475a-bef8-489986d5d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Using Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9578774-1faf-4122-84bb-d47e6b8957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directories\n",
    "BASE_MODEL_DIR = \"./base_model\"\n",
    "LORA_MODEL_DIR = \"./lora_model\"\n",
    "ADAPTER_MODEL_DIR = \"./adapter_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6786e0aa-af95-4583-afe0-6aef67376e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 1B and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Using LLama 1B as base model\n",
    "\n",
    "# Couldn't train Llama because of lower mem GPUs so shifting to roberta\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ensure tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068915-0dc2-404e-b8a2-6cb8ff75c635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59e493e-5941-4a6c-b968-6db0a2ba5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)  # Use full training dataset\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)    # Use full testing dataset\n",
    "\n",
    "# Veyr big dataset\n",
    "# Load a sentiment dataset (example: SST2)\n",
    "# ds = load_dataset(\"facebook/xnli\", \"all_languages\")\n",
    "# train_data = ds['train']\n",
    "# val_data = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99124809-bb4f-4c16-8f0b-2d3d2eab2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model, stage_name=\"Model\"):\n",
    "    print(f\"\\nTrainable Parameters in {stage_name}:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  - {name}: {param.numel()} params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738b4c-5eac-4474-b8a7-f1059757b2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e38754b-57ab-48ec-8ddf-58af4d37ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate periodically during training\n",
    "    #eval_steps=100,               # Frequency of evaluation (adjust as needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training for GPU\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    ")\n",
    "\n",
    "# Train base model\n",
    "trainer_base = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f194fb5-d8e7-434b-bf82-1eadc1809c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044c143-513d-48a6-b77c-27978ffacc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b655e5-9fc9-4cff-9684-2a3661d6bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_params(base_model, stage_name=\"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a387cc-974f-41cb-937d-4edf8387bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Base Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/4689 06:36 < 30:16:39, 0.04 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Resize model embeddings after adding new special tokens\u001b[39;00m\n\u001b[0;32m      3\u001b[0m base_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\accelerate\\accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "If \n",
    "print(\"\\nTraining Base Model...\")\n",
    "# Resize model embeddings after adding new special tokens\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a31c3a-d6d7-4d3f-93e6-3520a66719d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836e7b4-f721-4baa-bf44-848aba3b81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save base model\n",
    "tokenizer.save_pretrained(\"./base_model\")\n",
    "base_model.save_pretrained(\"./base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd97c7-ae67-47cc-8828-6f352f74a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "print(\"\\nEvaluating Base Model...\")\n",
    "base_results = trainer_base.evaluate()\n",
    "print(\"Base Model Results:\", base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181e522-f4c0-4926-bd38-6985d2a6b1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b1dd5-5fc1-4e67-b8ed-0fe3b999aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\", \n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "lora_model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "start_time = time.time()\n",
    "print(\"\\nTraining LoRA Model...\")\n",
    "trainer_lora.train()\n",
    "print(f\"LoRa trained in: {time.time() - start_time}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f322e-1953-4bd0-ade7-199f31a7190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA model\n",
    "tokenizer.save_pretrained(\"./lora_model\")\n",
    "lora_model.save_pretrained(\"./lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6226322-1a15-4026-abd0-f0573a802818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a95146-ca6f-45a2-8271-bb8a88a8c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LoRA model\n",
    "print(\"\\nEvaluating LoRA Model...\")\n",
    "lora_results = trainer_lora.evaluate()\n",
    "print(\"LoRA Model Results:\", lora_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d195363a-5a33-455a-aab0-1b30a06bffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaLayer, RobertaAttention, RobertaIntermediate, RobertaOutput\n",
    "\n",
    "# class CustomRobertaLayer(RobertaModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.attention = RobertaAttention(config)  # Multi-head attention\n",
    "#         self.intermediate = RobertaIntermediate(config)  # Feed-forward network\n",
    "#         self.output = RobertaOutput(config)  # Projection back\n",
    "#         self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)\n",
    "#         self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.up_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         hidden_states,\n",
    "#         attention_mask=None,\n",
    "#         head_mask=None,\n",
    "#         output_attentions=None,\n",
    "#         output_hidden_states=None, input_ids=None, token_type_ids=None, position_ids=None, \n",
    "#         inputs_embeds=None, labels=None, \n",
    "#     ):\n",
    "#         # 1. Self-attention\n",
    "#         attention_output = self.attention(\n",
    "#             hidden_states, attention_mask=attention_mask, head_mask=head_mask\n",
    "#         )\n",
    "        \n",
    "#         # 2. Downsample â†’ Activation â†’ Upsample\n",
    "#         downsampled = self.activation(self.down_layer(attention_output))\n",
    "#         upsampled = self.up_layer(downsampled)\n",
    "#         normalized = self.up_norm(upsampled + attention_output)  # Add residual connection\n",
    "\n",
    "#         # 3. Intermediate feed-forward network\n",
    "#         intermediate_output = self.intermediate(normalized)\n",
    "\n",
    "#         # 4. Final output projection and residual connection\n",
    "#         layer_output = self.output(intermediate_output, normalized)\n",
    "#         return layer_output\n",
    "\n",
    "# class CustomRobertaLayer(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.attention = RobertaAttention(config)\n",
    "#         self.intermediate = RobertaIntermediate(config)\n",
    "#         self.output = RobertaOutput(config)\n",
    "#         self.down_layer = nn.Linear(config.hidden_size, 512)\n",
    "#         self.up_layer = nn.Linear(512, config.hidden_size)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.up_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "#     def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "#         hidden_states = self.attention(hidden_states, attention_mask, head_mask)\n",
    "#         intermediate_output = self.intermediate(hidden_states)\n",
    "#         output = self.output(intermediate_output, hidden_states)\n",
    "#         output = self.down_layer(output)\n",
    "#         output = self.activation(output)\n",
    "#         output = self.up_layer(output)\n",
    "#         return self.up_norm(output)\n",
    "\n",
    "class CustomRobertaLayer(RobertaLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)  # Down-project\n",
    "        self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)  # Up-project\n",
    "        self.activation = nn.ReLU()  # You can use other activations like GELU\n",
    "        self.up_norm = nn.LayerNorm(config.hidden_size)  #Normalization layer \n",
    "\n",
    "    def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_value=None,\n",
    "    use_cache=False,\n",
    "    output_attentions=False,\n",
    "    **kwargs,\n",
    "    ):\n",
    "    # Ensure the attention mask is in the correct dtype\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # Match precision (e.g., float16)\n",
    "        \n",
    "        # Original attention operation\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "    \n",
    "        attention_output = attention_outputs[0]\n",
    "        print(f\"Attention Output Shape: {attention_output.shape}\")\n",
    "        \n",
    "        # Down-projection\n",
    "        down_projected = self.activation(self.down_layer(attention_output))\n",
    "        print(f\"Down Projected Shape: {down_projected.shape}\")\n",
    "        # Up-projection\n",
    "        up_projected = self.activation(self.up_layer(down_projected))\n",
    "        print(f\"Up Projected Shape: {up_projected.shape}\")\n",
    "        #normalization\n",
    "        normalized_output = self.up_norm(up_projected)\n",
    "        print(f\"Normalization Shape: {normalized_output.shape}\")\n",
    "        \n",
    "        # Add & Norm after FF layers\n",
    "        layer_output = self.output(hidden_states=normalized_output, input_tensor=attention_output)\n",
    "        return (layer_output,) + attention_outputs[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f593cc0d-7ef9-4ae2-b99d-ebf775b342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder, RobertaConfig, RobertaEmbeddings\n",
    "\n",
    "\n",
    "class CustomRobertaModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the encoder with the custom encoder\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = CustomRobertaEncoder(config)\n",
    "\n",
    "        # Add the classification head at the end\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, config.num_labels),\n",
    "        )\n",
    "\n",
    "        # Freeze existing layers and enable gradients only for the new layers\n",
    "        self.freeze_pretrained_layers()\n",
    "\n",
    "    def freeze_pretrained_layers(self):\n",
    "        # Freeze all layers except the new classifier and encoder's added layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"classifier\" in name or \"down_layer\" in name or \"up_layer\" in name or \"up_norm\" in name:\n",
    "                param.requires_grad = True  # Enable gradients for new layers\n",
    "            else:\n",
    "                param.requires_grad = False  # Freeze existing layers\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, \n",
    "        head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, labels=None, \n",
    "    ):\n",
    "\n",
    "        print(\"Calling super()\")\n",
    "        print(\"What are labels?: \", labels)\n",
    "        # Embeddings layer\n",
    "        embeddings_output = self.embeddings(input_ids, token_type_ids, position_ids)\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_outputs = self.encoder(embeddings_output, attention_mask)\n",
    "\n",
    "        # Extract [CLS] token for classification\n",
    "        cls_token_output = encoder_outputs[:, 0, :]\n",
    "        \n",
    "        # outputs = super().forward(\n",
    "        #     input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, \n",
    "        #     head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states\n",
    "        # )\n",
    "        # print(\"Super Done\")\n",
    "        # pooled_output = outputs[0][:, 0, :]        \n",
    "        print(f\"Encoder Output Shape: {cls_token_output.shape}\")\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        print(f\"Logits Shape: {logits.shape}\") \n",
    "\n",
    "        return (logits,) + outputs[2:]  # Return logits along with other outputs\n",
    "        \n",
    "class CustomRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([CustomRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff5136a-2737-40ef-9dba-b66f9c75d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CustomRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x CustomRobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (down_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (up_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (activation): ReLU()\n",
       "        (up_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "# Load the configuration\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomRobertaModel(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_model = RobertaModel.from_pretrained(model_name, num_labels=2)\n",
    "custom_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "custom_model.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb534130-7ea0-457f-9545-2543091836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaModel,\n",
    "    RobertaEncoder,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaConfig,\n",
    ")\n",
    "\n",
    "# Custom Layer that adds Down-Up Projection and LayerNorm after FF layer\n",
    "class CustomRobertaLayer(RobertaLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)  # Down-projection\n",
    "        self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)    # Up-projection\n",
    "        self.activation = nn.ReLU()                                              # Activation function\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)                       # LayerNorm after Up-projection\n",
    "        # intializing all as new layers\n",
    "        self.down_layer._is_new = True\n",
    "        self.up_layer._is_new = True\n",
    "        self.activation._is_new = True\n",
    "        self.layer_norm._is_new = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Ensure the attention mask matches the required dimensions\n",
    "        if attention_mask is not None:\n",
    "            # Expand dimensions for multi-head attention\n",
    "            attention_mask = attention_mask[:, None, None, :]  # Shape: [batch_size, 1, 1, seq_len]\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # Match precision (e.g., float16)\n",
    "\n",
    "\n",
    "        # Attention sub-layer\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output = attention_outputs[0]\n",
    "\n",
    "        # Feed-forward sub-layer\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(hidden_states=intermediate_output, input_tensor=attention_output)\n",
    "\n",
    "        # Down-projection, activation, up-projection, and LayerNorm\n",
    "        down_projected = self.activation(self.down_layer(layer_output))\n",
    "        up_projected = self.activation(self.up_layer(down_projected))\n",
    "        norm_output = self.layer_norm(up_projected + layer_output)  # Residual connection\n",
    "\n",
    "        return (norm_output,) + attention_outputs[1:]  # Return outputs\n",
    "\n",
    "# Custom Encoder\n",
    "class CustomRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([CustomRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "# Custom Model\n",
    "class CustomRobertaModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the encoder with the custom encoder\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = CustomRobertaEncoder(config)\n",
    "\n",
    "        # Add the classification head at the end\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, config.num_labels),\n",
    "        )\n",
    "        self.classifier._is_new = True\n",
    "\n",
    "        # Freeze existing layers if needed\n",
    "        self.freeze_pretrained_layers()\n",
    "\n",
    "    def freeze_pretrained_layers(self):\n",
    "        # Freeze all layers except the classifier and custom layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"classifier\" in name or \"down_layer\" in name or \"up_layer\" in name or \"layer_norm\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # Embedding layer\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        # Extract the [CLS] token representation\n",
    "        cls_token_output = sequence_output[:, 0, :]\n",
    "\n",
    "       # Classification head\n",
    "        logits = self.classifier(cls_token_output)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Return loss if available, otherwise logits\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f617597-870e-4722-ba37-a16166ca068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer ('embeddings.word_embeddings.weight', Parameter containing:\n",
      "tensor([[-0.1406, -0.0096,  0.0391,  ...,  0.0508, -0.0059, -0.0360],\n",
      "        [ 0.0078, -0.0156,  0.0156,  ..., -0.0156,  0.0231,  0.0156],\n",
      "        [-0.0828, -0.0007, -0.1174,  ...,  0.1086,  0.0696, -0.0356],\n",
      "        ...,\n",
      "        [ 0.0393,  0.0031,  0.0465,  ..., -0.0240, -0.0505,  0.0342],\n",
      "        [ 0.0499,  0.0272,  0.0413,  ..., -0.0370, -0.0100,  0.0071],\n",
      "        [-0.0149, -0.0114, -0.0222,  ...,  0.0441,  0.0116, -0.0330]])) weights are unchanged.\n",
      "Layer ('embeddings.position_embeddings.weight', Parameter containing:\n",
      "tensor([[-0.0038,  0.0253, -0.0092,  ...,  0.0177,  0.0062, -0.0162],\n",
      "        [ 0.0117, -0.0019, -0.0267,  ...,  0.0062, -0.0193,  0.0264],\n",
      "        [ 0.0316,  0.0148, -0.0549,  ..., -0.0717, -0.0460,  0.0468],\n",
      "        ...,\n",
      "        [-0.0209, -0.0052,  0.0484,  ..., -0.0394,  0.0463,  0.0537],\n",
      "        [-0.0274,  0.1172,  0.0470,  ...,  0.0170, -0.1204,  0.0525],\n",
      "        [ 0.0969, -0.0729,  0.0558,  ..., -0.1204, -0.1075,  0.0489]])) weights are unchanged.\n",
      "Layer ('embeddings.token_type_embeddings.weight', Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])) weights are unchanged.\n",
      "Layer ('embeddings.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9346, 0.9233, 0.9126,  ..., 0.9409, 0.9150, 0.9023])) weights are unchanged.\n",
      "Layer ('embeddings.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0314,  0.0421,  0.1926,  ..., -0.2253, -0.0894,  0.1249])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0042,  0.0347, -0.0016,  ...,  0.0033,  0.0597, -0.0437],\n",
      "        [-0.0249,  0.0521, -0.0166,  ..., -0.0306, -0.0132,  0.0103],\n",
      "        [ 0.0034,  0.0732, -0.0307,  ...,  0.0802,  0.0114, -0.0111],\n",
      "        ...,\n",
      "        [-0.0588,  0.0189, -0.0424,  ..., -0.0325,  0.0049,  0.0689],\n",
      "        [ 0.0419,  0.0228, -0.0643,  ..., -0.0526, -0.0163,  0.0185],\n",
      "        [-0.0199, -0.0427, -0.0098,  ...,  0.0461,  0.0247, -0.0177]])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.3108,  0.0559, -0.0737,  ..., -0.0701, -0.0528, -0.0657])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0045, -0.0172, -0.0128,  ..., -0.0050,  0.0072, -0.0141],\n",
      "        [-0.0238, -0.0002,  0.0267,  ...,  0.0393,  0.0430, -0.0210],\n",
      "        [-0.0289, -0.0530, -0.0137,  ..., -0.0343,  0.0066,  0.0190],\n",
      "        ...,\n",
      "        [-0.0704, -0.0226, -0.0191,  ..., -0.0182,  0.0135,  0.1044],\n",
      "        [ 0.0148,  0.0049, -0.0193,  ..., -0.0014, -0.0083,  0.0438],\n",
      "        [-0.0076, -0.0642,  0.0450,  ...,  0.0460,  0.0176, -0.0481]])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.self.key.bias', Parameter containing:\n",
      "tensor([-0.0041, -0.0033, -0.0012,  ...,  0.0013,  0.0017,  0.0018])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0324,  0.0018, -0.0231,  ..., -0.0245,  0.0013,  0.0222],\n",
      "        [ 0.0566,  0.0464,  0.0023,  ..., -0.0177,  0.0915, -0.0171],\n",
      "        [-0.0168, -0.0433,  0.0110,  ..., -0.0506,  0.0007,  0.0638],\n",
      "        ...,\n",
      "        [-0.0097,  0.0080, -0.0125,  ...,  0.0354,  0.0260,  0.0143],\n",
      "        [-0.0036, -0.0114, -0.0567,  ...,  0.0374, -0.0321,  0.0308],\n",
      "        [ 0.0022, -0.0065, -0.0113,  ..., -0.0243,  0.0887, -0.0165]])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0014,  0.0015, -0.0090,  ..., -0.0231, -0.0206, -0.0350])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0034,  0.0416, -0.0175,  ..., -0.0163, -0.0335, -0.0146],\n",
      "        [-0.0356,  0.0113, -0.0142,  ...,  0.0337, -0.0059,  0.0267],\n",
      "        [ 0.0272, -0.0738,  0.0181,  ..., -0.0316, -0.0052,  0.0991],\n",
      "        ...,\n",
      "        [ 0.0261,  0.0025,  0.0229,  ...,  0.0137, -0.0114, -0.0403],\n",
      "        [-0.0065,  0.0527, -0.0471,  ...,  0.0420,  0.0255, -0.0172],\n",
      "        [ 0.0477,  0.0249,  0.0919,  ...,  0.0262,  0.0092,  0.0080]])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0139,  0.0291,  0.0847,  ...,  0.0735, -0.0089,  0.0106])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9800, 0.9888, 0.9741,  ..., 0.9834, 0.9907, 0.9956])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.4314,  0.2761, -0.0082,  ...,  0.0117,  0.3293, -0.2979])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0575, -0.0649, -0.0924,  ...,  0.0054,  0.0177, -0.0157],\n",
      "        [ 0.0159, -0.0288,  0.0206,  ...,  0.0146, -0.0403,  0.1211],\n",
      "        [ 0.0371, -0.0655, -0.0027,  ...,  0.0308, -0.0264, -0.0214],\n",
      "        ...,\n",
      "        [ 0.0159, -0.0911,  0.0036,  ...,  0.0337, -0.0490,  0.0005],\n",
      "        [ 0.1338,  0.0505, -0.1335,  ..., -0.1487, -0.0338,  0.0233],\n",
      "        [ 0.0709, -0.0305, -0.0605,  ...,  0.0105, -0.0557, -0.0309]])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0954, -0.0762, -0.0827,  ..., -0.1061, -0.0706, -0.0935])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0440,  0.1030,  0.0294,  ..., -0.0545,  0.0581,  0.0656],\n",
      "        [ 0.0179,  0.0095, -0.0106,  ..., -0.0047,  0.0200, -0.0118],\n",
      "        [ 0.0270, -0.0709,  0.0514,  ..., -0.0402, -0.0098,  0.0029],\n",
      "        ...,\n",
      "        [-0.0147, -0.0044,  0.0532,  ...,  0.0102, -0.0615,  0.0132],\n",
      "        [-0.0500,  0.0330, -0.0726,  ...,  0.0566, -0.0357, -0.0030],\n",
      "        [-0.0944, -0.0431, -0.0970,  ..., -0.0548,  0.0547, -0.0312]])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0639, -0.0394,  0.0420,  ...,  0.0079, -0.0959,  0.0559])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9692, 0.9600, 0.9668,  ..., 0.9717, 0.9692, 0.9492])) weights are unchanged.\n",
      "Layer ('encoder.layer.0.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.3965, -0.1881,  0.0423,  ..., -0.0483, -0.2747,  0.2007])) weights are unchanged.\n",
      "Newly Added: encoder.layer.0.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0153,  0.0366,  0.0568,  ...,  0.0262, -0.0589, -0.0554],\n",
      "        [ 0.0449,  0.0390, -0.0470,  ...,  0.0335,  0.0345, -0.0380],\n",
      "        [-0.0057,  0.0476, -0.0217,  ...,  0.0589, -0.0392,  0.0243],\n",
      "        ...,\n",
      "        [-0.0276, -0.0376,  0.0580,  ..., -0.0052, -0.0306,  0.0504],\n",
      "        [-0.0041,  0.0533,  0.0221,  ..., -0.0210, -0.0389,  0.0427],\n",
      "        [ 0.0319, -0.0418,  0.0401,  ..., -0.0044, -0.0099,  0.0109]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.0.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.0.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0538, -0.0222, -0.0267,  ..., -0.0066,  0.0188, -0.0119],\n",
      "        [-0.0234,  0.0092,  0.0469,  ...,  0.0329,  0.0138,  0.0513],\n",
      "        [-0.0410, -0.0474,  0.0119,  ..., -0.0438, -0.0529, -0.0204],\n",
      "        ...,\n",
      "        [ 0.0130,  0.0612,  0.0219,  ...,  0.0417,  0.0582, -0.0442],\n",
      "        [-0.0510,  0.0308, -0.0064,  ..., -0.0621,  0.0334,  0.0601],\n",
      "        [-0.0299,  0.0464,  0.0311,  ..., -0.0492,  0.0350, -0.0073]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.0.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.0.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.0.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.1.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 2.1683e-02, -1.1279e-01,  1.9333e-02,  ..., -6.5796e-02,\n",
      "         -2.9007e-02, -3.4943e-02],\n",
      "        [ 3.2166e-02, -9.5062e-03,  9.8694e-02,  ..., -2.5238e-02,\n",
      "          2.5101e-02,  1.6699e-01],\n",
      "        [ 1.0048e-02, -4.2664e-02,  3.2776e-02,  ...,  8.3694e-03,\n",
      "          1.6046e-04, -3.9864e-03],\n",
      "        ...,\n",
      "        [ 1.5839e-02,  3.2074e-02,  6.5674e-02,  ...,  4.5624e-02,\n",
      "         -5.2429e-02,  1.0413e-01],\n",
      "        [-1.1688e-01,  7.1594e-02,  6.8054e-02,  ..., -1.3435e-02,\n",
      "         -7.6172e-02,  6.0883e-02],\n",
      "        [ 2.9373e-02, -6.9031e-02,  2.3865e-02,  ...,  6.5063e-02,\n",
      "          1.2312e-03,  5.0087e-03]])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.0743,  0.0511, -0.0696,  ...,  0.0821,  0.0454, -0.0778])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0103,  0.0596, -0.0087,  ..., -0.0296,  0.0145, -0.0166],\n",
      "        [-0.0197, -0.0879, -0.0577,  ...,  0.0533, -0.0383,  0.0147],\n",
      "        [-0.0135,  0.0399,  0.0354,  ...,  0.0063,  0.0013, -0.0112],\n",
      "        ...,\n",
      "        [-0.0620, -0.0673,  0.0816,  ...,  0.0229,  0.0219,  0.0021],\n",
      "        [ 0.0475, -0.0484,  0.0353,  ...,  0.0483,  0.0384, -0.0329],\n",
      "        [ 0.0616,  0.0388,  0.0559,  ...,  0.0919,  0.0252,  0.0011]])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 1.3943e-03, -8.1110e-04, -8.1825e-04,  ...,  2.2817e-04,\n",
      "        -7.8976e-05,  1.0090e-03])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0652,  0.0030, -0.0267,  ..., -0.0068,  0.0537,  0.0282],\n",
      "        [-0.0198,  0.0352, -0.0343,  ..., -0.0100,  0.0327, -0.0210],\n",
      "        [ 0.0018,  0.0371,  0.0038,  ...,  0.0246,  0.0854, -0.0176],\n",
      "        ...,\n",
      "        [-0.0037, -0.0809, -0.0334,  ..., -0.0224,  0.0370,  0.0235],\n",
      "        [-0.0616,  0.0683, -0.0165,  ..., -0.0056,  0.0229, -0.0136],\n",
      "        [ 0.0363,  0.0345,  0.0145,  ...,  0.0121,  0.0313, -0.0161]])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0066,  0.0039,  0.0057,  ...,  0.0015, -0.0548, -0.0020])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0389, -0.0113,  0.0055,  ..., -0.0724, -0.0307,  0.0182],\n",
      "        [ 0.0018, -0.0532, -0.0046,  ..., -0.0745, -0.0389,  0.0057],\n",
      "        [ 0.0433,  0.0118, -0.0229,  ..., -0.0305,  0.0177,  0.0338],\n",
      "        ...,\n",
      "        [-0.0064, -0.0179,  0.0367,  ...,  0.0226, -0.0260,  0.0481],\n",
      "        [-0.0196,  0.0106,  0.0085,  ..., -0.0491, -0.0395, -0.0521],\n",
      "        [ 0.0055, -0.0220,  0.0424,  ...,  0.0079, -0.0465,  0.0259]])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.2167,  0.0391, -0.0821,  ..., -0.0754,  0.2332,  0.0728])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9829, 0.9995, 0.9580,  ..., 0.9795, 0.9883, 0.9731])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.3291,  0.1810, -0.0328,  ..., -0.0730,  0.2494, -0.2292])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0294, -0.0344, -0.0014,  ..., -0.0199,  0.1002,  0.0521],\n",
      "        [-0.0024, -0.0701, -0.0299,  ...,  0.0352, -0.0020, -0.0547],\n",
      "        [-0.0043,  0.0847,  0.0724,  ...,  0.0141,  0.0217, -0.0074],\n",
      "        ...,\n",
      "        [-0.0388, -0.0439,  0.1240,  ...,  0.0003,  0.0130, -0.0310],\n",
      "        [ 0.0744, -0.0583, -0.1044,  ..., -0.0316,  0.0242, -0.0829],\n",
      "        [ 0.0011,  0.0749,  0.0948,  ..., -0.0186, -0.0264,  0.0312]])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.intermediate.dense.bias', Parameter containing:\n",
      "tensor([ 0.0954, -0.0535, -0.0536,  ..., -0.0871, -0.0856, -0.0858])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0212, -0.0177,  0.0631,  ..., -0.0227, -0.0908,  0.0513],\n",
      "        [ 0.0463, -0.0184, -0.0130,  ..., -0.0318, -0.0057, -0.0450],\n",
      "        [ 0.0411,  0.0374,  0.0648,  ..., -0.0264, -0.0519,  0.0962],\n",
      "        ...,\n",
      "        [ 0.0419, -0.0616, -0.0754,  ..., -0.0400,  0.0235,  0.0345],\n",
      "        [ 0.0093,  0.0853,  0.1106,  ...,  0.0648, -0.0422, -0.0220],\n",
      "        [-0.0651,  0.0728,  0.0051,  ..., -0.0348,  0.0692,  0.0396]])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0104,  0.0057,  0.0673,  ...,  0.0118, -0.0482,  0.0131])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9590, 0.9941, 0.9395,  ..., 0.9688, 0.9673, 0.9390])) weights are unchanged.\n",
      "Layer ('encoder.layer.1.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.1896, -0.1931, -0.0493,  ..., -0.0134, -0.2729,  0.1577])) weights are unchanged.\n",
      "Newly Added: encoder.layer.1.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0156, -0.0395, -0.0247,  ..., -0.0085, -0.0434,  0.0512],\n",
      "        [-0.0297, -0.0498, -0.0029,  ...,  0.0085,  0.0504,  0.0334],\n",
      "        [-0.0142,  0.0616,  0.0030,  ...,  0.0423, -0.0159,  0.0265],\n",
      "        ...,\n",
      "        [ 0.0599, -0.0088,  0.0372,  ...,  0.0538,  0.0128,  0.0243],\n",
      "        [ 0.0044,  0.0608, -0.0196,  ...,  0.0474, -0.0132, -0.0442],\n",
      "        [-0.0624,  0.0199,  0.0037,  ..., -0.0342,  0.0377,  0.0109]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.1.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.1.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0142,  0.0224, -0.0446,  ..., -0.0223, -0.0447, -0.0022],\n",
      "        [-0.0356,  0.0186,  0.0233,  ...,  0.0167, -0.0618,  0.0478],\n",
      "        [ 0.0160, -0.0362, -0.0123,  ...,  0.0056, -0.0334,  0.0078],\n",
      "        ...,\n",
      "        [-0.0595, -0.0020, -0.0212,  ..., -0.0029, -0.0502, -0.0030],\n",
      "        [ 0.0405, -0.0500,  0.0244,  ..., -0.0141, -0.0206, -0.0338],\n",
      "        [ 0.0368, -0.0316, -0.0182,  ..., -0.0394,  0.0525, -0.0353]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.1.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.1.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.1.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.2.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0037, -0.0497,  0.0443,  ..., -0.0283, -0.0679, -0.0152],\n",
      "        [-0.0826, -0.0897,  0.0353,  ..., -0.0249,  0.0029,  0.0137],\n",
      "        [-0.0217, -0.0073, -0.0402,  ..., -0.0335, -0.0129, -0.0217],\n",
      "        ...,\n",
      "        [ 0.0205, -0.0191, -0.0120,  ..., -0.0353,  0.0823, -0.0728],\n",
      "        [ 0.0316, -0.0968, -0.0501,  ..., -0.0464, -0.0008,  0.0120],\n",
      "        [ 0.0208, -0.0242, -0.0710,  ...,  0.0729, -0.0063,  0.0469]])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0626,  0.1261,  0.1142,  ..., -0.1519,  0.0648, -0.1615])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0140,  0.1364,  0.0394,  ...,  0.0352,  0.0181, -0.0569],\n",
      "        [-0.0002,  0.0158, -0.0162,  ..., -0.0303, -0.0384,  0.0427],\n",
      "        [ 0.0659, -0.0079,  0.0019,  ..., -0.0539,  0.0268, -0.0468],\n",
      "        ...,\n",
      "        [-0.0029, -0.0351, -0.0347,  ..., -0.0019, -0.0549, -0.0683],\n",
      "        [-0.0551, -0.0336,  0.0355,  ...,  0.0027, -0.0502,  0.0192],\n",
      "        [ 0.0318,  0.0149, -0.0259,  ...,  0.0670,  0.0113,  0.0532]])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.self.key.bias', Parameter containing:\n",
      "tensor([-9.1743e-04, -2.7084e-03, -1.1196e-03,  ..., -7.6890e-05,\n",
      "        -6.1846e-04, -9.4986e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0054,  0.0502,  0.0221,  ...,  0.0291,  0.0287,  0.0009],\n",
      "        [-0.0007,  0.0914, -0.0208,  ..., -0.0035, -0.0303, -0.0043],\n",
      "        [ 0.0202,  0.0468,  0.0071,  ...,  0.0363,  0.0282, -0.0201],\n",
      "        ...,\n",
      "        [-0.0112,  0.0281, -0.0109,  ..., -0.0392, -0.0270,  0.0395],\n",
      "        [ 0.0610,  0.0394, -0.0400,  ..., -0.0234,  0.0649,  0.0303],\n",
      "        [-0.0240, -0.0088,  0.0009,  ..., -0.0116,  0.0122,  0.0531]])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0301,  0.0159,  0.0106,  ..., -0.0054,  0.0105, -0.0050])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-1.3542e-02, -1.6357e-02,  2.1469e-02,  ..., -5.3062e-03,\n",
      "         -1.1940e-02,  2.3575e-02],\n",
      "        [ 3.4885e-03,  1.6861e-02, -1.4534e-02,  ..., -4.6448e-02,\n",
      "         -1.1993e-02, -2.7409e-03],\n",
      "        [ 5.9967e-03,  4.0375e-02, -7.1228e-02,  ..., -1.2650e-02,\n",
      "         -1.3779e-02, -1.5381e-02],\n",
      "        ...,\n",
      "        [-2.9907e-02, -7.3051e-03,  4.8889e-02,  ..., -2.9926e-03,\n",
      "          9.1248e-03, -1.2840e-02],\n",
      "        [-8.0872e-02,  7.3493e-05,  3.3379e-03,  ...,  4.3915e-02,\n",
      "          1.9791e-02, -9.0790e-03],\n",
      "        [ 5.4291e-02,  4.8279e-02,  4.6325e-04,  ...,  5.1231e-03,\n",
      "          5.4657e-02,  2.5986e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0610, -0.0594,  0.0323,  ...,  0.0803, -0.0398,  0.0735])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9814, 0.9849, 0.9858,  ..., 0.9629, 0.9565, 0.9712])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0940, -0.0031, -0.3413,  ..., -0.0275, -0.0715,  0.2152])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0228,  0.0859,  0.0685,  ..., -0.0477, -0.0161, -0.0438],\n",
      "        [-0.0249, -0.0678,  0.0927,  ..., -0.0751,  0.0543, -0.0039],\n",
      "        [-0.0729,  0.0043,  0.0925,  ...,  0.0138,  0.0226,  0.0032],\n",
      "        ...,\n",
      "        [-0.0362, -0.0121,  0.0057,  ...,  0.0064, -0.0012,  0.0199],\n",
      "        [-0.0673,  0.0101, -0.0132,  ..., -0.0005,  0.0340, -0.0955],\n",
      "        [ 0.0006,  0.0221,  0.0193,  ..., -0.0164, -0.0526, -0.1001]])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0233, -0.0803, -0.0687,  ...,  0.0582, -0.0864, -0.0893])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0239,  0.0827,  0.0165,  ..., -0.0398, -0.1088,  0.0011],\n",
      "        [-0.0065,  0.0176, -0.0627,  ...,  0.0477, -0.0254, -0.0119],\n",
      "        [ 0.0536, -0.0567,  0.0587,  ...,  0.0124, -0.0474, -0.0762],\n",
      "        ...,\n",
      "        [ 0.0061, -0.0446,  0.0693,  ...,  0.0141,  0.1027,  0.0016],\n",
      "        [ 0.0486, -0.0089, -0.0314,  ...,  0.0111, -0.0954, -0.0092],\n",
      "        [ 0.0309, -0.0491, -0.0233,  ..., -0.0424,  0.0161,  0.0007]])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0090, -0.0412,  0.0115,  ...,  0.0299,  0.0136,  0.0445])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9653, 0.9683, 0.9536,  ..., 0.9731, 0.9771, 0.9595])) weights are unchanged.\n",
      "Layer ('encoder.layer.2.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0324, -0.1078,  0.2080,  ..., -0.0503, -0.0276, -0.2032])) weights are unchanged.\n",
      "Newly Added: encoder.layer.2.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0349,  0.0516, -0.0522,  ...,  0.0441, -0.0028,  0.0085],\n",
      "        [ 0.0154,  0.0036,  0.0321,  ...,  0.0504, -0.0195,  0.0515],\n",
      "        [ 0.0442, -0.0094, -0.0427,  ..., -0.0479, -0.0364,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0397, -0.0528,  0.0346,  ...,  0.0595, -0.0615, -0.0205],\n",
      "        [-0.0230, -0.0022, -0.0046,  ...,  0.0236,  0.0011, -0.0325],\n",
      "        [ 0.0297,  0.0028,  0.0154,  ...,  0.0081, -0.0364, -0.0385]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.2.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.2.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0549, -0.0047, -0.0079,  ...,  0.0498,  0.0010,  0.0054],\n",
      "        [ 0.0031, -0.0108, -0.0049,  ..., -0.0033, -0.0225, -0.0603],\n",
      "        [ 0.0174, -0.0196,  0.0148,  ...,  0.0494, -0.0042, -0.0028],\n",
      "        ...,\n",
      "        [-0.0324,  0.0546,  0.0590,  ..., -0.0409, -0.0173,  0.0134],\n",
      "        [ 0.0375, -0.0030,  0.0040,  ...,  0.0382,  0.0291, -0.0329],\n",
      "        [ 0.0531,  0.0025,  0.0380,  ..., -0.0569, -0.0615, -0.0425]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.2.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.2.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.2.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.3.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0712,  0.0097,  0.0376,  ...,  0.0291,  0.0526, -0.0108],\n",
      "        [-0.0191, -0.0765, -0.0407,  ...,  0.0356, -0.0146,  0.0311],\n",
      "        [-0.0204, -0.0179,  0.0803,  ...,  0.0351, -0.0190,  0.0064],\n",
      "        ...,\n",
      "        [-0.0564, -0.0120, -0.0109,  ...,  0.0377,  0.0884, -0.0202],\n",
      "        [ 0.0684, -0.0347,  0.0419,  ..., -0.0892,  0.0264, -0.0361],\n",
      "        [ 0.0323,  0.0648, -0.0203,  ..., -0.0419, -0.0321,  0.0206]])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0244, -0.0524,  0.0302,  ...,  0.0131,  0.1157, -0.0828])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.1331, -0.0380,  0.0011,  ...,  0.0231,  0.0477, -0.0254],\n",
      "        [ 0.0243,  0.1003,  0.0936,  ..., -0.0249,  0.0847, -0.0457],\n",
      "        [ 0.0059, -0.0403,  0.0914,  ..., -0.0175,  0.0155, -0.0206],\n",
      "        ...,\n",
      "        [-0.0251, -0.0050, -0.0037,  ...,  0.0738, -0.0612, -0.0409],\n",
      "        [ 0.0022, -0.0244, -0.0063,  ...,  0.0406, -0.0319,  0.0085],\n",
      "        [ 0.0315,  0.0121,  0.0219,  ...,  0.0159, -0.0307, -0.0179]])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.self.key.bias', Parameter containing:\n",
      "tensor([-5.9187e-05,  1.0948e-03,  1.8919e-04,  ...,  1.3676e-03,\n",
      "        -7.5769e-04,  4.3488e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0117, -0.0319, -0.0748,  ...,  0.0300,  0.0969,  0.0223],\n",
      "        [-0.0171,  0.0381,  0.0467,  ..., -0.0291,  0.0078, -0.0063],\n",
      "        [ 0.0394,  0.0378, -0.0704,  ..., -0.0519,  0.0271,  0.0231],\n",
      "        ...,\n",
      "        [ 0.0719,  0.0008,  0.0126,  ...,  0.0225,  0.0311,  0.0434],\n",
      "        [-0.0226,  0.0504, -0.0327,  ..., -0.0040, -0.1065, -0.0126],\n",
      "        [ 0.0267, -0.0087, -0.0442,  ..., -0.0314, -0.0029, -0.0478]])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0181, -0.0088, -0.0070,  ...,  0.0067,  0.0059, -0.0009])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0004,  0.0026,  0.0343,  ...,  0.0010, -0.0266, -0.0284],\n",
      "        [ 0.0110, -0.0084,  0.0408,  ...,  0.0134, -0.0187,  0.0137],\n",
      "        [-0.0790,  0.0118, -0.0061,  ..., -0.0122,  0.0570, -0.0051],\n",
      "        ...,\n",
      "        [ 0.0211, -0.0012, -0.0217,  ...,  0.0223, -0.0326,  0.0138],\n",
      "        [ 0.0132,  0.0420,  0.0026,  ..., -0.0075, -0.0126, -0.0088],\n",
      "        [-0.0280, -0.0183,  0.0176,  ..., -0.0087,  0.0168,  0.0360]])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0423, -0.0195, -0.0266,  ..., -0.0225,  0.0345, -0.0204])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9863, 0.9819, 0.9771,  ..., 0.9863, 0.9785, 0.9663])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1804, -0.1191, -0.3018,  ..., -0.0452,  0.1131,  0.0351])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0115,  0.0395, -0.0339,  ..., -0.0525, -0.0232, -0.0353],\n",
      "        [ 0.0370,  0.0315,  0.0163,  ...,  0.0240, -0.0438, -0.0146],\n",
      "        [ 0.0343, -0.0618, -0.0168,  ...,  0.0667, -0.1826,  0.0077],\n",
      "        ...,\n",
      "        [-0.0285, -0.0330, -0.0359,  ...,  0.0337, -0.0243, -0.0533],\n",
      "        [ 0.0299, -0.0596, -0.0386,  ..., -0.0016,  0.0213, -0.0295],\n",
      "        [ 0.0002,  0.0047, -0.0463,  ...,  0.0121, -0.0643, -0.0451]])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0911, -0.1101, -0.0567,  ..., -0.1008, -0.0185, -0.0873])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0193, -0.0331, -0.0232,  ...,  0.0206, -0.0674, -0.0084],\n",
      "        [ 0.0246, -0.0143, -0.0244,  ..., -0.0045, -0.0454, -0.0682],\n",
      "        [-0.0762, -0.0782,  0.0211,  ...,  0.0562, -0.0289, -0.0016],\n",
      "        ...,\n",
      "        [ 0.0081,  0.0290,  0.0782,  ..., -0.0535, -0.0729,  0.0006],\n",
      "        [-0.0964, -0.0807, -0.0590,  ..., -0.0215, -0.0072, -0.1068],\n",
      "        [ 0.1379,  0.0524, -0.0634,  ...,  0.0483, -0.0443,  0.0300]])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0875,  0.0331,  0.0640,  ...,  0.0408, -0.0828,  0.2712])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9751, 0.9917, 0.9678,  ..., 0.9717, 0.9673, 0.9497])) weights are unchanged.\n",
      "Layer ('encoder.layer.3.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0133, -0.0074,  0.1967,  ..., -0.0252, -0.1181, -0.0804])) weights are unchanged.\n",
      "Newly Added: encoder.layer.3.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0014,  0.0264,  0.0471,  ...,  0.0332, -0.0500, -0.0049],\n",
      "        [ 0.0609,  0.0607, -0.0263,  ...,  0.0437,  0.0618, -0.0361],\n",
      "        [-0.0364, -0.0385, -0.0255,  ...,  0.0183,  0.0302, -0.0250],\n",
      "        ...,\n",
      "        [ 0.0377,  0.0332, -0.0082,  ...,  0.0027, -0.0426, -0.0192],\n",
      "        [ 0.0083,  0.0510,  0.0407,  ..., -0.0500, -0.0449,  0.0445],\n",
      "        [ 0.0086,  0.0241, -0.0595,  ..., -0.0363,  0.0043,  0.0558]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.3.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.3.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0236,  0.0148, -0.0279,  ..., -0.0341,  0.0415,  0.0351],\n",
      "        [ 0.0286, -0.0393,  0.0218,  ...,  0.0100,  0.0487,  0.0551],\n",
      "        [-0.0329,  0.0335, -0.0300,  ...,  0.0209,  0.0286, -0.0142],\n",
      "        ...,\n",
      "        [ 0.0463, -0.0601,  0.0141,  ..., -0.0004,  0.0243, -0.0604],\n",
      "        [ 0.0112,  0.0520,  0.0099,  ..., -0.0393, -0.0328,  0.0262],\n",
      "        [ 0.0236,  0.0213,  0.0293,  ..., -0.0612,  0.0103, -0.0254]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.3.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.3.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.3.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.4.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0065, -0.0140, -0.0589,  ..., -0.0077, -0.0347, -0.0074],\n",
      "        [-0.0023,  0.0019,  0.0432,  ..., -0.0606, -0.0117,  0.0692],\n",
      "        [-0.0847,  0.0070,  0.0154,  ..., -0.0649, -0.0119,  0.0284],\n",
      "        ...,\n",
      "        [ 0.0165,  0.0070, -0.0132,  ...,  0.0207,  0.0043,  0.0014],\n",
      "        [ 0.0171, -0.0274,  0.0376,  ..., -0.0103, -0.0284,  0.0164],\n",
      "        [ 0.0352, -0.1013, -0.0187,  ..., -0.0378, -0.0362, -0.1453]])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.2100,  0.0039,  0.2280,  ..., -0.2532, -0.1885,  0.2032])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0186, -0.0761, -0.0492,  ..., -0.0070, -0.0438,  0.0079],\n",
      "        [-0.0532, -0.0105,  0.0205,  ...,  0.0041, -0.0238,  0.0127],\n",
      "        [ 0.0140,  0.0293,  0.0414,  ...,  0.0326,  0.0664, -0.0059],\n",
      "        ...,\n",
      "        [ 0.0372,  0.0804,  0.0374,  ...,  0.0555,  0.0097,  0.0903],\n",
      "        [-0.0149, -0.0255, -0.0283,  ..., -0.0180, -0.0418,  0.0627],\n",
      "        [ 0.0045,  0.0278, -0.0062,  ...,  0.0266, -0.0330,  0.0235]])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 0.0006, -0.0003,  0.0003,  ..., -0.0005, -0.0002, -0.0002])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0093,  0.0007,  0.0084,  ...,  0.0206,  0.0029,  0.0479],\n",
      "        [ 0.0198,  0.0607, -0.0226,  ...,  0.0636, -0.0177, -0.0742],\n",
      "        [ 0.0368,  0.0164,  0.0247,  ...,  0.0519, -0.0342,  0.0644],\n",
      "        ...,\n",
      "        [ 0.0117,  0.0651,  0.0219,  ...,  0.0487, -0.0018,  0.0248],\n",
      "        [ 0.0275,  0.0076,  0.0163,  ...,  0.0290, -0.0032,  0.0635],\n",
      "        [ 0.0088, -0.1032, -0.0013,  ..., -0.0404,  0.0336,  0.0076]])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0094, -0.0032,  0.0089,  ..., -0.0073,  0.0025, -0.0005])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0335, -0.0706, -0.0206,  ..., -0.0066, -0.0078,  0.0664],\n",
      "        [-0.0454, -0.0166, -0.0117,  ...,  0.0456,  0.0285,  0.0088],\n",
      "        [ 0.0059,  0.0088, -0.0098,  ..., -0.0417,  0.0453, -0.0116],\n",
      "        ...,\n",
      "        [ 0.0046,  0.0050,  0.0074,  ...,  0.0126,  0.0025, -0.0157],\n",
      "        [-0.0023,  0.0142, -0.0296,  ...,  0.0532,  0.0049, -0.0253],\n",
      "        [ 0.0175,  0.0455, -0.0144,  ..., -0.0102, -0.0273,  0.0093]])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0074, -0.0126, -0.0088,  ...,  0.0169, -0.0641, -0.0042])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9902, 0.9985, 0.9868,  ..., 0.9873, 0.9971, 0.9585])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1681,  0.0775, -0.2976,  ...,  0.0609,  0.0173,  0.2048])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0026,  0.0550, -0.0331,  ..., -0.0766,  0.0542,  0.0404],\n",
      "        [-0.0143, -0.0167, -0.0072,  ..., -0.0156, -0.0169, -0.0902],\n",
      "        [-0.0327,  0.0142,  0.0496,  ..., -0.0940,  0.0210, -0.0462],\n",
      "        ...,\n",
      "        [-0.0588, -0.0233,  0.0649,  ...,  0.1270,  0.0421, -0.0712],\n",
      "        [-0.0042, -0.0811, -0.0883,  ..., -0.0558, -0.0258,  0.0617],\n",
      "        [-0.0369, -0.0236,  0.0060,  ...,  0.0209,  0.0364, -0.0447]])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0895, -0.0796, -0.0721,  ..., -0.0707, -0.0522, -0.0544])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0204,  0.0603,  0.0074,  ...,  0.0366, -0.0113, -0.0281],\n",
      "        [-0.0467, -0.0277, -0.0513,  ...,  0.0353, -0.0570,  0.0018],\n",
      "        [ 0.0151,  0.0825,  0.1046,  ...,  0.0023,  0.0529, -0.0290],\n",
      "        ...,\n",
      "        [-0.0349, -0.0451, -0.0652,  ...,  0.1400, -0.0656,  0.0148],\n",
      "        [ 0.0388, -0.0215,  0.0321,  ...,  0.1024, -0.0144,  0.0144],\n",
      "        [ 0.0075, -0.0456, -0.0986,  ..., -0.0384,  0.0642, -0.0393]])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0841, -0.0167,  0.1401,  ...,  0.0091, -0.1005,  0.1653])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9727, 0.9961, 0.9751,  ..., 0.9771, 0.9961, 0.9673])) weights are unchanged.\n",
      "Layer ('encoder.layer.4.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0118, -0.1010,  0.2786,  ..., -0.1054, -0.1014, -0.1643])) weights are unchanged.\n",
      "Newly Added: encoder.layer.4.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0267,  0.0097,  0.0263,  ...,  0.0113, -0.0504,  0.0463],\n",
      "        [ 0.0162, -0.0389,  0.0229,  ...,  0.0005,  0.0313, -0.0382],\n",
      "        [-0.0072, -0.0220, -0.0478,  ..., -0.0573,  0.0480,  0.0132],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0047,  0.0599,  ...,  0.0268, -0.0376,  0.0524],\n",
      "        [-0.0190,  0.0587, -0.0260,  ..., -0.0583, -0.0281, -0.0056],\n",
      "        [-0.0232, -0.0391,  0.0593,  ...,  0.0419,  0.0333, -0.0348]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.4.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.4.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0211,  0.0256,  0.0232,  ..., -0.0107, -0.0356,  0.0377],\n",
      "        [-0.0302,  0.0068,  0.0111,  ...,  0.0167, -0.0050, -0.0549],\n",
      "        [-0.0356,  0.0525,  0.0450,  ...,  0.0529,  0.0404, -0.0590],\n",
      "        ...,\n",
      "        [-0.0120,  0.0199,  0.0166,  ..., -0.0234,  0.0234, -0.0619],\n",
      "        [-0.0524, -0.0161, -0.0151,  ...,  0.0179, -0.0287,  0.0428],\n",
      "        [-0.0272,  0.0340, -0.0105,  ...,  0.0032,  0.0234,  0.0072]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.4.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.4.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.4.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.5.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0701, -0.0117,  0.0259,  ..., -0.0172,  0.0208,  0.0345],\n",
      "        [ 0.0538, -0.0120,  0.0066,  ...,  0.0030,  0.0462, -0.1401],\n",
      "        [-0.0547, -0.0127, -0.0211,  ...,  0.0235, -0.0474, -0.0021],\n",
      "        ...,\n",
      "        [-0.0178, -0.0200, -0.0732,  ..., -0.0316,  0.0170,  0.0017],\n",
      "        [ 0.0476, -0.0839,  0.0221,  ...,  0.0347,  0.0223, -0.0324],\n",
      "        [-0.0077,  0.0272,  0.0586,  ...,  0.0086,  0.0671,  0.0533]])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.0328, -0.0290, -0.0242,  ...,  0.0297,  0.1254, -0.1331])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 1.5879e-03,  6.4697e-02, -9.2773e-03,  ..., -4.0405e-02,\n",
      "          5.9929e-03, -1.4702e-02],\n",
      "        [ 3.9032e-02, -2.1317e-02, -3.8635e-02,  ...,  3.6407e-02,\n",
      "          3.1616e-02, -3.7872e-02],\n",
      "        [ 3.5095e-02, -1.6632e-02, -3.3173e-02,  ..., -1.1328e-01,\n",
      "          4.2572e-02,  3.2471e-02],\n",
      "        ...,\n",
      "        [ 2.8229e-02,  7.7188e-05, -4.5624e-03,  ...,  2.7634e-02,\n",
      "          6.5491e-02,  4.1595e-02],\n",
      "        [ 1.7899e-02,  6.6490e-03, -1.3237e-02,  ...,  1.6998e-02,\n",
      "         -1.5038e-02,  1.0391e-02],\n",
      "        [-7.2937e-02,  1.3714e-03,  5.9418e-02,  ..., -5.3040e-02,\n",
      "          1.7471e-02,  3.1319e-03]])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 8.0585e-04, -8.8274e-05, -1.3983e-04,  ...,  1.6794e-03,\n",
      "         8.8596e-04,  4.0960e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-2.1271e-02,  5.2246e-02,  1.2596e-02,  ..., -3.1972e-04,\n",
      "         -6.9771e-03,  5.1422e-02],\n",
      "        [-6.7139e-02, -6.5491e-02, -1.3092e-02,  ...,  2.2369e-02,\n",
      "         -4.3396e-02, -5.9357e-02],\n",
      "        [ 3.8574e-02, -4.2725e-02, -2.9297e-02,  ..., -2.9802e-05,\n",
      "         -1.2581e-02,  3.2257e-02],\n",
      "        ...,\n",
      "        [ 3.0365e-02, -2.0691e-02, -1.3786e-02,  ..., -3.0777e-02,\n",
      "         -9.1705e-03, -3.1525e-02],\n",
      "        [ 5.6519e-02,  2.6764e-02, -9.6985e-02,  ...,  1.2199e-02,\n",
      "         -8.4305e-03,  4.6875e-02],\n",
      "        [-3.5492e-02,  7.0190e-02, -2.3026e-02,  ..., -2.2247e-02,\n",
      "         -2.1133e-02,  2.3499e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0022, -0.0049, -0.0022,  ...,  0.0084, -0.0059, -0.0095])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0295, -0.0094, -0.0190,  ...,  0.0242,  0.0242, -0.0228],\n",
      "        [-0.0983,  0.0297,  0.0096,  ..., -0.0517, -0.0190,  0.0466],\n",
      "        [ 0.0653, -0.0074,  0.0428,  ..., -0.0186, -0.0409, -0.0307],\n",
      "        ...,\n",
      "        [-0.0107, -0.0522, -0.0144,  ..., -0.0068,  0.0210, -0.0367],\n",
      "        [ 0.0008,  0.0127,  0.0365,  ...,  0.0086, -0.0692, -0.0016],\n",
      "        [ 0.0068,  0.0255,  0.0211,  ...,  0.0341, -0.0081, -0.0044]])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0058,  0.0079, -0.0753,  ..., -0.0275, -0.0629, -0.0144])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9912, 0.9990, 0.9985,  ..., 0.9790, 0.9917, 0.9697])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1207,  0.0558, -0.2698,  ...,  0.0012, -0.0933,  0.0925])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0023, -0.0378, -0.0315,  ..., -0.0089, -0.0048, -0.0246],\n",
      "        [-0.0047, -0.0434, -0.0201,  ..., -0.0168,  0.0679, -0.0439],\n",
      "        [ 0.0646,  0.0493,  0.0061,  ..., -0.0126, -0.0359,  0.0527],\n",
      "        ...,\n",
      "        [-0.0152, -0.0545, -0.0124,  ...,  0.0603,  0.0032, -0.0709],\n",
      "        [-0.0238, -0.1223,  0.0007,  ...,  0.0288,  0.0658, -0.0305],\n",
      "        [-0.0576,  0.0427,  0.0489,  ...,  0.0288, -0.0007,  0.0439]])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0727, -0.1107, -0.1094,  ..., -0.0519, -0.1167, -0.1059])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0307,  0.0609,  0.0028,  ..., -0.0154, -0.0379,  0.0259],\n",
      "        [ 0.0447, -0.0581,  0.0644,  ..., -0.0441, -0.0212, -0.0804],\n",
      "        [ 0.0062,  0.0119, -0.0137,  ..., -0.0322, -0.0124, -0.0699],\n",
      "        ...,\n",
      "        [-0.0677, -0.0647, -0.0333,  ...,  0.0010,  0.1031, -0.0217],\n",
      "        [-0.0117,  0.0200, -0.0771,  ..., -0.0256, -0.0110, -0.0042],\n",
      "        [ 0.0014, -0.0229,  0.0839,  ..., -0.0513,  0.0290, -0.0355]])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1071,  0.0106,  0.1376,  ..., -0.0015, -0.0591,  0.1449])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9727, 0.9937, 0.9907,  ..., 0.9756, 0.9917, 0.9565])) weights are unchanged.\n",
      "Layer ('encoder.layer.5.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0165, -0.1136,  0.2712,  ..., -0.1176, -0.0460, -0.1471])) weights are unchanged.\n",
      "Newly Added: encoder.layer.5.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0357, -0.0152,  0.0462,  ..., -0.0296,  0.0476,  0.0466],\n",
      "        [-0.0266,  0.0032,  0.0355,  ..., -0.0131, -0.0224, -0.0057],\n",
      "        [-0.0100,  0.0102,  0.0561,  ...,  0.0086, -0.0492, -0.0103],\n",
      "        ...,\n",
      "        [-0.0082, -0.0102, -0.0522,  ...,  0.0460,  0.0049, -0.0609],\n",
      "        [-0.0143, -0.0473, -0.0465,  ...,  0.0447,  0.0153,  0.0558],\n",
      "        [ 0.0104,  0.0497,  0.0069,  ..., -0.0177,  0.0612,  0.0253]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.5.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.5.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0499,  0.0001,  0.0483,  ...,  0.0225,  0.0558,  0.0262],\n",
      "        [ 0.0221, -0.0624, -0.0245,  ..., -0.0236, -0.0110, -0.0521],\n",
      "        [ 0.0399, -0.0462, -0.0242,  ...,  0.0523, -0.0295, -0.0481],\n",
      "        ...,\n",
      "        [ 0.0402,  0.0549, -0.0031,  ..., -0.0176, -0.0116,  0.0419],\n",
      "        [-0.0370, -0.0535,  0.0056,  ...,  0.0058, -0.0347,  0.0258],\n",
      "        [ 0.0592,  0.0578, -0.0105,  ..., -0.0101,  0.0125,  0.0123]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.5.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.5.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.5.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.6.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0198, -0.0261,  0.0551,  ...,  0.0417,  0.0497,  0.0078],\n",
      "        [-0.0055, -0.0328, -0.0066,  ..., -0.0061, -0.0501, -0.0333],\n",
      "        [ 0.0378,  0.1501,  0.0873,  ...,  0.0482,  0.0451,  0.0360],\n",
      "        ...,\n",
      "        [ 0.0099, -0.0024, -0.1047,  ...,  0.0317,  0.0684,  0.0650],\n",
      "        [ 0.0257,  0.0184,  0.0190,  ...,  0.0053,  0.0265,  0.0864],\n",
      "        [-0.0392, -0.0010,  0.0403,  ..., -0.0429,  0.0341, -0.0313]])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.3074,  0.0543, -0.3076,  ..., -0.0909,  0.0283,  0.0039])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 2.7771e-02, -3.0380e-02,  6.7505e-02,  ..., -1.1589e-02,\n",
      "         -2.7512e-02, -8.3256e-04],\n",
      "        [-1.6342e-02,  3.0457e-02,  1.9424e-02,  ...,  9.9426e-02,\n",
      "          7.6233e-02,  1.5747e-02],\n",
      "        [ 5.5145e-02,  2.4891e-04,  1.8967e-02,  ..., -1.4679e-02,\n",
      "         -3.5004e-02, -4.1199e-02],\n",
      "        ...,\n",
      "        [-1.8112e-02,  1.6144e-02,  3.8261e-03,  ..., -3.4180e-02,\n",
      "          1.3832e-02, -2.2268e-04],\n",
      "        [-7.5134e-02, -6.3110e-02, -1.2035e-03,  ...,  4.5319e-02,\n",
      "          3.7659e-02, -7.0267e-03],\n",
      "        [ 9.6817e-03,  3.3997e-02,  6.7949e-05,  ...,  5.9082e-02,\n",
      "          3.0640e-02, -8.9188e-03]])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 7.9823e-04, -6.5517e-04,  1.4269e-04,  ...,  7.2896e-05,\n",
      "         2.9230e-04,  2.2483e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0128,  0.0156, -0.0072,  ..., -0.0307, -0.0132, -0.0014],\n",
      "        [-0.0018,  0.0396,  0.0188,  ..., -0.0561,  0.0386,  0.0136],\n",
      "        [-0.0172, -0.0562,  0.0533,  ...,  0.0457, -0.0023,  0.0233],\n",
      "        ...,\n",
      "        [ 0.0145,  0.0403,  0.0008,  ...,  0.0475, -0.0590,  0.0015],\n",
      "        [ 0.0092, -0.0157, -0.0046,  ..., -0.0295,  0.0175, -0.0145],\n",
      "        [ 0.0117,  0.0240,  0.0225,  ..., -0.0343,  0.1074,  0.0263]])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0072, -0.0078, -0.0015,  ..., -0.0179, -0.0083, -0.0025])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0388,  0.0045, -0.0160,  ...,  0.0404, -0.0314, -0.0048],\n",
      "        [ 0.0188, -0.0354, -0.0397,  ...,  0.0212,  0.0386, -0.0201],\n",
      "        [ 0.0496, -0.0577, -0.0385,  ...,  0.0269,  0.0673, -0.0362],\n",
      "        ...,\n",
      "        [-0.0166,  0.0493,  0.0010,  ..., -0.0426,  0.0055,  0.0208],\n",
      "        [-0.0266,  0.0227,  0.0433,  ...,  0.0251, -0.0196, -0.0406],\n",
      "        [-0.0219,  0.0471,  0.0232,  ...,  0.0600,  0.0048,  0.0190]])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0075, -0.0111,  0.0319,  ..., -0.0002, -0.0668,  0.0089])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9849, 0.9995, 0.9995,  ..., 0.9829, 0.9795, 0.9517])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1209,  0.0573, -0.2834,  ..., -0.0227, -0.1025,  0.1007])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0052, -0.0053, -0.0128,  ..., -0.0038,  0.0404,  0.0694],\n",
      "        [ 0.0060,  0.0058, -0.0009,  ..., -0.0269,  0.0212, -0.0418],\n",
      "        [-0.0605, -0.0773, -0.0483,  ...,  0.0749, -0.0172, -0.0795],\n",
      "        ...,\n",
      "        [-0.0475, -0.0405,  0.0061,  ..., -0.0118,  0.0809, -0.0515],\n",
      "        [ 0.0037,  0.0027,  0.0085,  ...,  0.0124,  0.0114,  0.0080],\n",
      "        [-0.0103, -0.0606,  0.0460,  ..., -0.0205,  0.0188, -0.1267]])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0795, -0.0929, -0.1035,  ..., -0.0771, -0.0702, -0.1737])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0271, -0.0347, -0.0198,  ...,  0.0352,  0.0227, -0.0079],\n",
      "        [ 0.0097, -0.0428, -0.0086,  ..., -0.0074,  0.0348,  0.0107],\n",
      "        [ 0.0246, -0.0133, -0.0906,  ...,  0.0432, -0.0434,  0.0382],\n",
      "        ...,\n",
      "        [ 0.0093, -0.0624,  0.0096,  ..., -0.0162,  0.0570, -0.0461],\n",
      "        [ 0.0286,  0.0006, -0.1135,  ..., -0.0179, -0.0282,  0.0639],\n",
      "        [ 0.0405,  0.0091,  0.0193,  ..., -0.0305,  0.0257, -0.0179]])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0679,  0.0312,  0.0668,  ..., -0.0091, -0.0555,  0.0709])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9824, 0.9897, 1.0000,  ..., 0.9795, 0.9751, 0.9609])) weights are unchanged.\n",
      "Layer ('encoder.layer.6.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0241, -0.1124,  0.1122,  ..., -0.0810, -0.0277, -0.1335])) weights are unchanged.\n",
      "Newly Added: encoder.layer.6.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0117, -0.0521,  0.0472,  ..., -0.0465, -0.0102,  0.0232],\n",
      "        [ 0.0085, -0.0385, -0.0572,  ..., -0.0231, -0.0323,  0.0085],\n",
      "        [ 0.0401,  0.0266,  0.0178,  ..., -0.0291,  0.0316, -0.0022],\n",
      "        ...,\n",
      "        [-0.0407, -0.0514, -0.0039,  ...,  0.0223,  0.0430, -0.0284],\n",
      "        [-0.0373, -0.0501,  0.0097,  ...,  0.0165, -0.0439,  0.0484],\n",
      "        [ 0.0354, -0.0024,  0.0042,  ...,  0.0443,  0.0063,  0.0267]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.6.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.6.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0561,  0.0129,  0.0184,  ...,  0.0147, -0.0273, -0.0506],\n",
      "        [-0.0328, -0.0468, -0.0329,  ..., -0.0415, -0.0036,  0.0386],\n",
      "        [-0.0332,  0.0321,  0.0101,  ..., -0.0012, -0.0462, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0359, -0.0184, -0.0353,  ..., -0.0061, -0.0308, -0.0246],\n",
      "        [-0.0365,  0.0042, -0.0323,  ...,  0.0117, -0.0133,  0.0208],\n",
      "        [-0.0153,  0.0351, -0.0013,  ..., -0.0261,  0.0165, -0.0038]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.6.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.6.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.6.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.7.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0184, -0.0201,  0.0036,  ...,  0.0256, -0.0393, -0.1075],\n",
      "        [ 0.0565, -0.0805, -0.0152,  ...,  0.0168, -0.0428,  0.0195],\n",
      "        [-0.1337,  0.0188,  0.0227,  ...,  0.0094,  0.0819,  0.0018],\n",
      "        ...,\n",
      "        [-0.0295,  0.1948,  0.0428,  ...,  0.0566,  0.0104,  0.1293],\n",
      "        [ 0.0082,  0.0250, -0.0315,  ...,  0.0128,  0.0742,  0.1000],\n",
      "        [ 0.0170,  0.0125,  0.0275,  ...,  0.0941,  0.1177,  0.0461]])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.0159,  0.0128, -0.0539,  ..., -0.2546,  0.1262, -0.2625])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0045, -0.0021, -0.0544,  ..., -0.0084, -0.0034,  0.0627],\n",
      "        [ 0.0125,  0.0051,  0.0239,  ...,  0.0232, -0.0456,  0.0063],\n",
      "        [-0.0080, -0.0432, -0.0181,  ...,  0.0223, -0.0365,  0.0291],\n",
      "        ...,\n",
      "        [-0.0247,  0.0017,  0.0285,  ..., -0.0513, -0.0775, -0.0425],\n",
      "        [-0.0698,  0.0384,  0.0177,  ..., -0.0067, -0.0243,  0.0805],\n",
      "        [ 0.0225,  0.0233,  0.0276,  ...,  0.0106, -0.0107, -0.0332]])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.self.key.bias', Parameter containing:\n",
      "tensor([-0.0004, -0.0008, -0.0002,  ...,  0.0003,  0.0006,  0.0002])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0062, -0.0215,  0.0021,  ...,  0.0200, -0.0024, -0.0427],\n",
      "        [-0.0114, -0.0298, -0.0223,  ..., -0.0180, -0.0340, -0.0479],\n",
      "        [ 0.0233, -0.0204,  0.0246,  ..., -0.0148,  0.0224,  0.0078],\n",
      "        ...,\n",
      "        [-0.0019, -0.0428,  0.0140,  ...,  0.0232,  0.0383,  0.0101],\n",
      "        [-0.0551, -0.0564, -0.0037,  ...,  0.0457,  0.0208,  0.0209],\n",
      "        [ 0.0353, -0.0013, -0.0257,  ...,  0.0329, -0.0365, -0.0166]])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0014,  0.0333, -0.0516,  ...,  0.0141,  0.0035,  0.0053])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0013, -0.0080,  0.0293,  ...,  0.0084,  0.0642, -0.0313],\n",
      "        [-0.0392,  0.0024,  0.0165,  ...,  0.0410, -0.0790,  0.0256],\n",
      "        [-0.0339, -0.0198,  0.0130,  ...,  0.0442,  0.0382,  0.0347],\n",
      "        ...,\n",
      "        [ 0.0449,  0.0030,  0.0521,  ...,  0.0071, -0.0655, -0.0060],\n",
      "        [ 0.0102,  0.0133,  0.0110,  ..., -0.0082,  0.0157,  0.0328],\n",
      "        [-0.0123, -0.0276, -0.0095,  ..., -0.0523, -0.0144,  0.0467]])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0173,  0.0161, -0.0387,  ..., -0.0473, -0.0037,  0.0277])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9907, 0.9868, 1.0000,  ..., 0.9810, 0.9590, 0.9746])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0919,  0.0110, -0.4243,  ..., -0.0744, -0.1014, -0.0009])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0643,  0.0358,  0.0168,  ...,  0.0629,  0.0051,  0.0346],\n",
      "        [ 0.0648,  0.0390, -0.0084,  ..., -0.0023, -0.0386, -0.0366],\n",
      "        [-0.0376, -0.0243, -0.0010,  ...,  0.0164,  0.0161,  0.0332],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0324,  0.0043,  ..., -0.0392, -0.0227,  0.0073],\n",
      "        [-0.0329, -0.1088,  0.0165,  ..., -0.0867, -0.0081, -0.0115],\n",
      "        [-0.0504, -0.0500,  0.0031,  ...,  0.0442, -0.0249,  0.0342]])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0926, -0.0908, -0.0112,  ...,  0.0072, -0.0848, -0.0317])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0114,  0.0720, -0.0147,  ...,  0.0148, -0.1044, -0.0399],\n",
      "        [ 0.0326,  0.0779,  0.0047,  ..., -0.0380, -0.0641,  0.0314],\n",
      "        [-0.0134, -0.0088, -0.0089,  ...,  0.0290,  0.0138,  0.0067],\n",
      "        ...,\n",
      "        [-0.0593,  0.0385, -0.0249,  ...,  0.0267, -0.0116,  0.0278],\n",
      "        [-0.0098, -0.0016, -0.0242,  ...,  0.0292,  0.0196,  0.0135],\n",
      "        [ 0.0695, -0.0198,  0.0122,  ..., -0.0290,  0.0178, -0.0433]])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1935,  0.0500,  0.1160,  ..., -0.0164, -0.0422,  0.0584])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9888, 0.9893, 0.9990,  ..., 0.9810, 0.9771, 0.9517])) weights are unchanged.\n",
      "Layer ('encoder.layer.7.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0313, -0.0964, -0.0033,  ..., -0.0478, -0.0408, -0.1099])) weights are unchanged.\n",
      "Newly Added: encoder.layer.7.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0199,  0.0068,  0.0179,  ..., -0.0107, -0.0223,  0.0620],\n",
      "        [-0.0178,  0.0288,  0.0047,  ..., -0.0616, -0.0065,  0.0556],\n",
      "        [ 0.0462,  0.0079,  0.0241,  ...,  0.0483, -0.0185, -0.0214],\n",
      "        ...,\n",
      "        [-0.0114, -0.0592,  0.0074,  ..., -0.0507, -0.0608,  0.0550],\n",
      "        [-0.0596,  0.0540, -0.0427,  ..., -0.0560,  0.0495,  0.0465],\n",
      "        [-0.0024, -0.0087, -0.0536,  ...,  0.0238, -0.0383,  0.0516]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.7.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.7.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0458, -0.0039,  0.0046,  ...,  0.0511,  0.0346,  0.0229],\n",
      "        [-0.0618,  0.0550, -0.0574,  ..., -0.0461,  0.0178,  0.0038],\n",
      "        [ 0.0322, -0.0194, -0.0294,  ..., -0.0011,  0.0565, -0.0272],\n",
      "        ...,\n",
      "        [-0.0439, -0.0303, -0.0363,  ..., -0.0274,  0.0390,  0.0317],\n",
      "        [ 0.0535,  0.0249,  0.0221,  ...,  0.0266,  0.0322,  0.0083],\n",
      "        [ 0.0276, -0.0375,  0.0065,  ...,  0.0548, -0.0538, -0.0486]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.7.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.7.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.7.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.8.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 3.5126e-02,  3.3295e-02,  3.0731e-02,  ...,  2.1011e-02,\n",
      "          1.7136e-02,  1.1436e-02],\n",
      "        [-9.0698e-02, -6.7383e-02,  6.7078e-02,  ..., -3.6926e-02,\n",
      "          2.4414e-03,  1.0948e-02],\n",
      "        [-6.6101e-02,  1.0541e-01, -3.0579e-02,  ...,  4.3030e-02,\n",
      "          6.7566e-02, -2.7409e-03],\n",
      "        ...,\n",
      "        [ 6.0822e-02,  4.2175e-02, -2.8477e-03,  ..., -7.7553e-03,\n",
      "          8.5876e-02, -5.7831e-03],\n",
      "        [ 1.1147e-02,  8.0078e-02, -3.1647e-02,  ..., -1.2573e-01,\n",
      "          4.1260e-02,  2.8183e-02],\n",
      "        [-2.8198e-02, -7.9956e-02,  4.7266e-05,  ...,  5.4779e-03,\n",
      "         -1.7303e-02, -4.4373e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0943,  0.1276,  0.0213,  ..., -0.1385, -0.0545,  0.0571])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0713,  0.0465,  0.0005,  ..., -0.1571,  0.0556, -0.0124],\n",
      "        [-0.0549, -0.0030,  0.1162,  ...,  0.0144,  0.0181, -0.0310],\n",
      "        [ 0.0414, -0.0372, -0.0593,  ..., -0.0314,  0.1133, -0.0304],\n",
      "        ...,\n",
      "        [-0.0090,  0.0548, -0.0418,  ...,  0.0569,  0.0009,  0.0328],\n",
      "        [-0.0787,  0.0292, -0.0483,  ...,  0.0121,  0.0411, -0.0278],\n",
      "        [-0.0113, -0.0854,  0.0329,  ...,  0.0147,  0.0084, -0.0656]])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 3.2246e-05,  1.8108e-04, -3.4857e-04,  ...,  7.0214e-05,\n",
      "        -1.5354e-04,  1.8835e-05])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0437,  0.0020, -0.0155,  ..., -0.0463,  0.0313,  0.0349],\n",
      "        [-0.0494, -0.0345, -0.0009,  ...,  0.0656,  0.0010,  0.0500],\n",
      "        [ 0.0020, -0.0055,  0.0049,  ..., -0.0005, -0.0087, -0.0667],\n",
      "        ...,\n",
      "        [ 0.0032,  0.0339,  0.0179,  ..., -0.0219, -0.0401, -0.0317],\n",
      "        [ 0.0526, -0.0489,  0.0020,  ..., -0.0063, -0.0175, -0.0130],\n",
      "        [-0.0276, -0.0160,  0.0036,  ...,  0.0417, -0.0164, -0.0402]])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0069, -0.0147,  0.0093,  ..., -0.0142, -0.0061, -0.0035])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0290, -0.0029, -0.0091,  ..., -0.0007,  0.0065, -0.0013],\n",
      "        [ 0.0020, -0.0158,  0.0277,  ..., -0.0002,  0.0400,  0.0616],\n",
      "        [ 0.0497, -0.0378,  0.0262,  ...,  0.0438, -0.0356,  0.0063],\n",
      "        ...,\n",
      "        [-0.0339,  0.0262, -0.0167,  ...,  0.0360, -0.0335, -0.0305],\n",
      "        [-0.0475,  0.0050, -0.0264,  ...,  0.0027,  0.0017,  0.0064],\n",
      "        [ 0.0269, -0.0111,  0.0308,  ...,  0.0231,  0.0037,  0.0472]])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0503,  0.0053,  0.0043,  ...,  0.0089, -0.0961,  0.0549])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9883, 0.9956, 1.0000,  ..., 0.9888, 0.9946, 0.9736])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1704,  0.0124, -0.4998,  ..., -0.0114, -0.1092, -0.0147])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0093, -0.0008,  0.0194,  ..., -0.0634,  0.0100, -0.0768],\n",
      "        [ 0.0294, -0.0720,  0.0389,  ..., -0.1108,  0.0507, -0.0224],\n",
      "        [ 0.0321,  0.0978,  0.0402,  ...,  0.0280,  0.0110,  0.0154],\n",
      "        ...,\n",
      "        [ 0.0444, -0.0175,  0.0360,  ..., -0.0202, -0.0172,  0.0371],\n",
      "        [ 0.0878,  0.0206,  0.0094,  ...,  0.0159,  0.0532, -0.0740],\n",
      "        [ 0.0359,  0.0054,  0.0193,  ..., -0.0244, -0.0676, -0.0166]])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0615, -0.0998, -0.0534,  ..., -0.0757, -0.0580, -0.0348])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0066,  0.0095,  0.0095,  ..., -0.0306,  0.0687,  0.0158],\n",
      "        [-0.0350,  0.0171,  0.0659,  ...,  0.0198,  0.0092,  0.0272],\n",
      "        [ 0.0367, -0.0168,  0.0182,  ...,  0.0372, -0.0040, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0693, -0.0601,  0.0317,  ...,  0.0518, -0.0111,  0.0214],\n",
      "        [-0.0159, -0.0063,  0.0140,  ..., -0.0641, -0.0101, -0.0051],\n",
      "        [ 0.0210,  0.0233,  0.0253,  ...,  0.0256,  0.0080,  0.0250]])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1677,  0.0007,  0.0620,  ..., -0.0411, -0.0503, -0.0571])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9751, 0.9946, 1.0000,  ..., 0.9771, 0.9795, 0.9541])) weights are unchanged.\n",
      "Layer ('encoder.layer.8.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0214, -0.1109, -0.0194,  ..., -0.0840, -0.0250, -0.0840])) weights are unchanged.\n",
      "Newly Added: encoder.layer.8.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0025,  0.0033,  0.0226,  ..., -0.0151,  0.0347,  0.0203],\n",
      "        [ 0.0625,  0.0136, -0.0585,  ...,  0.0068, -0.0611,  0.0431],\n",
      "        [ 0.0101, -0.0559,  0.0341,  ..., -0.0220,  0.0467,  0.0299],\n",
      "        ...,\n",
      "        [ 0.0127, -0.0298, -0.0462,  ...,  0.0080,  0.0621, -0.0209],\n",
      "        [-0.0390,  0.0238,  0.0020,  ..., -0.0105,  0.0603,  0.0307],\n",
      "        [ 0.0129, -0.0209,  0.0542,  ...,  0.0492,  0.0499, -0.0025]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.8.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.8.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0212, -0.0406, -0.0323,  ..., -0.0311,  0.0084,  0.0115],\n",
      "        [-0.0332, -0.0292,  0.0471,  ..., -0.0290, -0.0230,  0.0425],\n",
      "        [-0.0425, -0.0093,  0.0144,  ..., -0.0352,  0.0409,  0.0081],\n",
      "        ...,\n",
      "        [ 0.0069,  0.0416, -0.0269,  ...,  0.0310,  0.0221, -0.0605],\n",
      "        [ 0.0086, -0.0014,  0.0126,  ...,  0.0227, -0.0274,  0.0181],\n",
      "        [-0.0568, -0.0021,  0.0376,  ..., -0.0136,  0.0251, -0.0430]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.8.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.8.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.8.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.9.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0088, -0.0836, -0.0362,  ..., -0.0324,  0.0670, -0.0665],\n",
      "        [ 0.0210, -0.0313, -0.0089,  ..., -0.0323,  0.0508,  0.0569],\n",
      "        [ 0.0203,  0.0261,  0.0156,  ..., -0.0396,  0.0146, -0.0259],\n",
      "        ...,\n",
      "        [ 0.0586, -0.0261, -0.0010,  ..., -0.0128, -0.0658, -0.0121],\n",
      "        [ 0.0457,  0.0061, -0.0795,  ...,  0.0384,  0.0273,  0.0222],\n",
      "        [-0.0217, -0.0197, -0.0412,  ..., -0.0468, -0.0359, -0.0468]])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0128, -0.0311, -0.0597,  ...,  0.0042, -0.0584,  0.0740])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0356,  0.0501,  0.0002,  ..., -0.0135, -0.0303, -0.0376],\n",
      "        [ 0.0377, -0.0073, -0.0035,  ..., -0.0349, -0.0193, -0.0414],\n",
      "        [ 0.0373, -0.0497,  0.0698,  ..., -0.0678, -0.0273,  0.0132],\n",
      "        ...,\n",
      "        [-0.0112,  0.0115,  0.0026,  ...,  0.0087,  0.0543,  0.0075],\n",
      "        [ 0.0435,  0.0352, -0.0113,  ...,  0.0144,  0.0286,  0.0100],\n",
      "        [-0.0410,  0.0096, -0.0300,  ...,  0.0856, -0.0035,  0.1017]])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.self.key.bias', Parameter containing:\n",
      "tensor([-1.2445e-04, -6.7949e-06,  2.7442e-04,  ...,  1.6046e-04,\n",
      "         2.4748e-04,  1.4639e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0243,  0.0025,  0.0106,  ..., -0.0256, -0.0547,  0.0285],\n",
      "        [-0.0216, -0.0318, -0.0006,  ..., -0.0532, -0.0037,  0.0562],\n",
      "        [ 0.0006,  0.0536,  0.0112,  ...,  0.0215,  0.0266,  0.0113],\n",
      "        ...,\n",
      "        [-0.0540,  0.0349,  0.0133,  ...,  0.0559,  0.0595, -0.0052],\n",
      "        [ 0.0543,  0.0149, -0.0107,  ..., -0.0321, -0.0369, -0.0331],\n",
      "        [-0.0347,  0.0137,  0.0083,  ...,  0.0207,  0.0062, -0.0189]])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0011, -0.0075,  0.0011,  ...,  0.0322, -0.0014,  0.0014])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0178, -0.0267,  0.0084,  ...,  0.0193, -0.0103, -0.0196],\n",
      "        [ 0.0035, -0.0424,  0.0039,  ..., -0.0461,  0.0175,  0.0204],\n",
      "        [ 0.0344, -0.0676, -0.0007,  ..., -0.0129,  0.0405,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0024,  0.0125,  0.0321,  ..., -0.0183,  0.0039,  0.0020],\n",
      "        [-0.0166, -0.0070, -0.0078,  ..., -0.0807, -0.0195,  0.0549],\n",
      "        [ 0.0274,  0.0131,  0.0289,  ..., -0.0128, -0.0118, -0.0038]])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0463,  0.0033, -0.1741,  ...,  0.0533, -0.0485,  0.0045])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9844, 0.9922, 0.9995,  ..., 0.9839, 0.9927, 0.9873])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1785, -0.0274, -0.4651,  ..., -0.0481, -0.1164, -0.0284])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0258,  0.0055, -0.0157,  ...,  0.0245,  0.0215,  0.0020],\n",
      "        [ 0.0071, -0.0009,  0.0148,  ...,  0.0831,  0.0248,  0.0139],\n",
      "        [ 0.0492, -0.0511,  0.0135,  ...,  0.0628, -0.0786,  0.0659],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0355,  0.0134,  ...,  0.0725,  0.0058, -0.0191],\n",
      "        [ 0.0134, -0.0574,  0.0068,  ...,  0.0003, -0.0117, -0.0106],\n",
      "        [-0.0201, -0.0328,  0.0111,  ...,  0.0095,  0.0479,  0.0180]])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0536, -0.0799, -0.0246,  ..., -0.0476, -0.0424, -0.1010])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0027,  0.0216, -0.0166,  ..., -0.0072,  0.0223,  0.0036],\n",
      "        [ 0.0338,  0.0773, -0.0190,  ..., -0.0200, -0.0089, -0.0116],\n",
      "        [ 0.0111,  0.0050,  0.0170,  ...,  0.0125, -0.0050, -0.0016],\n",
      "        ...,\n",
      "        [ 0.0139,  0.0930, -0.0511,  ..., -0.0506,  0.0202,  0.0169],\n",
      "        [ 0.0312,  0.0473, -0.0491,  ...,  0.0432,  0.0333, -0.0017],\n",
      "        [-0.0401, -0.0263, -0.0482,  ..., -0.0115,  0.0226,  0.0052]])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1375,  0.0675, -0.0873,  ..., -0.0446, -0.0779, -0.0238])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9761, 0.9883, 0.9985,  ..., 0.9717, 0.9810, 0.9736])) weights are unchanged.\n",
      "Layer ('encoder.layer.9.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0323, -0.0623, -0.2095,  ..., -0.0591, -0.0127, -0.0453])) weights are unchanged.\n",
      "Newly Added: encoder.layer.9.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0567, -0.0392,  0.0306,  ...,  0.0206,  0.0240, -0.0253],\n",
      "        [ 0.0428, -0.0053,  0.0195,  ...,  0.0224,  0.0122,  0.0342],\n",
      "        [-0.0003, -0.0219, -0.0108,  ...,  0.0456,  0.0152, -0.0498],\n",
      "        ...,\n",
      "        [ 0.0373, -0.0467, -0.0489,  ..., -0.0604, -0.0254,  0.0250],\n",
      "        [-0.0399, -0.0044,  0.0473,  ..., -0.0400, -0.0623,  0.0383],\n",
      "        [-0.0275,  0.0208, -0.0399,  ..., -0.0187, -0.0249, -0.0416]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.9.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.9.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0197,  0.0040,  0.0211,  ...,  0.0540, -0.0524, -0.0414],\n",
      "        [-0.0242,  0.0610,  0.0150,  ..., -0.0209, -0.0287,  0.0311],\n",
      "        [-0.0194,  0.0405,  0.0055,  ...,  0.0478, -0.0522,  0.0180],\n",
      "        ...,\n",
      "        [-0.0465,  0.0274, -0.0440,  ...,  0.0218,  0.0278,  0.0185],\n",
      "        [-0.0260,  0.0471, -0.0516,  ...,  0.0172, -0.0570,  0.0400],\n",
      "        [ 0.0127, -0.0385,  0.0173,  ..., -0.0171, -0.0588,  0.0398]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.9.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.9.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.9.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.10.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0668,  0.0395, -0.0004,  ..., -0.1471,  0.0453,  0.0175],\n",
      "        [ 0.0671, -0.1068, -0.0105,  ...,  0.0779,  0.0038, -0.0300],\n",
      "        [-0.0308, -0.0107, -0.0375,  ...,  0.0803,  0.0948, -0.0399],\n",
      "        ...,\n",
      "        [ 0.0371,  0.0345,  0.2365,  ...,  0.0906, -0.0638,  0.0621],\n",
      "        [ 0.0191,  0.0181,  0.0627,  ...,  0.1147,  0.0485, -0.0492],\n",
      "        [ 0.0312, -0.0193,  0.0494,  ...,  0.0547,  0.0596, -0.0202]])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.0240,  0.0314, -0.0165,  ...,  0.0296,  0.0553,  0.0010])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0114,  0.0411,  0.0549,  ..., -0.0200, -0.0267, -0.0488],\n",
      "        [-0.0967, -0.0958,  0.0256,  ..., -0.0674,  0.0008, -0.0060],\n",
      "        [-0.0316,  0.0770, -0.0063,  ..., -0.1061,  0.0011, -0.0569],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0087,  0.2559,  ..., -0.0732, -0.0281,  0.0292],\n",
      "        [ 0.0110, -0.0090,  0.0813,  ..., -0.0077, -0.0020, -0.0164],\n",
      "        [-0.0285, -0.0459,  0.0270,  ...,  0.0627, -0.0080, -0.0252]])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.self.key.bias', Parameter containing:\n",
      "tensor([-0.0006, -0.0008,  0.0005,  ...,  0.0003,  0.0003, -0.0005])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0108,  0.0436,  0.0095,  ..., -0.0033, -0.0260, -0.0308],\n",
      "        [ 0.0254, -0.0047, -0.0097,  ..., -0.0139, -0.0032, -0.0180],\n",
      "        [ 0.0362,  0.0508,  0.0036,  ..., -0.0017,  0.0288, -0.0195],\n",
      "        ...,\n",
      "        [-0.0179,  0.0052, -0.0024,  ...,  0.0018,  0.0069, -0.0426],\n",
      "        [ 0.0129,  0.0164, -0.0078,  ...,  0.0193, -0.0385,  0.0125],\n",
      "        [-0.0268, -0.0146, -0.0151,  ..., -0.0245,  0.0116, -0.0153]])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0218,  0.0105, -0.0332,  ...,  0.0203, -0.0139,  0.0499])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0086,  0.0545, -0.0077,  ...,  0.0129,  0.0360,  0.0214],\n",
      "        [-0.0180, -0.0062, -0.0151,  ..., -0.0445,  0.0245, -0.0141],\n",
      "        [ 0.0307,  0.0225, -0.1379,  ..., -0.0699, -0.0630, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0060, -0.0074, -0.0363,  ..., -0.0074, -0.0069, -0.0202],\n",
      "        [ 0.0295, -0.0010,  0.0472,  ..., -0.0181,  0.0014,  0.0158],\n",
      "        [ 0.0526, -0.0041,  0.0057,  ..., -0.0058, -0.0108, -0.0267]])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0048,  0.0635,  0.0536,  ...,  0.0353, -0.0608,  0.0744])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9741, 0.9902, 1.0000,  ..., 0.9888, 0.9912, 0.9873])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1809, -0.0026, -0.4712,  ..., -0.0688, -0.1521, -0.0406])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0170, -0.0047,  0.0079,  ..., -0.0167, -0.0128, -0.0762],\n",
      "        [ 0.0044,  0.0204,  0.0335,  ...,  0.0525,  0.0372, -0.0066],\n",
      "        [-0.0009, -0.0162,  0.0282,  ...,  0.0289,  0.0335,  0.0027],\n",
      "        ...,\n",
      "        [ 0.0213,  0.0234,  0.0227,  ...,  0.0712, -0.0364,  0.0042],\n",
      "        [ 0.0516, -0.0172,  0.0032,  ...,  0.0350,  0.0613, -0.0302],\n",
      "        [-0.0083, -0.0191, -0.0078,  ...,  0.0173,  0.0031, -0.0231]])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0576, -0.0498, -0.0150,  ..., -0.0920, -0.0903, -0.0146])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0175,  0.0028, -0.0025,  ...,  0.0580,  0.0151, -0.0558],\n",
      "        [-0.0335,  0.0099,  0.0010,  ...,  0.0138, -0.0243, -0.0472],\n",
      "        [ 0.0066, -0.0061,  0.0110,  ..., -0.0004, -0.0114,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0103, -0.0139, -0.0114,  ...,  0.0241, -0.0045, -0.0060],\n",
      "        [-0.0190, -0.0946, -0.0464,  ...,  0.0260,  0.0098, -0.0483],\n",
      "        [-0.0257, -0.0003, -0.0446,  ...,  0.0070,  0.0239,  0.0109]])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.output.dense.bias', Parameter containing:\n",
      "tensor([-0.2128,  0.0545,  0.0881,  ..., -0.0428,  0.0073, -0.1137])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9780, 0.9873, 1.0000,  ..., 0.9785, 0.9775, 0.9863])) weights are unchanged.\n",
      "Layer ('encoder.layer.10.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0438, -0.0731, -0.2014,  ..., -0.0418,  0.0383, -0.0472])) weights are unchanged.\n",
      "Newly Added: encoder.layer.10.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0441,  0.0380,  0.0195,  ...,  0.0038, -0.0293, -0.0075],\n",
      "        [-0.0543,  0.0571, -0.0103,  ...,  0.0480, -0.0130,  0.0044],\n",
      "        [ 0.0216, -0.0130,  0.0351,  ...,  0.0500,  0.0331,  0.0624],\n",
      "        ...,\n",
      "        [-0.0072, -0.0480,  0.0136,  ..., -0.0408,  0.0163,  0.0022],\n",
      "        [-0.0203, -0.0555,  0.0478,  ...,  0.0611, -0.0081, -0.0513],\n",
      "        [-0.0031, -0.0421,  0.0451,  ..., -0.0485, -0.0193, -0.0303]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.10.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.10.up_layer.weight Parameter containing:\n",
      "tensor([[-5.7159e-02,  1.5216e-02,  5.0178e-02,  ..., -2.0267e-02,\n",
      "         -3.9077e-03,  1.4888e-02],\n",
      "        [ 2.2084e-02,  5.5756e-02,  9.2131e-03,  ...,  2.8524e-02,\n",
      "         -3.1585e-02, -2.3851e-02],\n",
      "        [ 4.0714e-02, -5.7052e-02, -4.2059e-02,  ...,  4.6883e-02,\n",
      "         -5.1737e-02, -7.2196e-06],\n",
      "        ...,\n",
      "        [ 3.1917e-02, -4.2760e-02,  1.3102e-02,  ...,  2.8881e-02,\n",
      "         -5.9916e-02,  2.1295e-02],\n",
      "        [ 4.4078e-02, -2.9518e-02,  2.8186e-02,  ..., -4.7337e-02,\n",
      "          2.0544e-02, -6.0744e-02],\n",
      "        [-1.8342e-02,  1.9162e-02, -6.1069e-03,  ...,  4.3262e-02,\n",
      "         -9.8158e-03,  5.4649e-02]], requires_grad=True)\n",
      "Newly Added: encoder.layer.10.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.10.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.10.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.11.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0359, -0.0105,  0.0260,  ...,  0.0173,  0.0845,  0.0364],\n",
      "        [ 0.0718, -0.0424, -0.0025,  ..., -0.0235,  0.0124,  0.0317],\n",
      "        [ 0.0531, -0.0502, -0.2163,  ...,  0.0499,  0.0731, -0.0330],\n",
      "        ...,\n",
      "        [ 0.0942, -0.0224,  0.0830,  ..., -0.0426,  0.0641, -0.0682],\n",
      "        [-0.0686, -0.0214, -0.0633,  ..., -0.0210, -0.0335, -0.0839],\n",
      "        [ 0.0368,  0.0057, -0.0370,  ..., -0.0504, -0.0948, -0.0488]])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.self.query.bias', Parameter containing:\n",
      "tensor([0.0101, 0.0287, 0.0471,  ..., 0.1022, 0.0180, 0.0231])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0868, -0.0034,  0.0750,  ...,  0.0058, -0.0243, -0.0135],\n",
      "        [-0.0622, -0.0167, -0.0323,  ...,  0.0196, -0.0278,  0.0744],\n",
      "        [-0.1177,  0.0089, -0.2360,  ..., -0.0573, -0.0034,  0.0053],\n",
      "        ...,\n",
      "        [-0.0395, -0.0316,  0.1110,  ..., -0.0794, -0.0348, -0.0245],\n",
      "        [-0.0146,  0.0096, -0.0642,  ..., -0.1068,  0.0084,  0.0112],\n",
      "        [ 0.0099, -0.0781, -0.0356,  ..., -0.0432, -0.0186, -0.0165]])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 7.2336e-04, -4.6968e-05, -6.4433e-05,  ...,  8.2135e-05,\n",
      "         1.0127e-04,  1.1945e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0105, -0.0314,  0.0028,  ...,  0.0624,  0.0033, -0.0007],\n",
      "        [ 0.0258, -0.0349, -0.0043,  ..., -0.0773, -0.0299,  0.0397],\n",
      "        [-0.0460, -0.0108,  0.0267,  ..., -0.0847, -0.0278,  0.0197],\n",
      "        ...,\n",
      "        [-0.0057,  0.0449, -0.0015,  ...,  0.0018, -0.0459, -0.0010],\n",
      "        [ 0.0418,  0.0280,  0.0008,  ...,  0.0479,  0.0335, -0.0786],\n",
      "        [-0.0130,  0.0006,  0.0019,  ..., -0.0466,  0.0334, -0.0237]])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0098,  0.0112,  0.0059,  ..., -0.0277,  0.0116, -0.0346])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0064, -0.0247,  0.0216,  ...,  0.0061,  0.0041, -0.0246],\n",
      "        [ 0.0448,  0.0078,  0.0229,  ...,  0.0075, -0.0028,  0.0312],\n",
      "        [-0.0249,  0.0068, -0.0322,  ...,  0.0071,  0.0181, -0.0200],\n",
      "        ...,\n",
      "        [-0.0108,  0.0028,  0.0651,  ...,  0.0223,  0.0253, -0.0173],\n",
      "        [ 0.0073,  0.0197,  0.0347,  ...,  0.0108,  0.0302, -0.0065],\n",
      "        [ 0.0246, -0.0564, -0.0038,  ..., -0.0197, -0.0256, -0.0251]])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0512,  0.1448, -0.1544,  ...,  0.0590, -0.0475,  0.0369])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9819, 0.9927, 0.9995,  ..., 0.9932, 0.9951, 0.9814])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.2123,  0.0092, -0.3467,  ..., -0.0619, -0.0711, -0.0355])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0066, -0.0360, -0.0077,  ...,  0.0530,  0.0284,  0.0414],\n",
      "        [ 0.1045,  0.0584,  0.0039,  ...,  0.0587,  0.0348, -0.0853],\n",
      "        [ 0.0033,  0.0461, -0.0063,  ...,  0.0395,  0.0085, -0.0366],\n",
      "        ...,\n",
      "        [ 0.0612,  0.0117,  0.0085,  ..., -0.0023,  0.0500, -0.1039],\n",
      "        [ 0.0515,  0.0045,  0.0147,  ...,  0.0644, -0.0276,  0.0378],\n",
      "        [ 0.0696,  0.0207,  0.0364,  ...,  0.0150,  0.0030,  0.0118]])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.intermediate.dense.bias', Parameter containing:\n",
      "tensor([ 0.0242, -0.0457, -0.0556,  ..., -0.0540, -0.0638,  0.0376])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0081,  0.0322, -0.0033,  ...,  0.0089,  0.0267, -0.0505],\n",
      "        [ 0.0181,  0.0423,  0.0047,  ..., -0.0196,  0.0453,  0.0066],\n",
      "        [-0.0157,  0.0210, -0.0084,  ..., -0.0044, -0.0071, -0.0028],\n",
      "        ...,\n",
      "        [ 0.0507, -0.0553,  0.0636,  ..., -0.0220, -0.0163, -0.0121],\n",
      "        [ 0.0124,  0.0298, -0.0421,  ...,  0.0137, -0.0341,  0.0141],\n",
      "        [ 0.0262, -0.0401,  0.0013,  ..., -0.0313, -0.0600, -0.0131]])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1758, -0.0094, -0.0047,  ..., -0.0142, -0.0839, -0.1055])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9868, 0.9902, 0.9995,  ..., 0.9785, 0.9819, 0.9912])) weights are unchanged.\n",
      "Layer ('encoder.layer.11.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0854, -0.0873,  0.1804,  ..., -0.0376, -0.0192, -0.0719])) weights are unchanged.\n",
      "Newly Added: encoder.layer.11.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0207, -0.0393,  0.0549,  ..., -0.0229,  0.0551,  0.0277],\n",
      "        [ 0.0602,  0.0008,  0.0473,  ..., -0.0426, -0.0017,  0.0377],\n",
      "        [-0.0158,  0.0010,  0.0028,  ..., -0.0248,  0.0207, -0.0591],\n",
      "        ...,\n",
      "        [-0.0284,  0.0425, -0.0064,  ..., -0.0473, -0.0512, -0.0530],\n",
      "        [-0.0337, -0.0582, -0.0190,  ..., -0.0259, -0.0386,  0.0243],\n",
      "        [-0.0288, -0.0324,  0.0211,  ..., -0.0016, -0.0427, -0.0501]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.11.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.11.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0289, -0.0430, -0.0586,  ...,  0.0043, -0.0120,  0.0196],\n",
      "        [ 0.0228,  0.0374,  0.0089,  ..., -0.0397, -0.0366, -0.0172],\n",
      "        [ 0.0119, -0.0169, -0.0022,  ..., -0.0193, -0.0181, -0.0456],\n",
      "        ...,\n",
      "        [ 0.0096,  0.0273, -0.0449,  ..., -0.0116,  0.0199,  0.0513],\n",
      "        [ 0.0371,  0.0450,  0.0365,  ...,  0.0258, -0.0117,  0.0489],\n",
      "        [-0.0569, -0.0461,  0.0348,  ..., -0.0255,  0.0537,  0.0058]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.11.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.11.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.11.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.12.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-4.8248e-02, -2.4353e-02,  8.8013e-02,  ...,  2.8210e-03,\n",
      "          5.3040e-02,  5.8960e-02],\n",
      "        [-2.2797e-02,  6.5918e-03,  1.3893e-02,  ..., -7.9712e-02,\n",
      "         -6.4209e-02,  5.5054e-02],\n",
      "        [-5.6580e-02,  3.9429e-02, -1.3452e-01,  ...,  5.6244e-02,\n",
      "          2.9358e-02,  5.0140e-02],\n",
      "        ...,\n",
      "        [ 2.6474e-02,  2.0798e-02,  2.8906e-01,  ..., -1.4511e-02,\n",
      "          9.4788e-02, -2.3163e-02],\n",
      "        [-8.4717e-02,  1.3260e-02, -1.0864e-01,  ..., -3.3783e-02,\n",
      "          1.4267e-02, -2.4338e-02],\n",
      "        [-1.7214e-04, -4.5509e-03, -3.1311e-02,  ..., -9.0576e-02,\n",
      "         -4.3030e-03, -7.9468e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.0043, -0.0057,  0.0025,  ..., -0.2384, -0.0195, -0.0033])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-2.5574e-02, -1.2622e-01,  7.4524e-02,  ...,  3.8239e-02,\n",
      "         -1.8127e-02,  6.6772e-02],\n",
      "        [ 1.2213e-01,  2.3590e-02, -7.7271e-02,  ..., -1.1098e-04,\n",
      "         -2.7359e-02, -1.6998e-02],\n",
      "        [-4.4128e-02,  3.1464e-02, -1.0779e-01,  ..., -7.7698e-02,\n",
      "         -4.5410e-02,  3.5744e-03],\n",
      "        ...,\n",
      "        [-2.8687e-03,  1.6907e-02, -3.7109e-01,  ...,  5.7739e-02,\n",
      "         -6.8855e-03, -9.5749e-03],\n",
      "        [ 2.4353e-02, -5.4283e-03, -1.9458e-01,  ..., -3.4790e-02,\n",
      "          8.0032e-03,  5.6671e-02],\n",
      "        [ 3.2539e-03, -3.9032e-02, -9.5947e-02,  ...,  3.8177e-02,\n",
      "          9.7275e-03,  9.8190e-03]])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 1.3804e-04, -1.0312e-04,  8.2076e-05,  ..., -1.7776e-02,\n",
      "         4.8614e-04, -1.2803e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0893, -0.0031,  0.0086,  ...,  0.0463, -0.0019, -0.0062],\n",
      "        [ 0.0316,  0.0995, -0.0115,  ..., -0.0075,  0.0302,  0.0842],\n",
      "        [ 0.0159,  0.0052, -0.0010,  ..., -0.0360, -0.0432,  0.0349],\n",
      "        ...,\n",
      "        [-0.0188, -0.0383, -0.0152,  ..., -0.0053, -0.0154,  0.0626],\n",
      "        [-0.0364, -0.0448,  0.0235,  ..., -0.0142, -0.0369,  0.0624],\n",
      "        [ 0.0020, -0.0307,  0.0003,  ..., -0.0330,  0.0365,  0.0232]])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0051, -0.0076,  0.0001,  ...,  0.0301,  0.0155,  0.0311])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0450, -0.0196, -0.0211,  ...,  0.0204,  0.0004,  0.0002],\n",
      "        [-0.0482,  0.0172, -0.0388,  ..., -0.0312,  0.0041,  0.0178],\n",
      "        [-0.0409, -0.0195,  0.0294,  ...,  0.0137,  0.0365,  0.0012],\n",
      "        ...,\n",
      "        [ 0.0179, -0.0126, -0.0410,  ..., -0.0137,  0.0118,  0.0297],\n",
      "        [ 0.0020, -0.0267,  0.0167,  ..., -0.0174,  0.0195, -0.0211],\n",
      "        [ 0.0055,  0.0380,  0.0408,  ..., -0.0110, -0.0284, -0.0253]])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0294, -0.0021,  0.0633,  ..., -0.0137, -0.0420, -0.0079])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9917, 0.9810, 1.0000,  ..., 0.9883, 0.9834, 0.9878])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1647,  0.0041, -0.3013,  ..., -0.0280, -0.1448, -0.0363])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0728, -0.0337,  0.0162,  ..., -0.0141, -0.0043,  0.0090],\n",
      "        [-0.0167, -0.0280,  0.0182,  ..., -0.0484, -0.0390, -0.0160],\n",
      "        [-0.0386, -0.0446,  0.0778,  ..., -0.0022, -0.0158, -0.0240],\n",
      "        ...,\n",
      "        [ 0.0212, -0.0458,  0.0282,  ..., -0.1234, -0.0192, -0.0034],\n",
      "        [ 0.0130, -0.0501,  0.0147,  ..., -0.0493, -0.0166,  0.0440],\n",
      "        [ 0.0185,  0.0066, -0.0056,  ...,  0.0158, -0.0275, -0.0091]])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.1078, -0.0406, -0.0113,  ..., -0.1227, -0.0853, -0.1052])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0306, -0.0029, -0.0674,  ...,  0.0268,  0.0262, -0.0149],\n",
      "        [-0.0147,  0.0207,  0.0109,  ..., -0.0289,  0.0092, -0.0362],\n",
      "        [-0.0001,  0.0008, -0.0151,  ..., -0.0019, -0.0288, -0.0024],\n",
      "        ...,\n",
      "        [-0.0101, -0.0117,  0.0156,  ..., -0.0420, -0.0115, -0.0244],\n",
      "        [-0.0176, -0.0110, -0.0232,  ..., -0.0110, -0.0246,  0.0201],\n",
      "        [-0.0544, -0.0361,  0.0327,  ...,  0.0056, -0.0025, -0.0104]])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.output.dense.bias', Parameter containing:\n",
      "tensor([-0.2179,  0.1670, -0.0790,  ..., -0.0088, -0.0806, -0.1232])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9873, 0.9902, 0.9961,  ..., 0.9902, 0.9829, 0.9912])) weights are unchanged.\n",
      "Layer ('encoder.layer.12.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0085, -0.0781,  0.0873,  ..., -0.0697, -0.0124, -0.0617])) weights are unchanged.\n",
      "Newly Added: encoder.layer.12.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0515, -0.0283,  0.0163,  ..., -0.0614,  0.0379, -0.0257],\n",
      "        [ 0.0502,  0.0398,  0.0042,  ...,  0.0024, -0.0026,  0.0345],\n",
      "        [-0.0407,  0.0263, -0.0193,  ..., -0.0068, -0.0554, -0.0411],\n",
      "        ...,\n",
      "        [-0.0501,  0.0597,  0.0146,  ...,  0.0029, -0.0075, -0.0618],\n",
      "        [ 0.0315,  0.0184, -0.0362,  ...,  0.0318, -0.0416,  0.0389],\n",
      "        [-0.0204, -0.0498, -0.0039,  ..., -0.0074, -0.0062,  0.0365]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.12.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.12.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0444,  0.0027,  0.0078,  ..., -0.0117, -0.0141,  0.0338],\n",
      "        [-0.0445,  0.0546, -0.0189,  ..., -0.0282,  0.0312, -0.0050],\n",
      "        [ 0.0302,  0.0035,  0.0554,  ..., -0.0425,  0.0043,  0.0432],\n",
      "        ...,\n",
      "        [ 0.0532,  0.0427, -0.0371,  ..., -0.0423, -0.0096,  0.0414],\n",
      "        [-0.0172,  0.0505,  0.0299,  ...,  0.0418, -0.0023, -0.0357],\n",
      "        [ 0.0431, -0.0540, -0.0478,  ..., -0.0392,  0.0180, -0.0282]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.12.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.12.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.12.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.13.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0455,  0.0479, -0.0504,  ..., -0.0465,  0.0422, -0.0587],\n",
      "        [-0.0257, -0.0095,  0.0492,  ...,  0.0336,  0.0297,  0.0588],\n",
      "        [-0.0049, -0.0271,  0.0013,  ..., -0.0378, -0.0797, -0.0139],\n",
      "        ...,\n",
      "        [ 0.0635, -0.0097,  0.0294,  ...,  0.0070, -0.0195, -0.0164],\n",
      "        [-0.0054, -0.0265, -0.1438,  ...,  0.0446,  0.0222, -0.0248],\n",
      "        [ 0.0502,  0.0561, -0.1592,  ...,  0.0250,  0.0377,  0.0656]])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0478,  0.0441,  0.0069,  ...,  0.0427,  0.1793,  0.0486])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0273,  0.0611, -0.0421,  ..., -0.0171,  0.0268, -0.0717],\n",
      "        [ 0.0547, -0.0045,  0.0743,  ..., -0.0325, -0.0027, -0.0322],\n",
      "        [ 0.0187,  0.0651, -0.0432,  ...,  0.1571,  0.0240, -0.0781],\n",
      "        ...,\n",
      "        [-0.0465, -0.0481, -0.0114,  ..., -0.0238,  0.0125, -0.0228],\n",
      "        [ 0.0216,  0.0326, -0.2133,  ..., -0.0190, -0.0205,  0.0028],\n",
      "        [-0.0430,  0.0468, -0.1322,  ...,  0.0230,  0.0081,  0.0321]])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 1.1492e-04, -1.3244e-04, -1.2541e-04,  ..., -2.2590e-05,\n",
      "         5.2452e-04, -2.1493e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 5.4169e-02,  6.5857e-02, -1.5411e-02,  ...,  4.0192e-02,\n",
      "          6.0654e-03,  3.8544e-02],\n",
      "        [ 1.5060e-02,  5.4688e-02,  6.7673e-03,  ..., -3.4485e-02,\n",
      "         -2.1866e-02,  2.2156e-02],\n",
      "        [-4.0131e-03,  5.0476e-02,  9.4509e-04,  ..., -2.2385e-02,\n",
      "         -5.5237e-02,  3.9032e-02],\n",
      "        ...,\n",
      "        [ 4.1687e-02, -7.2815e-02,  1.9485e-02,  ...,  2.4438e-04,\n",
      "         -1.5373e-02, -2.3651e-02],\n",
      "        [-6.4707e-04,  1.3819e-03, -3.8727e-02,  ...,  7.4501e-03,\n",
      "         -1.1011e-01, -9.1324e-03],\n",
      "        [-4.1962e-02, -9.0210e-02,  2.5192e-02,  ..., -4.9829e-05,\n",
      "         -7.6027e-03, -1.8570e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0120,  0.0083,  0.0064,  ..., -0.0070, -0.0073, -0.0006])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 1.1597e-03, -3.9368e-02,  3.0930e-02,  ..., -5.1788e-02,\n",
      "          7.5042e-05,  1.0547e-01],\n",
      "        [ 8.6365e-03,  5.9090e-03, -8.0338e-03,  ...,  9.2621e-03,\n",
      "         -6.3110e-02,  4.0710e-02],\n",
      "        [-3.0231e-03, -1.0353e-02, -1.6602e-02,  ...,  1.4336e-02,\n",
      "         -1.9272e-02, -9.4604e-04],\n",
      "        ...,\n",
      "        [-3.7781e-02, -4.0283e-03, -4.6417e-02,  ...,  1.6050e-03,\n",
      "         -4.2786e-02,  1.6785e-02],\n",
      "        [-4.2725e-02,  5.7617e-02, -2.3499e-03,  ...,  1.5976e-02,\n",
      "          7.9773e-02, -2.9816e-02],\n",
      "        [-7.5340e-04,  2.4319e-03,  1.3817e-02,  ...,  2.0889e-02,\n",
      "          3.5919e-02,  5.2246e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0424,  0.0432, -0.0920,  ...,  0.0381, -0.0336,  0.0339])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9800, 0.9888, 0.9995,  ..., 0.9907, 0.9854, 0.9800])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1782,  0.0724, -0.2690,  ..., -0.0588, -0.1191, -0.0616])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0206, -0.0216,  0.0201,  ..., -0.0340, -0.0562, -0.0418],\n",
      "        [ 0.0350,  0.0091,  0.0103,  ...,  0.0518,  0.0219, -0.0013],\n",
      "        [ 0.0593, -0.0431,  0.0546,  ...,  0.1133, -0.0112, -0.0639],\n",
      "        ...,\n",
      "        [ 0.0290, -0.0111, -0.0066,  ..., -0.0107,  0.0370,  0.0054],\n",
      "        [ 0.0091,  0.0431,  0.0933,  ...,  0.0428,  0.0407, -0.0331],\n",
      "        [ 0.0352, -0.1298,  0.0410,  ...,  0.0057,  0.0025, -0.0753]])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.1381, -0.0530, -0.0888,  ..., -0.1020, -0.0327, -0.0376])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0632, -0.0329, -0.0447,  ..., -0.0350,  0.0098, -0.0068],\n",
      "        [-0.0083,  0.0006, -0.0560,  ...,  0.0086,  0.0221, -0.0253],\n",
      "        [ 0.0144,  0.0069,  0.0032,  ..., -0.0019,  0.0063, -0.0035],\n",
      "        ...,\n",
      "        [-0.0301,  0.0345,  0.0015,  ..., -0.0324,  0.0109,  0.0352],\n",
      "        [-0.0869, -0.0237,  0.0318,  ..., -0.0194,  0.0598,  0.0162],\n",
      "        [-0.0275, -0.0035, -0.0280,  ..., -0.0252, -0.0013, -0.0437]])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1068,  0.0576, -0.0722,  ..., -0.0243, -0.0502, -0.0876])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9854, 0.9917, 0.9976,  ..., 0.9893, 0.9883, 0.9941])) weights are unchanged.\n",
      "Layer ('encoder.layer.13.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([ 0.0677, -0.1158,  0.2407,  ..., -0.0533, -0.0042, -0.0442])) weights are unchanged.\n",
      "Newly Added: encoder.layer.13.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0018,  0.0095, -0.0438,  ..., -0.0339, -0.0187, -0.0260],\n",
      "        [ 0.0119,  0.0314, -0.0185,  ..., -0.0280, -0.0284, -0.0296],\n",
      "        [-0.0160, -0.0584, -0.0103,  ..., -0.0405,  0.0186,  0.0185],\n",
      "        ...,\n",
      "        [ 0.0509,  0.0119, -0.0538,  ...,  0.0447,  0.0404, -0.0144],\n",
      "        [-0.0362, -0.0575,  0.0380,  ..., -0.0254,  0.0493, -0.0338],\n",
      "        [-0.0521, -0.0055, -0.0586,  ..., -0.0380,  0.0607, -0.0196]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.13.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.13.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0622, -0.0552, -0.0337,  ..., -0.0205, -0.0425, -0.0147],\n",
      "        [-0.0397, -0.0508, -0.0142,  ..., -0.0596, -0.0513, -0.0099],\n",
      "        [-0.0069, -0.0049,  0.0243,  ..., -0.0219,  0.0080, -0.0210],\n",
      "        ...,\n",
      "        [-0.0443,  0.0422, -0.0152,  ..., -0.0300,  0.0298,  0.0498],\n",
      "        [-0.0549,  0.0467,  0.0319,  ..., -0.0063, -0.0321, -0.0280],\n",
      "        [-0.0064, -0.0593,  0.0054,  ...,  0.0184, -0.0599,  0.0231]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.13.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.13.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.13.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.14.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0418, -0.0572,  0.0107,  ..., -0.1229, -0.0238, -0.1603],\n",
      "        [-0.0687,  0.0298, -0.0959,  ...,  0.0302, -0.0210, -0.0344],\n",
      "        [-0.0059, -0.0319,  0.1085,  ..., -0.0079, -0.0604,  0.0327],\n",
      "        ...,\n",
      "        [ 0.0638,  0.0020,  0.0229,  ..., -0.0384, -0.0695,  0.0419],\n",
      "        [-0.0183,  0.0397, -0.0165,  ..., -0.0928, -0.0313,  0.0583],\n",
      "        [-0.0709,  0.0263,  0.0084,  ...,  0.0567,  0.0292,  0.0414]])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.1929,  0.1409, -0.1237,  ...,  0.2324, -0.1196,  0.0184])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0361,  0.0812,  0.0727,  ...,  0.0073,  0.0036, -0.0072],\n",
      "        [-0.0668, -0.0050, -0.1243,  ..., -0.0121,  0.0117,  0.0431],\n",
      "        [-0.0206,  0.0272,  0.0479,  ...,  0.0236, -0.1163,  0.0597],\n",
      "        ...,\n",
      "        [ 0.0782, -0.0606,  0.0646,  ..., -0.0999, -0.0449, -0.0251],\n",
      "        [ 0.0454,  0.0769,  0.0446,  ..., -0.0770,  0.0686,  0.0378],\n",
      "        [-0.0684, -0.0022,  0.0574,  ..., -0.0072, -0.0539, -0.0274]])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 1.7738e-04,  3.3617e-05, -9.8884e-05,  ..., -9.8407e-05,\n",
      "        -1.3292e-05,  8.5413e-05])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0426,  0.0020,  0.0435,  ..., -0.0439, -0.0045, -0.0067],\n",
      "        [-0.0024, -0.0706, -0.0085,  ...,  0.0609,  0.0624, -0.0542],\n",
      "        [ 0.0273, -0.0343, -0.0240,  ..., -0.0640,  0.0098,  0.0175],\n",
      "        ...,\n",
      "        [ 0.0815,  0.0248, -0.0183,  ...,  0.0238, -0.0285, -0.0345],\n",
      "        [ 0.0719,  0.0167, -0.0145,  ..., -0.0026, -0.0222,  0.0306],\n",
      "        [-0.0723, -0.0345, -0.0237,  ..., -0.0012,  0.0480, -0.1311]])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.self.value.bias', Parameter containing:\n",
      "tensor([-1.6034e-05,  6.6223e-03, -1.1932e-02,  ..., -2.5606e-04,\n",
      "         4.6997e-03,  1.3130e-02])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0121, -0.0237,  0.0357,  ..., -0.0438, -0.0008,  0.0846],\n",
      "        [ 0.0157, -0.0408,  0.0494,  ..., -0.0697, -0.0416, -0.0058],\n",
      "        [ 0.0408, -0.0354,  0.0162,  ...,  0.0049,  0.0071, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0660, -0.0345,  0.0698,  ...,  0.0081, -0.0047, -0.0087],\n",
      "        [-0.0057, -0.0632, -0.0202,  ...,  0.0262,  0.0090, -0.0085],\n",
      "        [-0.0124, -0.0319,  0.0255,  ...,  0.0642,  0.0189,  0.0696]])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0237,  0.0022,  0.0312,  ...,  0.0649, -0.0415, -0.0053])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9976, 0.9834, 0.9995,  ..., 0.9897, 0.9907, 0.9897])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0862, -0.0132, -0.1949,  ..., -0.0627, -0.1387, -0.0533])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0089, -0.0012,  0.0392,  ...,  0.0006, -0.0177,  0.0384],\n",
      "        [ 0.0134, -0.0522, -0.0091,  ...,  0.0626, -0.0230,  0.0482],\n",
      "        [ 0.0454, -0.0664,  0.0102,  ...,  0.0639,  0.0668, -0.0522],\n",
      "        ...,\n",
      "        [ 0.0739, -0.0393,  0.0083,  ...,  0.0084, -0.0064, -0.0343],\n",
      "        [-0.0350, -0.0586, -0.0121,  ...,  0.0300,  0.0122,  0.0179],\n",
      "        [ 0.0352, -0.0425, -0.0666,  ...,  0.0371, -0.0159,  0.0051]])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.1082, -0.1091, -0.1114,  ..., -0.0575, -0.0803, -0.0822])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0192,  0.0286,  0.0078,  ...,  0.0274,  0.0086, -0.0657],\n",
      "        [ 0.0073, -0.0104, -0.0873,  ..., -0.0097,  0.0153, -0.0649],\n",
      "        [-0.0104, -0.0191,  0.0363,  ...,  0.0092,  0.0053,  0.0124],\n",
      "        ...,\n",
      "        [ 0.0606,  0.0090,  0.0072,  ..., -0.0466,  0.0364, -0.0313],\n",
      "        [-0.0511,  0.0133,  0.0243,  ...,  0.0441,  0.0016, -0.0316],\n",
      "        [ 0.0271,  0.0105, -0.0592,  ..., -0.0047, -0.0201, -0.0696]])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1095,  0.0499, -0.1957,  ..., -0.0115, -0.1292, -0.0691])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9902, 0.9932, 0.9980,  ..., 0.9912, 0.9902, 0.9937])) weights are unchanged.\n",
      "Layer ('encoder.layer.14.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0243, -0.1097,  0.2498,  ..., -0.0511, -0.0148, -0.0590])) weights are unchanged.\n",
      "Newly Added: encoder.layer.14.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0561,  0.0463, -0.0544,  ...,  0.0504,  0.0538, -0.0516],\n",
      "        [-0.0550, -0.0294, -0.0524,  ...,  0.0135, -0.0387, -0.0233],\n",
      "        [ 0.0547,  0.0510, -0.0390,  ...,  0.0474, -0.0079,  0.0071],\n",
      "        ...,\n",
      "        [ 0.0526,  0.0577, -0.0550,  ...,  0.0238, -0.0247,  0.0045],\n",
      "        [-0.0609, -0.0007,  0.0524,  ...,  0.0087, -0.0273, -0.0531],\n",
      "        [ 0.0080,  0.0204, -0.0593,  ...,  0.0511, -0.0430, -0.0101]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.14.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.14.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0341,  0.0371, -0.0463,  ..., -0.0541,  0.0185, -0.0598],\n",
      "        [-0.0592,  0.0178, -0.0540,  ...,  0.0466,  0.0593, -0.0612],\n",
      "        [-0.0421,  0.0101, -0.0389,  ..., -0.0073,  0.0343, -0.0433],\n",
      "        ...,\n",
      "        [ 0.0317, -0.0050,  0.0142,  ...,  0.0466, -0.0609, -0.0312],\n",
      "        [-0.0554,  0.0023,  0.0584,  ...,  0.0461, -0.0330,  0.0099],\n",
      "        [-0.0130,  0.0203,  0.0600,  ..., -0.0037,  0.0543,  0.0473]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.14.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.14.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.14.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.15.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0603,  0.0549, -0.1349,  ..., -0.0166,  0.0379,  0.0195],\n",
      "        [-0.0213,  0.0183,  0.0864,  ...,  0.0865, -0.0282, -0.0223],\n",
      "        [-0.0031, -0.0199, -0.0308,  ..., -0.0581,  0.0585, -0.0828],\n",
      "        ...,\n",
      "        [ 0.0503,  0.1158, -0.0247,  ...,  0.0257, -0.0180,  0.0143],\n",
      "        [-0.0176,  0.0906,  0.0346,  ..., -0.0041, -0.0627,  0.0394],\n",
      "        [-0.0561, -0.0343,  0.0112,  ..., -0.0132,  0.0933, -0.0541]])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0129, -0.0730, -0.0246,  ..., -0.1173, -0.0878, -0.0032])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0439,  0.0160, -0.2184,  ..., -0.0843, -0.0103, -0.0081],\n",
      "        [ 0.0632, -0.0185, -0.0567,  ..., -0.0118,  0.0014, -0.0240],\n",
      "        [-0.0313,  0.0868, -0.0751,  ..., -0.0443, -0.0617, -0.0159],\n",
      "        ...,\n",
      "        [ 0.0161,  0.0137, -0.0536,  ..., -0.1035,  0.0357, -0.0165],\n",
      "        [-0.0976, -0.0162, -0.0396,  ..., -0.0035,  0.0132,  0.0325],\n",
      "        [ 0.0533, -0.0453,  0.0261,  ..., -0.0328,  0.0472,  0.0596]])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 1.5175e-04,  2.8491e-05,  7.7367e-05,  ...,  4.1699e-04,\n",
      "         1.2910e-04, -2.3174e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0675,  0.0462,  0.0061,  ...,  0.0010, -0.0249,  0.0655],\n",
      "        [-0.0136, -0.0303, -0.0319,  ..., -0.0392,  0.0378, -0.0667],\n",
      "        [-0.0002, -0.0093,  0.0031,  ..., -0.1061,  0.1046,  0.0972],\n",
      "        ...,\n",
      "        [ 0.0162, -0.0580,  0.0007,  ...,  0.0371, -0.0079,  0.0765],\n",
      "        [-0.0472, -0.0864,  0.0048,  ...,  0.0608,  0.0164,  0.0131],\n",
      "        [-0.0640,  0.0043, -0.0136,  ..., -0.0651, -0.1081,  0.0020]])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0044,  0.0052,  0.0011,  ...,  0.0032, -0.0049, -0.0215])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0145,  0.0036, -0.0207,  ..., -0.0356, -0.0312,  0.0246],\n",
      "        [ 0.0096,  0.0243, -0.0348,  ..., -0.0030,  0.0482, -0.0247],\n",
      "        [-0.0271, -0.0385,  0.0714,  ..., -0.0159, -0.0170, -0.0122],\n",
      "        ...,\n",
      "        [-0.0029,  0.0529,  0.0168,  ..., -0.0370, -0.0089,  0.0220],\n",
      "        [-0.0599,  0.0954,  0.0481,  ..., -0.0032,  0.0190,  0.0881],\n",
      "        [ 0.0252, -0.0557,  0.0103,  ..., -0.0115, -0.0378, -0.0202]])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0166,  0.0168,  0.0535,  ...,  0.0018, -0.0478, -0.0282])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9907, 0.9902, 0.9995,  ..., 0.9912, 0.9912, 0.9863])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1105, -0.0164, -0.1700,  ..., -0.0920, -0.0811, -0.1239])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0213, -0.0180,  0.0235,  ...,  0.0108,  0.0501, -0.0308],\n",
      "        [ 0.0533, -0.0539,  0.0039,  ..., -0.0229, -0.0466, -0.0109],\n",
      "        [-0.0267,  0.0527,  0.0239,  ...,  0.0947,  0.0064,  0.0230],\n",
      "        ...,\n",
      "        [-0.0404, -0.0328,  0.0203,  ...,  0.0801,  0.0421,  0.0255],\n",
      "        [-0.0407,  0.0697,  0.0570,  ..., -0.0116,  0.0226, -0.0111],\n",
      "        [ 0.0679,  0.0079,  0.0062,  ...,  0.0431,  0.0704,  0.0233]])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0442, -0.1065, -0.0770,  ..., -0.0345,  0.0104, -0.0830])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0236,  0.0637, -0.0440,  ...,  0.0001,  0.0173, -0.0096],\n",
      "        [-0.0219,  0.0112, -0.0345,  ..., -0.0557, -0.0029, -0.0037],\n",
      "        [-0.0170, -0.0079, -0.0259,  ..., -0.0027,  0.0165,  0.0137],\n",
      "        ...,\n",
      "        [ 0.0050,  0.0238,  0.0551,  ..., -0.0087,  0.0036, -0.0013],\n",
      "        [-0.0014, -0.0082,  0.0664,  ..., -0.0423, -0.0170,  0.0014],\n",
      "        [-0.0456,  0.0188,  0.0205,  ...,  0.0323, -0.0090,  0.0380]])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.output.dense.bias', Parameter containing:\n",
      "tensor([-0.1302,  0.0304, -0.2482,  ..., -0.0318, -0.0862, -0.0997])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9878, 0.9858, 0.9980,  ..., 0.9858, 0.9956, 0.9873])) weights are unchanged.\n",
      "Layer ('encoder.layer.15.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0084, -0.0667,  0.1746,  ..., -0.0156, -0.0310, -0.0163])) weights are unchanged.\n",
      "Newly Added: encoder.layer.15.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0609, -0.0054, -0.0088,  ...,  0.0006, -0.0566,  0.0349],\n",
      "        [ 0.0543,  0.0577,  0.0180,  ...,  0.0292, -0.0618,  0.0605],\n",
      "        [ 0.0168,  0.0358,  0.0151,  ...,  0.0375, -0.0601,  0.0395],\n",
      "        ...,\n",
      "        [ 0.0204,  0.0469,  0.0141,  ...,  0.0261, -0.0331,  0.0381],\n",
      "        [ 0.0196,  0.0526,  0.0561,  ..., -0.0484, -0.0078, -0.0057],\n",
      "        [-0.0457,  0.0575,  0.0332,  ..., -0.0170, -0.0314,  0.0251]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.15.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.15.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0445,  0.0263,  0.0030,  ..., -0.0439,  0.0317, -0.0426],\n",
      "        [ 0.0116, -0.0050, -0.0158,  ..., -0.0027, -0.0570, -0.0199],\n",
      "        [ 0.0025, -0.0328,  0.0322,  ..., -0.0141,  0.0046, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0185,  0.0563,  0.0167,  ..., -0.0606,  0.0079,  0.0335],\n",
      "        [-0.0596,  0.0622, -0.0452,  ...,  0.0283,  0.0448,  0.0620],\n",
      "        [ 0.0396,  0.0527,  0.0557,  ...,  0.0034,  0.0140, -0.0545]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.15.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.15.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.15.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.16.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0181,  0.0007, -0.0403,  ..., -0.0125,  0.0904,  0.0359],\n",
      "        [-0.0462, -0.0651, -0.0251,  ...,  0.0128, -0.0204, -0.0379],\n",
      "        [ 0.0030, -0.0060,  0.0837,  ..., -0.0683, -0.0606, -0.0084],\n",
      "        ...,\n",
      "        [ 0.0708, -0.0920, -0.0287,  ..., -0.0148,  0.0367,  0.0216],\n",
      "        [-0.0503,  0.0418,  0.0564,  ..., -0.0879,  0.0313, -0.1166],\n",
      "        [ 0.0139,  0.0086,  0.0346,  ..., -0.0363, -0.0885, -0.0324]])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0086, -0.0678,  0.0128,  ..., -0.0054,  0.0858,  0.0125])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0545, -0.0127, -0.0312,  ...,  0.0773, -0.0125, -0.0558],\n",
      "        [ 0.0523, -0.0141, -0.0301,  ...,  0.0747, -0.0461, -0.0289],\n",
      "        [-0.0166,  0.0686,  0.0809,  ..., -0.0362, -0.0610,  0.0317],\n",
      "        ...,\n",
      "        [ 0.0494,  0.0356,  0.0022,  ..., -0.0696, -0.0158, -0.0023],\n",
      "        [ 0.0020,  0.0830,  0.0183,  ..., -0.0271,  0.0520,  0.0037],\n",
      "        [ 0.0491, -0.0265,  0.0750,  ..., -0.0914, -0.0370,  0.0438]])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.self.key.bias', Parameter containing:\n",
      "tensor([-0.0001, -0.0002,  0.0003,  ..., -0.0002, -0.0003,  0.0002])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0400, -0.0014,  0.0109,  ...,  0.0370, -0.0582,  0.0564],\n",
      "        [ 0.0258,  0.0591, -0.0330,  ...,  0.0011,  0.0239,  0.0850],\n",
      "        [ 0.0102,  0.0055, -0.0079,  ..., -0.0437, -0.0563,  0.0283],\n",
      "        ...,\n",
      "        [-0.0574, -0.1272,  0.0226,  ..., -0.0890, -0.0105,  0.0545],\n",
      "        [-0.0023, -0.0322, -0.0334,  ..., -0.0229,  0.0053,  0.1261],\n",
      "        [ 0.0407,  0.1235, -0.0006,  ..., -0.0213,  0.0101, -0.0260]])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0092,  0.0055,  0.0028,  ..., -0.0103, -0.0154, -0.0096])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0401, -0.0427, -0.0117,  ..., -0.0007, -0.0209,  0.0090],\n",
      "        [ 0.0033,  0.0200,  0.0106,  ...,  0.0346, -0.0651, -0.0219],\n",
      "        [ 0.0540, -0.0717,  0.0195,  ..., -0.0156, -0.0266,  0.0376],\n",
      "        ...,\n",
      "        [ 0.0193,  0.0085, -0.0580,  ...,  0.0844,  0.0392,  0.0032],\n",
      "        [-0.0482, -0.0361, -0.0245,  ...,  0.0190, -0.0724,  0.0356],\n",
      "        [ 0.0028, -0.0149, -0.0032,  ..., -0.0692, -0.0658, -0.0381]])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0023, -0.0222,  0.0812,  ..., -0.0592,  0.0309, -0.0677])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9976, 0.9941, 0.9985,  ..., 0.9927, 0.9966, 0.9985])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0786, -0.0412, -0.2092,  ..., -0.0619, -0.0682, -0.0662])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0744, -0.0222,  0.0453,  ...,  0.0469,  0.0105, -0.0307],\n",
      "        [ 0.0130,  0.0436,  0.0237,  ...,  0.0630, -0.0745, -0.0010],\n",
      "        [ 0.0125,  0.0171, -0.0135,  ...,  0.1301, -0.0050, -0.0166],\n",
      "        ...,\n",
      "        [-0.0781, -0.0634,  0.0136,  ...,  0.0249,  0.0261, -0.0214],\n",
      "        [-0.0370, -0.0229,  0.0406,  ...,  0.0074, -0.0491, -0.0480],\n",
      "        [-0.0144, -0.0860,  0.0385,  ...,  0.0244, -0.0488,  0.0309]])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0782, -0.0590, -0.0987,  ..., -0.0640, -0.0779, -0.0865])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0126,  0.0353,  0.0685,  ..., -0.0136, -0.0020,  0.0062],\n",
      "        [-0.0156, -0.0026,  0.0127,  ..., -0.0336, -0.0683, -0.0239],\n",
      "        [ 0.0065, -0.0025, -0.0164,  ...,  0.0017,  0.0001,  0.0072],\n",
      "        ...,\n",
      "        [-0.0159,  0.0420,  0.0054,  ...,  0.0057, -0.0330, -0.0575],\n",
      "        [-0.0547, -0.0407,  0.0453,  ..., -0.0065, -0.0313,  0.0169],\n",
      "        [-0.0048,  0.0191, -0.0108,  ..., -0.0396,  0.0099,  0.0361]])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0817,  0.0127, -0.2003,  ..., -0.0393, -0.0637, -0.1033])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9800, 0.9961, 0.9976,  ..., 0.9751, 0.9912, 0.9878])) weights are unchanged.\n",
      "Layer ('encoder.layer.16.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0197, -0.0468,  0.1810,  ..., -0.0244, -0.0306, -0.0292])) weights are unchanged.\n",
      "Newly Added: encoder.layer.16.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0110,  0.0337, -0.0532,  ..., -0.0251,  0.0432,  0.0238],\n",
      "        [-0.0514, -0.0338, -0.0170,  ...,  0.0108, -0.0415, -0.0126],\n",
      "        [-0.0329, -0.0431,  0.0380,  ...,  0.0165,  0.0193, -0.0360],\n",
      "        ...,\n",
      "        [ 0.0185, -0.0016,  0.0223,  ...,  0.0317, -0.0407,  0.0221],\n",
      "        [-0.0116,  0.0081, -0.0035,  ...,  0.0518,  0.0220,  0.0200],\n",
      "        [ 0.0362, -0.0077,  0.0346,  ...,  0.0245, -0.0281,  0.0440]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.16.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.16.up_layer.weight Parameter containing:\n",
      "tensor([[ 5.1932e-02,  1.5394e-02,  5.9921e-02,  ...,  5.3876e-02,\n",
      "          1.1934e-02, -5.7239e-02],\n",
      "        [ 1.2417e-02, -4.2417e-02,  2.7973e-02,  ..., -1.0035e-02,\n",
      "         -9.0687e-03,  5.2055e-02],\n",
      "        [ 1.5701e-02,  2.5195e-02, -5.4464e-02,  ..., -5.2451e-02,\n",
      "         -1.0750e-02,  1.6755e-02],\n",
      "        ...,\n",
      "        [ 7.0244e-05,  4.4671e-02, -3.8275e-02,  ...,  3.3348e-02,\n",
      "         -2.2487e-02,  1.2764e-02],\n",
      "        [ 2.6352e-02,  4.9984e-02,  1.8502e-02,  ...,  2.2313e-02,\n",
      "         -5.6527e-02, -3.3444e-02],\n",
      "        [ 4.9468e-02,  4.6218e-02,  6.0957e-02,  ..., -3.0292e-02,\n",
      "         -1.5191e-02,  2.4433e-02]], requires_grad=True)\n",
      "Newly Added: encoder.layer.16.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.16.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.16.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.17.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0059, -0.0176,  0.0623,  ...,  0.0439, -0.0280,  0.0618],\n",
      "        [ 0.0307,  0.0175, -0.0220,  ...,  0.0893, -0.0311,  0.0485],\n",
      "        [ 0.1371,  0.0717,  0.0295,  ..., -0.0817, -0.0425, -0.0058],\n",
      "        ...,\n",
      "        [-0.0277, -0.0424,  0.0736,  ..., -0.1266,  0.0551, -0.0769],\n",
      "        [ 0.0473, -0.0100,  0.0445,  ...,  0.0012, -0.0466,  0.0326],\n",
      "        [ 0.0128,  0.0892, -0.0296,  ...,  0.0012,  0.0184, -0.0199]])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.1750,  0.0581,  0.0232,  ..., -0.0486,  0.0414,  0.0123])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0275, -0.0096,  0.0547,  ..., -0.0641,  0.0376,  0.0312],\n",
      "        [ 0.0471,  0.0113,  0.0325,  ...,  0.0895, -0.0342,  0.0204],\n",
      "        [ 0.0122,  0.0310, -0.0353,  ..., -0.0669,  0.0823,  0.0039],\n",
      "        ...,\n",
      "        [ 0.0195,  0.0567,  0.0245,  ...,  0.0180,  0.0390, -0.0757],\n",
      "        [ 0.0323,  0.0488,  0.0486,  ...,  0.0550,  0.0232,  0.0183],\n",
      "        [-0.0881, -0.0012, -0.0291,  ...,  0.0582,  0.0088,  0.0661]])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 2.4738e-03, -2.5332e-05,  1.6463e-04,  ..., -2.6345e-05,\n",
      "         3.8385e-04,  2.0969e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0169,  0.0037, -0.0071,  ...,  0.0121,  0.0018,  0.0049],\n",
      "        [-0.0184,  0.0200, -0.0104,  ...,  0.0231,  0.0121,  0.0261],\n",
      "        [ 0.0075,  0.0108, -0.0098,  ...,  0.0164, -0.0020, -0.0743],\n",
      "        ...,\n",
      "        [-0.0255,  0.0303,  0.0053,  ..., -0.0275, -0.0812, -0.1122],\n",
      "        [ 0.0160, -0.0989, -0.0301,  ...,  0.0024, -0.0298,  0.0952],\n",
      "        [ 0.0591, -0.0671,  0.0100,  ..., -0.1134,  0.0461, -0.0467]])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0083, -0.0015, -0.0008,  ...,  0.0017,  0.0088,  0.0051])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0004, -0.0222, -0.0162,  ..., -0.0206, -0.0317,  0.0528],\n",
      "        [ 0.0261,  0.0082,  0.0129,  ...,  0.0754, -0.0500,  0.0668],\n",
      "        [ 0.0009,  0.0086, -0.0033,  ...,  0.0147, -0.0292, -0.0058],\n",
      "        ...,\n",
      "        [ 0.0317,  0.0098, -0.0392,  ...,  0.0292,  0.0295,  0.0222],\n",
      "        [-0.0284,  0.0252, -0.0098,  ..., -0.0478,  0.0639, -0.0310],\n",
      "        [ 0.0260, -0.0278, -0.0553,  ..., -0.0027, -0.0306,  0.0146]])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0237,  0.0379,  0.0300,  ..., -0.0083, -0.0495, -0.0181])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9897, 0.9980, 0.9985,  ..., 0.9849, 0.9883, 0.9868])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0952, -0.0611, -0.2151,  ..., -0.0820, -0.0676, -0.0848])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0054, -0.0426,  0.0231,  ..., -0.0525, -0.0379, -0.0280],\n",
      "        [ 0.0825, -0.0315,  0.0197,  ...,  0.0231, -0.0607, -0.0035],\n",
      "        [-0.0049,  0.0135,  0.0119,  ...,  0.0313,  0.0277,  0.0140],\n",
      "        ...,\n",
      "        [ 0.0736, -0.0579, -0.0371,  ..., -0.0064,  0.0345,  0.0078],\n",
      "        [-0.0064,  0.0938,  0.0126,  ..., -0.0512, -0.0674,  0.0136],\n",
      "        [-0.0544, -0.0244, -0.0054,  ..., -0.0251, -0.0028,  0.0256]])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0443, -0.0899, -0.0250,  ...,  0.0191, -0.1120, -0.0447])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0276,  0.0674, -0.0157,  ...,  0.0050, -0.0232, -0.0563],\n",
      "        [ 0.0066, -0.0062,  0.0209,  ...,  0.0031,  0.0029, -0.0068],\n",
      "        [-0.0171, -0.0252, -0.0003,  ...,  0.0138,  0.0316, -0.0162],\n",
      "        ...,\n",
      "        [ 0.0085,  0.0983, -0.0163,  ...,  0.0205, -0.0336,  0.0013],\n",
      "        [ 0.0115,  0.0116, -0.0235,  ...,  0.0177,  0.0370, -0.0013],\n",
      "        [-0.0273, -0.0365, -0.0159,  ...,  0.0101, -0.0193,  0.0145]])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0697,  0.0155, -0.1522,  ..., -0.0155, -0.0874, -0.0512])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9878, 0.9917, 0.9971,  ..., 0.9844, 0.9878, 0.9863])) weights are unchanged.\n",
      "Layer ('encoder.layer.17.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0041, -0.0266,  0.1265,  ..., -0.0130, -0.0370, -0.0070])) weights are unchanged.\n",
      "Newly Added: encoder.layer.17.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0340, -0.0331,  0.0173,  ...,  0.0464,  0.0617,  0.0307],\n",
      "        [-0.0039,  0.0481, -0.0115,  ..., -0.0547,  0.0521,  0.0032],\n",
      "        [ 0.0002, -0.0165, -0.0608,  ...,  0.0388, -0.0209,  0.0548],\n",
      "        ...,\n",
      "        [-0.0254, -0.0478, -0.0514,  ...,  0.0616, -0.0606, -0.0137],\n",
      "        [-0.0363, -0.0322, -0.0211,  ..., -0.0223, -0.0443,  0.0241],\n",
      "        [-0.0353, -0.0445,  0.0046,  ...,  0.0038, -0.0243, -0.0521]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.17.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.17.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0181,  0.0127,  0.0221,  ..., -0.0214,  0.0271,  0.0025],\n",
      "        [ 0.0244, -0.0507,  0.0221,  ...,  0.0477, -0.0158,  0.0355],\n",
      "        [-0.0371,  0.0376, -0.0138,  ..., -0.0086,  0.0135,  0.0023],\n",
      "        ...,\n",
      "        [ 0.0403, -0.0138, -0.0197,  ...,  0.0598,  0.0500,  0.0575],\n",
      "        [ 0.0548,  0.0227, -0.0189,  ...,  0.0106,  0.0221, -0.0457],\n",
      "        [-0.0520,  0.0055, -0.0239,  ..., -0.0615,  0.0090, -0.0089]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.17.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.17.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.17.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.18.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0368, -0.0007,  0.1381,  ...,  0.0251,  0.0652, -0.0248],\n",
      "        [ 0.0329,  0.0192,  0.0127,  ..., -0.1418, -0.0477, -0.0392],\n",
      "        [-0.0506,  0.0556,  0.1886,  ...,  0.0438, -0.0245, -0.0269],\n",
      "        ...,\n",
      "        [-0.0515, -0.0023, -0.0282,  ...,  0.0196, -0.0147, -0.0334],\n",
      "        [-0.0572,  0.0430, -0.0116,  ...,  0.0117,  0.0519,  0.1106],\n",
      "        [-0.0173,  0.0410,  0.1431,  ..., -0.0297,  0.0125, -0.0542]])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.2751, -0.0114, -0.1425,  ...,  0.0091, -0.0073,  0.0214])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0604, -0.0178,  0.0529,  ...,  0.0112, -0.0085,  0.0468],\n",
      "        [-0.0249,  0.0828,  0.1170,  ..., -0.0187, -0.0068, -0.0566],\n",
      "        [ 0.0669,  0.0488,  0.2588,  ..., -0.0248,  0.0008,  0.0093],\n",
      "        ...,\n",
      "        [-0.0557, -0.0357, -0.0140,  ..., -0.0325,  0.0621,  0.0245],\n",
      "        [ 0.0638, -0.0269, -0.0222,  ..., -0.0102, -0.0420, -0.0049],\n",
      "        [-0.0158, -0.0173,  0.1429,  ...,  0.0384, -0.0080,  0.0757]])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.self.key.bias', Parameter containing:\n",
      "tensor([-7.9012e-04,  9.0182e-05, -1.8477e-04,  ..., -8.7857e-05,\n",
      "         0.0000e+00,  1.6308e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0271,  0.0195, -0.0014,  ...,  0.0169,  0.0020,  0.0065],\n",
      "        [-0.0376,  0.0597,  0.0155,  ...,  0.0079,  0.0043,  0.0265],\n",
      "        [ 0.0106, -0.0389, -0.0328,  ...,  0.1053, -0.0260,  0.0039],\n",
      "        ...,\n",
      "        [ 0.0079, -0.0007,  0.0202,  ...,  0.0250, -0.0219, -0.0367],\n",
      "        [ 0.0381, -0.0021, -0.0423,  ...,  0.0071,  0.0331,  0.0050],\n",
      "        [ 0.0256,  0.0677, -0.0174,  ..., -0.0467,  0.0165,  0.0042]])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0077,  0.0054, -0.0174,  ..., -0.0076,  0.0026, -0.0114])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0349, -0.0210, -0.0621,  ..., -0.0274, -0.0069, -0.0414],\n",
      "        [-0.0216,  0.0699, -0.0530,  ...,  0.0045, -0.0254,  0.0208],\n",
      "        [-0.0423,  0.0154, -0.0098,  ...,  0.0121, -0.0246, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0319,  0.0095,  0.0671,  ..., -0.0081,  0.0234,  0.0187],\n",
      "        [-0.0066,  0.0196,  0.0186,  ..., -0.0183, -0.0006,  0.0153],\n",
      "        [-0.0059,  0.0059, -0.0079,  ..., -0.0037,  0.0286,  0.0220]])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0150,  0.0797,  0.0284,  ...,  0.0723,  0.0370,  0.0009])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9883, 0.9854, 0.9980,  ..., 0.9902, 0.9805, 0.9897])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0949, -0.0216, -0.2500,  ..., -0.0714, -0.0623, -0.0677])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0499, -0.0257,  0.0314,  ...,  0.0212,  0.0045, -0.0253],\n",
      "        [ 0.0610, -0.0431,  0.0097,  ..., -0.0140, -0.0008, -0.0472],\n",
      "        [ 0.0261, -0.0305, -0.0304,  ..., -0.0362,  0.0082, -0.0314],\n",
      "        ...,\n",
      "        [ 0.0177,  0.0676,  0.0403,  ...,  0.0306,  0.0318,  0.0077],\n",
      "        [-0.0311, -0.0509,  0.0051,  ...,  0.0421, -0.0243,  0.0077],\n",
      "        [ 0.0405,  0.0607, -0.0642,  ...,  0.0039,  0.0115, -0.0089]])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0718, -0.1019, -0.0046,  ..., -0.0475, -0.0967, -0.0808])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.output.dense.weight', Parameter containing:\n",
      "tensor([[-3.9368e-02,  9.1851e-05,  1.7990e-02,  ...,  2.1439e-02,\n",
      "         -2.9694e-02, -3.2532e-02],\n",
      "        [-3.6743e-02, -3.8269e-02, -1.8555e-02,  ...,  4.3457e-02,\n",
      "         -2.9785e-02, -5.8594e-03],\n",
      "        [ 1.6403e-02, -3.4580e-03, -5.3711e-03,  ..., -1.1696e-02,\n",
      "         -5.9967e-03, -1.6052e-02],\n",
      "        ...,\n",
      "        [ 1.9684e-02,  1.7548e-02,  4.0802e-02,  ..., -3.3569e-03,\n",
      "         -1.5240e-03, -8.4610e-03],\n",
      "        [-9.3079e-03, -1.3947e-02,  4.0078e-04,  ..., -4.6730e-03,\n",
      "          5.0690e-02,  1.2688e-02],\n",
      "        [-2.8137e-02, -1.4061e-02,  4.3915e-02,  ...,  2.9964e-03,\n",
      "         -5.7068e-02, -2.3087e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0768,  0.0013, -0.1594,  ...,  0.0428, -0.0415, -0.0421])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9839, 0.9976, 0.9985,  ..., 0.9805, 0.9888, 0.9824])) weights are unchanged.\n",
      "Layer ('encoder.layer.18.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0069, -0.0485,  0.1255,  ..., -0.0180, -0.0273, -0.0288])) weights are unchanged.\n",
      "Newly Added: encoder.layer.18.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0213,  0.0507, -0.0458,  ...,  0.0351,  0.0445,  0.0478],\n",
      "        [ 0.0557, -0.0332,  0.0478,  ..., -0.0410,  0.0146, -0.0281],\n",
      "        [-0.0114,  0.0138,  0.0589,  ...,  0.0267,  0.0110, -0.0053],\n",
      "        ...,\n",
      "        [-0.0589, -0.0202,  0.0474,  ..., -0.0588, -0.0170, -0.0322],\n",
      "        [ 0.0612,  0.0292,  0.0352,  ...,  0.0274, -0.0532, -0.0532],\n",
      "        [ 0.0115, -0.0602,  0.0306,  ..., -0.0414,  0.0093, -0.0189]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.18.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.18.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0496, -0.0417,  0.0569,  ...,  0.0161, -0.0017, -0.0294],\n",
      "        [ 0.0047,  0.0147, -0.0385,  ...,  0.0166,  0.0420,  0.0435],\n",
      "        [ 0.0157, -0.0202,  0.0157,  ..., -0.0617,  0.0197, -0.0253],\n",
      "        ...,\n",
      "        [ 0.0129, -0.0008,  0.0383,  ...,  0.0623, -0.0609, -0.0299],\n",
      "        [ 0.0123,  0.0137,  0.0004,  ..., -0.0005, -0.0446, -0.0233],\n",
      "        [-0.0053,  0.0125, -0.0016,  ...,  0.0115,  0.0580,  0.0192]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.18.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.18.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.18.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.19.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0488, -0.0313, -0.0164,  ..., -0.0706,  0.0616, -0.1434],\n",
      "        [ 0.0289,  0.0146, -0.1318,  ...,  0.0346,  0.0417,  0.0795],\n",
      "        [-0.0310, -0.0196,  0.0462,  ..., -0.0595, -0.0814,  0.0178],\n",
      "        ...,\n",
      "        [-0.0051, -0.0108, -0.1978,  ...,  0.0873, -0.0068, -0.0345],\n",
      "        [-0.1268,  0.0600,  0.0548,  ..., -0.0126,  0.0567, -0.0856],\n",
      "        [-0.0277,  0.0590,  0.0606,  ...,  0.0974,  0.0692, -0.0207]])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0142, -0.0614, -0.0122,  ..., -0.0220,  0.0374, -0.0224])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-0.0192,  0.0070, -0.0004,  ...,  0.0454, -0.0920, -0.0132],\n",
      "        [ 0.0641,  0.0660, -0.0974,  ...,  0.0575, -0.0417,  0.0629],\n",
      "        [-0.0591, -0.0168,  0.0367,  ...,  0.0198,  0.1102, -0.0428],\n",
      "        ...,\n",
      "        [-0.0803, -0.0241, -0.2639,  ...,  0.0140,  0.0654,  0.0288],\n",
      "        [-0.0142,  0.0080,  0.1636,  ...,  0.0250,  0.0858,  0.0178],\n",
      "        [ 0.0183, -0.0760,  0.1229,  ..., -0.0473, -0.0181,  0.0722]])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.self.key.bias', Parameter containing:\n",
      "tensor([-4.2260e-05, -1.2481e-04,  7.0095e-05,  ..., -7.7069e-05,\n",
      "         1.4648e-03, -8.9943e-05])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0267,  0.0117,  0.0284,  ...,  0.0279,  0.0203,  0.0256],\n",
      "        [-0.0065,  0.0102, -0.0029,  ...,  0.0341,  0.0251, -0.0186],\n",
      "        [-0.0350, -0.0134,  0.0146,  ...,  0.0354, -0.0216, -0.0325],\n",
      "        ...,\n",
      "        [ 0.0185,  0.0167,  0.0131,  ..., -0.0200, -0.0156, -0.0177],\n",
      "        [ 0.0228,  0.0262, -0.0367,  ...,  0.0289, -0.0192, -0.0463],\n",
      "        [-0.0175,  0.0600, -0.0252,  ...,  0.0376,  0.0251, -0.0035]])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 3.5515e-03,  7.2754e-02, -1.4174e-04,  ..., -4.5662e-03,\n",
      "        -1.8060e-05, -1.7288e-02])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0012,  0.0278, -0.0179,  ...,  0.0275,  0.0128, -0.0047],\n",
      "        [ 0.0061, -0.0302, -0.0474,  ..., -0.0034,  0.0368,  0.0051],\n",
      "        [ 0.0009, -0.0488, -0.0237,  ...,  0.0411, -0.0239,  0.0152],\n",
      "        ...,\n",
      "        [-0.0630, -0.0610, -0.0301,  ...,  0.0022, -0.0009,  0.0135],\n",
      "        [-0.0162,  0.0465,  0.0087,  ...,  0.0109,  0.0180, -0.0092],\n",
      "        [-0.0191, -0.0374,  0.0032,  ..., -0.0118, -0.0235, -0.0476]])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0040,  0.0612,  0.0789,  ..., -0.0015,  0.0257,  0.0067])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9800, 0.9893, 0.9985,  ..., 0.9722, 0.9883, 0.9946])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0800, -0.0363, -0.2133,  ..., -0.0625, -0.0837, -0.0623])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0426, -0.0422,  0.0147,  ...,  0.0865,  0.0757,  0.1552],\n",
      "        [ 0.0506, -0.0107,  0.0535,  ...,  0.0199,  0.0282,  0.0302],\n",
      "        [ 0.0195,  0.0072,  0.0384,  ..., -0.0201, -0.0061,  0.0091],\n",
      "        ...,\n",
      "        [ 0.0756, -0.0252,  0.0217,  ..., -0.0023, -0.0518,  0.0385],\n",
      "        [ 0.0292, -0.0079, -0.0383,  ..., -0.0158,  0.0333, -0.0844],\n",
      "        [ 0.0145,  0.0447,  0.0357,  ..., -0.0366, -0.0105,  0.0344]])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.1019,  0.0129, -0.0420,  ..., -0.1168,  0.0033, -0.0179])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.output.dense.weight', Parameter containing:\n",
      "tensor([[ 1.2154e-02, -1.5854e-02, -1.9333e-02,  ..., -4.1748e-02,\n",
      "         -9.3384e-03,  7.3128e-03],\n",
      "        [-5.8289e-02, -3.8849e-02,  7.7477e-03,  ...,  5.2612e-02,\n",
      "         -1.7654e-02,  2.0233e-02],\n",
      "        [ 1.3649e-02, -1.4389e-02, -4.1246e-05,  ..., -1.5732e-02,\n",
      "          8.7967e-03, -1.1463e-03],\n",
      "        ...,\n",
      "        [ 1.0193e-02, -8.7280e-03,  2.9602e-02,  ..., -4.6082e-02,\n",
      "          1.2802e-02,  2.3224e-02],\n",
      "        [ 2.8122e-02, -9.4452e-03, -3.1708e-02,  ..., -1.7868e-02,\n",
      "         -7.7362e-03, -9.5901e-03],\n",
      "        [ 8.0795e-03,  1.3931e-02, -2.8572e-03,  ...,  4.1687e-02,\n",
      "          4.0802e-02,  3.1982e-02]])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0392,  0.0275, -0.0501,  ...,  0.1114, -0.0571,  0.0029])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9639, 0.9946, 0.9966,  ..., 0.9761, 0.9912, 0.9854])) weights are unchanged.\n",
      "Layer ('encoder.layer.19.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0110, -0.0326,  0.0787,  ..., -0.0202, -0.0165, -0.0066])) weights are unchanged.\n",
      "Newly Added: encoder.layer.19.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0465, -0.0078,  0.0376,  ...,  0.0331, -0.0314, -0.0567],\n",
      "        [ 0.0364,  0.0163,  0.0165,  ...,  0.0139,  0.0539,  0.0176],\n",
      "        [ 0.0119, -0.0022, -0.0446,  ...,  0.0068,  0.0281, -0.0471],\n",
      "        ...,\n",
      "        [-0.0395,  0.0340, -0.0073,  ..., -0.0578,  0.0370, -0.0233],\n",
      "        [ 0.0120,  0.0247, -0.0104,  ..., -0.0258, -0.0027, -0.0155],\n",
      "        [ 0.0096,  0.0214, -0.0580,  ..., -0.0615, -0.0426, -0.0063]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.19.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.19.up_layer.weight Parameter containing:\n",
      "tensor([[-1.7648e-02,  2.3739e-02, -3.1724e-02,  ..., -4.8620e-02,\n",
      "          4.7356e-02, -2.0470e-02],\n",
      "        [ 4.2548e-02, -5.0382e-02,  4.8340e-02,  ...,  2.3812e-03,\n",
      "          3.3991e-02, -4.1895e-02],\n",
      "        [ 5.1860e-02,  4.9426e-02,  1.1215e-02,  ..., -8.3398e-03,\n",
      "          5.4802e-02, -5.4088e-02],\n",
      "        ...,\n",
      "        [-5.3564e-02, -1.2563e-02, -1.7323e-02,  ...,  1.7515e-02,\n",
      "         -3.7673e-02, -5.9922e-02],\n",
      "        [-5.0420e-02,  4.4045e-02,  4.7868e-03,  ..., -5.0177e-02,\n",
      "         -3.4727e-02, -1.4512e-02],\n",
      "        [ 2.5190e-05,  3.9384e-02,  3.3177e-02,  ...,  3.8283e-02,\n",
      "          1.1743e-02, -3.3527e-02]], requires_grad=True)\n",
      "Newly Added: encoder.layer.19.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.19.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.19.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.20.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0148,  0.0176, -0.0108,  ...,  0.0008, -0.0575,  0.0188],\n",
      "        [-0.0726, -0.0721,  0.0294,  ..., -0.0308, -0.0392, -0.0035],\n",
      "        [-0.0194, -0.0295, -0.0005,  ...,  0.0302,  0.0458, -0.0049],\n",
      "        ...,\n",
      "        [ 0.0953, -0.0103, -0.0276,  ...,  0.0711, -0.0227,  0.0172],\n",
      "        [-0.0012, -0.0018,  0.1982,  ..., -0.0235,  0.0233,  0.0022],\n",
      "        [-0.0254,  0.0136, -0.1221,  ..., -0.0454, -0.0356, -0.0508]])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0273,  0.0250,  0.0129,  ...,  0.0903, -0.0326,  0.0515])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0149, -0.0674, -0.0256,  ..., -0.1068, -0.0145, -0.0861],\n",
      "        [-0.0142, -0.0049,  0.0352,  ..., -0.0163, -0.0011, -0.0038],\n",
      "        [ 0.0268,  0.0476, -0.0873,  ...,  0.1043,  0.0214,  0.0358],\n",
      "        ...,\n",
      "        [-0.0373, -0.0528, -0.1083,  ..., -0.0246, -0.0157,  0.0292],\n",
      "        [-0.0326,  0.0550,  0.2246,  ...,  0.0258, -0.0636,  0.0289],\n",
      "        [-0.0196,  0.0241, -0.0450,  ..., -0.0934, -0.0156,  0.0167]])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 3.0875e-04, -2.9087e-05,  2.7800e-04,  ...,  1.7178e-04,\n",
      "        -3.9935e-06,  5.6088e-05])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0490, -0.0088,  0.0264,  ..., -0.0246, -0.0007,  0.0609],\n",
      "        [-0.0182,  0.0012, -0.0163,  ..., -0.0242,  0.1051,  0.0207],\n",
      "        [ 0.0055,  0.0318, -0.0122,  ..., -0.0679, -0.0781,  0.0168],\n",
      "        ...,\n",
      "        [-0.0656,  0.0290,  0.0199,  ...,  0.0664,  0.0086,  0.0179],\n",
      "        [ 0.0916,  0.0102,  0.0631,  ..., -0.0051,  0.0165, -0.0505],\n",
      "        [ 0.0213,  0.0426, -0.0117,  ..., -0.0252, -0.0093, -0.0742]])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0005,  0.0055,  0.0193,  ...,  0.0028,  0.0027, -0.0060])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0094, -0.0656, -0.0506,  ...,  0.0568, -0.0546, -0.0164],\n",
      "        [ 0.0133,  0.0436,  0.0168,  ..., -0.0145, -0.0156, -0.0552],\n",
      "        [ 0.0259,  0.0094,  0.0100,  ..., -0.0173, -0.0530, -0.0046],\n",
      "        ...,\n",
      "        [ 0.0438, -0.0265, -0.0441,  ..., -0.0726, -0.0226,  0.0400],\n",
      "        [ 0.0204,  0.0004,  0.0082,  ..., -0.0203, -0.0125,  0.0304],\n",
      "        [ 0.0090,  0.0065, -0.0165,  ...,  0.0239,  0.0129,  0.0367]])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0104,  0.0782,  0.0603,  ...,  0.0721,  0.0870,  0.0217])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9722, 0.9951, 0.9990,  ..., 0.9795, 0.9858, 0.9951])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0297, -0.0329, -0.1558,  ..., -0.0447, -0.0893, -0.0462])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0064, -0.0098,  0.0589,  ...,  0.0059,  0.0103,  0.0133],\n",
      "        [ 0.0205, -0.0090,  0.0042,  ...,  0.0076, -0.0656,  0.0705],\n",
      "        [-0.0152, -0.0145, -0.0554,  ..., -0.0222, -0.0313, -0.0316],\n",
      "        ...,\n",
      "        [ 0.0010, -0.0305,  0.0032,  ...,  0.0403,  0.0903,  0.0202],\n",
      "        [ 0.0587,  0.0489,  0.0597,  ..., -0.0512, -0.0094, -0.0286],\n",
      "        [-0.0177, -0.0424,  0.0497,  ..., -0.0111,  0.0010, -0.0072]])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0226, -0.0380, -0.0477,  ..., -0.0258, -0.0178, -0.0257])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0168,  0.0674,  0.0128,  ...,  0.0583, -0.0087, -0.0173],\n",
      "        [ 0.0103,  0.0308,  0.0497,  ...,  0.0043,  0.0151, -0.0399],\n",
      "        [-0.0119, -0.0019, -0.0032,  ..., -0.0084, -0.0094, -0.0122],\n",
      "        ...,\n",
      "        [ 0.0138,  0.0071,  0.0452,  ..., -0.0164,  0.0146, -0.0032],\n",
      "        [-0.0198, -0.0458,  0.0211,  ...,  0.0313,  0.0596,  0.0218],\n",
      "        [-0.0338,  0.0025,  0.0388,  ...,  0.0212,  0.0382, -0.0060]])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0565,  0.0040, -0.0087,  ...,  0.0621, -0.0942,  0.0491])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9663, 0.9922, 0.9976,  ..., 0.9805, 0.9839, 0.9800])) weights are unchanged.\n",
      "Layer ('encoder.layer.20.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0311, -0.0306,  0.0811,  ..., -0.0302, -0.0234, -0.0136])) weights are unchanged.\n",
      "Newly Added: encoder.layer.20.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0516,  0.0229, -0.0054,  ...,  0.0280,  0.0356,  0.0013],\n",
      "        [ 0.0335, -0.0405, -0.0245,  ..., -0.0410, -0.0002,  0.0133],\n",
      "        [ 0.0484, -0.0143, -0.0045,  ...,  0.0349,  0.0432,  0.0170],\n",
      "        ...,\n",
      "        [-0.0269, -0.0521, -0.0353,  ..., -0.0223,  0.0099,  0.0237],\n",
      "        [-0.0486, -0.0061, -0.0214,  ..., -0.0351, -0.0450, -0.0489],\n",
      "        [-0.0344,  0.0426,  0.0082,  ..., -0.0110, -0.0263, -0.0270]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.20.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.20.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0456, -0.0013, -0.0190,  ..., -0.0436,  0.0332,  0.0440],\n",
      "        [ 0.0026,  0.0228,  0.0144,  ...,  0.0361, -0.0411,  0.0243],\n",
      "        [ 0.0181,  0.0564, -0.0067,  ..., -0.0368, -0.0124,  0.0005],\n",
      "        ...,\n",
      "        [-0.0107, -0.0443,  0.0590,  ...,  0.0511,  0.0012, -0.0618],\n",
      "        [-0.0140,  0.0434, -0.0539,  ...,  0.0451, -0.0043, -0.0098],\n",
      "        [-0.0383, -0.0276,  0.0088,  ..., -0.0023,  0.0617,  0.0306]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.20.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.20.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.20.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.21.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0295, -0.0806, -0.2133,  ..., -0.0184, -0.0101, -0.0441],\n",
      "        [-0.0239, -0.0811,  0.0543,  ..., -0.0247,  0.0210,  0.0031],\n",
      "        [ 0.0551,  0.0401,  0.0606,  ...,  0.0321, -0.0398, -0.0782],\n",
      "        ...,\n",
      "        [ 0.0017,  0.0142, -0.1501,  ...,  0.0743,  0.0646,  0.0557],\n",
      "        [-0.0579,  0.0107,  0.0358,  ...,  0.0062,  0.0063,  0.0054],\n",
      "        [-0.0486, -0.0428,  0.0473,  ...,  0.0160, -0.0199,  0.0676]])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.1658, -0.0312, -0.0176,  ..., -0.0687,  0.0409, -0.0462])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0104,  0.0486, -0.1588,  ...,  0.0699,  0.0302,  0.0121],\n",
      "        [ 0.0435,  0.0441,  0.0706,  ...,  0.0067,  0.1317, -0.0435],\n",
      "        [-0.0004,  0.0016, -0.0025,  ..., -0.0806,  0.0142, -0.0119],\n",
      "        ...,\n",
      "        [-0.0566, -0.0455, -0.1238,  ...,  0.0283, -0.0529,  0.0316],\n",
      "        [ 0.0887, -0.0140,  0.0696,  ..., -0.0936,  0.0291, -0.0107],\n",
      "        [-0.0312,  0.0131,  0.0789,  ..., -0.0394,  0.0149, -0.0058]])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.self.key.bias', Parameter containing:\n",
      "tensor([-5.0629e-02,  3.5882e-05,  5.8603e-04,  ...,  3.9062e-03,\n",
      "        -3.6192e-04,  1.0155e-02])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0294, -0.0317, -0.0536,  ..., -0.0359,  0.0164, -0.0273],\n",
      "        [ 0.0420, -0.0497, -0.0003,  ...,  0.0234, -0.0509,  0.0677],\n",
      "        [ 0.0168, -0.0406, -0.0113,  ...,  0.0034, -0.0381,  0.0265],\n",
      "        ...,\n",
      "        [ 0.0199, -0.0723, -0.0101,  ..., -0.0240, -0.0741, -0.0296],\n",
      "        [ 0.0283, -0.0295, -0.0438,  ...,  0.0670, -0.0156,  0.0249],\n",
      "        [-0.0199, -0.0035,  0.0948,  ..., -0.0341, -0.0218,  0.0224]])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0073, -0.0018, -0.0013,  ..., -0.0114, -0.0259, -0.0122])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0022,  0.0444, -0.0341,  ..., -0.0173,  0.0066,  0.0372],\n",
      "        [ 0.0344,  0.0151, -0.0345,  ...,  0.0046,  0.0278, -0.0186],\n",
      "        [ 0.0102, -0.0402, -0.0073,  ..., -0.0136,  0.0143, -0.0398],\n",
      "        ...,\n",
      "        [ 0.0198,  0.0447, -0.0041,  ...,  0.0199, -0.0222,  0.0489],\n",
      "        [-0.0536,  0.0704, -0.0666,  ...,  0.0017, -0.0414, -0.0165],\n",
      "        [ 0.0158,  0.0129, -0.0546,  ...,  0.0504, -0.0234, -0.0265]])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0267,  0.0898,  0.0438,  ...,  0.0352,  0.1780,  0.0308])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9712, 0.9922, 1.0000,  ..., 0.9795, 0.9912, 0.9922])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0457, -0.0585, -0.1334,  ..., -0.0504, -0.1016, -0.0348])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0166, -0.0317,  0.0371,  ...,  0.0542,  0.0238, -0.0266],\n",
      "        [ 0.0017,  0.0008, -0.0734,  ...,  0.0260,  0.0014,  0.0590],\n",
      "        [ 0.0018,  0.0369, -0.0443,  ..., -0.0052,  0.0096,  0.0010],\n",
      "        ...,\n",
      "        [-0.0090, -0.0185,  0.0259,  ..., -0.0240, -0.0734, -0.0229],\n",
      "        [ 0.0128, -0.0698,  0.0179,  ..., -0.0241,  0.0056, -0.0253],\n",
      "        [ 0.0037,  0.0035,  0.0192,  ..., -0.0109,  0.0282,  0.0379]])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.intermediate.dense.bias', Parameter containing:\n",
      "tensor([-0.0258, -0.0454,  0.0190,  ..., -0.0300, -0.0034, -0.0152])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0118,  0.0010, -0.0039,  ...,  0.0360, -0.0206, -0.0352],\n",
      "        [ 0.0027,  0.0124, -0.0349,  ...,  0.0152, -0.0354,  0.0514],\n",
      "        [-0.0181,  0.0005,  0.0018,  ...,  0.0119,  0.0048,  0.0123],\n",
      "        ...,\n",
      "        [-0.0311,  0.0122,  0.0245,  ..., -0.0188, -0.0135, -0.0044],\n",
      "        [-0.0340,  0.0021, -0.0369,  ..., -0.0358, -0.1037,  0.0084],\n",
      "        [-0.0079, -0.0052,  0.0023,  ..., -0.0029, -0.0313,  0.0016]])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0101,  0.0066, -0.0726,  ...,  0.0920, -0.1208,  0.0153])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9697, 0.9912, 0.9961,  ..., 0.9834, 0.9863, 0.9961])) weights are unchanged.\n",
      "Layer ('encoder.layer.21.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0435, -0.0139,  0.0396,  ..., -0.0394, -0.0566, -0.0170])) weights are unchanged.\n",
      "Newly Added: encoder.layer.21.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0040,  0.0369, -0.0185,  ...,  0.0519, -0.0471, -0.0192],\n",
      "        [ 0.0066,  0.0408, -0.0585,  ..., -0.0122,  0.0334, -0.0433],\n",
      "        [ 0.0219, -0.0414, -0.0549,  ..., -0.0432,  0.0008,  0.0030],\n",
      "        ...,\n",
      "        [-0.0194, -0.0549, -0.0304,  ...,  0.0367,  0.0338,  0.0489],\n",
      "        [ 0.0337,  0.0156, -0.0563,  ..., -0.0546,  0.0215, -0.0331],\n",
      "        [ 0.0244, -0.0442, -0.0271,  ..., -0.0238,  0.0574,  0.0561]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.21.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.21.up_layer.weight Parameter containing:\n",
      "tensor([[ 0.0612,  0.0083,  0.0204,  ..., -0.0246,  0.0186,  0.0361],\n",
      "        [-0.0032, -0.0091,  0.0334,  ..., -0.0525, -0.0406,  0.0210],\n",
      "        [-0.0178, -0.0218,  0.0179,  ..., -0.0201,  0.0201, -0.0548],\n",
      "        ...,\n",
      "        [-0.0056, -0.0177,  0.0538,  ..., -0.0036,  0.0036, -0.0194],\n",
      "        [-0.0532, -0.0483, -0.0473,  ..., -0.0232,  0.0425, -0.0057],\n",
      "        [ 0.0325,  0.0251,  0.0454,  ...,  0.0466,  0.0548,  0.0126]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.21.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.21.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.21.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.22.attention.self.query.weight', Parameter containing:\n",
      "tensor([[ 0.0033,  0.0378,  0.0875,  ...,  0.0182,  0.0172,  0.0580],\n",
      "        [ 0.0182, -0.0406,  0.0043,  ..., -0.1017,  0.0530,  0.0099],\n",
      "        [ 0.0972, -0.0285,  0.1843,  ...,  0.0121,  0.0566, -0.0079],\n",
      "        ...,\n",
      "        [-0.0077, -0.0126, -0.0793,  ..., -0.0675, -0.0027,  0.0722],\n",
      "        [-0.0414,  0.0721, -0.0108,  ...,  0.0606, -0.0375, -0.0322],\n",
      "        [ 0.0363,  0.0595,  0.0999,  ...,  0.0265, -0.0226, -0.0805]])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.self.query.bias', Parameter containing:\n",
      "tensor([-0.0051,  0.0072,  0.1514,  ...,  0.0018,  0.0993, -0.0552])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.self.key.weight', Parameter containing:\n",
      "tensor([[-6.6284e-02, -5.3131e-02,  7.1960e-02,  ...,  4.0665e-03,\n",
      "         -5.8258e-02,  4.7577e-02],\n",
      "        [ 3.0533e-02, -2.7206e-02,  2.3895e-02,  ...,  3.1891e-02,\n",
      "         -1.0452e-02,  1.2917e-02],\n",
      "        [ 2.0981e-03,  3.3142e-02,  9.5581e-02,  ...,  1.7624e-02,\n",
      "         -1.3084e-02,  1.5991e-02],\n",
      "        ...,\n",
      "        [ 7.5378e-02,  7.6652e-05, -9.9060e-02,  ..., -6.1523e-02,\n",
      "         -5.4291e-02, -9.2346e-02],\n",
      "        [ 2.3193e-02,  6.7444e-02, -5.3619e-02,  ...,  3.6560e-02,\n",
      "          3.6591e-02, -3.8757e-02],\n",
      "        [-5.2948e-02,  1.7349e-02, -2.7420e-02,  ..., -1.0078e-02,\n",
      "          1.9791e-02, -8.2245e-03]])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 7.1526e-06, -1.4937e-04,  4.6706e-04,  ..., -2.0421e-04,\n",
      "        -5.9032e-04, -6.9571e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.self.value.weight', Parameter containing:\n",
      "tensor([[ 0.0254,  0.0334,  0.0428,  ...,  0.0432, -0.0222,  0.0514],\n",
      "        [-0.0437, -0.0137, -0.0157,  ...,  0.0526, -0.0729,  0.0563],\n",
      "        [-0.0193,  0.0346,  0.0061,  ..., -0.0042, -0.0190,  0.0230],\n",
      "        ...,\n",
      "        [-0.0412,  0.0008, -0.0150,  ..., -0.0330,  0.0291, -0.0468],\n",
      "        [ 0.0182,  0.0146,  0.0360,  ...,  0.0062,  0.0050, -0.0352],\n",
      "        [-0.0230, -0.0196, -0.0385,  ..., -0.0251,  0.0119,  0.0231]])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.self.value.bias', Parameter containing:\n",
      "tensor([ 0.0076, -0.0129,  0.0142,  ..., -0.0120, -0.0048, -0.0074])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0420,  0.0490, -0.0052,  ..., -0.0356, -0.0329, -0.0120],\n",
      "        [ 0.0088,  0.0243,  0.0286,  ..., -0.0477,  0.0071,  0.0270],\n",
      "        [ 0.0182,  0.0049, -0.0265,  ..., -0.0233,  0.0509, -0.0038],\n",
      "        ...,\n",
      "        [ 0.0328,  0.0277, -0.0227,  ...,  0.0107, -0.0222, -0.0115],\n",
      "        [ 0.0429, -0.0165,  0.0177,  ..., -0.0161, -0.0071, -0.0428],\n",
      "        [ 0.0327,  0.0400,  0.0391,  ..., -0.0348,  0.0059, -0.0033]])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0768,  0.0949,  0.0674,  ...,  0.0083,  0.1467, -0.0307])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9541, 0.9883, 0.9966,  ..., 0.9751, 0.9966, 0.9868])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0478, -0.0132, -0.1199,  ..., -0.0383, -0.1026,  0.0332])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0018, -0.0679, -0.0494,  ..., -0.0092,  0.0080,  0.0772],\n",
      "        [-0.0171, -0.0195,  0.0640,  ...,  0.0034,  0.0453, -0.0015],\n",
      "        [-0.0293, -0.0692,  0.0753,  ...,  0.0114, -0.0121, -0.0465],\n",
      "        ...,\n",
      "        [-0.0327,  0.0271,  0.0451,  ...,  0.0469,  0.0083,  0.0372],\n",
      "        [ 0.0629,  0.0210,  0.0444,  ...,  0.0233, -0.0061,  0.0048],\n",
      "        [ 0.0040, -0.0736, -0.0578,  ..., -0.0042,  0.0218, -0.0502]])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.intermediate.dense.bias', Parameter containing:\n",
      "tensor([ 0.0179, -0.0073, -0.0130,  ..., -0.0444,  0.0150, -0.0280])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0260,  0.0398, -0.0241,  ...,  0.0072, -0.0063, -0.0860],\n",
      "        [ 0.0119, -0.0277,  0.0232,  ..., -0.0092, -0.0304,  0.0136],\n",
      "        [-0.0164, -0.0119, -0.0038,  ...,  0.0016, -0.0003,  0.0111],\n",
      "        ...,\n",
      "        [-0.0134, -0.0316,  0.0100,  ..., -0.0212, -0.0068,  0.0564],\n",
      "        [ 0.0159, -0.0377,  0.0404,  ..., -0.0080, -0.0247, -0.0257],\n",
      "        [-0.0078,  0.0710,  0.0314,  ..., -0.0225,  0.0064, -0.0239]])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0236,  0.0465, -0.0757,  ...,  0.0240, -0.1323,  0.0455])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9561, 0.9937, 0.9971,  ..., 0.9824, 0.9912, 0.9829])) weights are unchanged.\n",
      "Layer ('encoder.layer.22.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0231,  0.0381,  0.0500,  ..., -0.0329, -0.0123,  0.0165])) weights are unchanged.\n",
      "Newly Added: encoder.layer.22.down_layer.weight Parameter containing:\n",
      "tensor([[-0.0480,  0.0019,  0.0142,  ..., -0.0242, -0.0428,  0.0519],\n",
      "        [-0.0231, -0.0198, -0.0015,  ...,  0.0056,  0.0582,  0.0266],\n",
      "        [ 0.0550, -0.0012,  0.0310,  ...,  0.0603, -0.0260, -0.0112],\n",
      "        ...,\n",
      "        [ 0.0563,  0.0454,  0.0261,  ..., -0.0344,  0.0469,  0.0622],\n",
      "        [-0.0571, -0.0467,  0.0209,  ...,  0.0072,  0.0292, -0.0603],\n",
      "        [ 0.0429, -0.0390, -0.0039,  ...,  0.0108,  0.0363, -0.0162]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.22.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.22.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0351,  0.0223,  0.0361,  ..., -0.0059, -0.0473, -0.0113],\n",
      "        [ 0.0212,  0.0585, -0.0483,  ..., -0.0110,  0.0624, -0.0224],\n",
      "        [-0.0201, -0.0130,  0.0202,  ...,  0.0152,  0.0455,  0.0402],\n",
      "        ...,\n",
      "        [ 0.0406, -0.0118,  0.0338,  ..., -0.0021, -0.0014, -0.0262],\n",
      "        [ 0.0587,  0.0199, -0.0112,  ..., -0.0160,  0.0572,  0.0430],\n",
      "        [-0.0379,  0.0300, -0.0218,  ...,  0.0561,  0.0384, -0.0229]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.22.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.22.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.22.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('encoder.layer.23.attention.self.query.weight', Parameter containing:\n",
      "tensor([[-0.0344,  0.0238,  0.0925,  ..., -0.0897,  0.0875,  0.0668],\n",
      "        [-0.0428,  0.0215,  0.1016,  ..., -0.0060, -0.0135, -0.0045],\n",
      "        [ 0.0348,  0.0420,  0.0307,  ..., -0.0086, -0.0893, -0.0343],\n",
      "        ...,\n",
      "        [ 0.1002, -0.0528,  0.1249,  ..., -0.0049, -0.0109,  0.0663],\n",
      "        [-0.0479,  0.0012, -0.0022,  ..., -0.0515, -0.0171, -0.0155],\n",
      "        [-0.0127, -0.0463, -0.0592,  ...,  0.0462, -0.0668,  0.0146]])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.self.query.bias', Parameter containing:\n",
      "tensor([ 0.0780,  0.2722,  0.0301,  ..., -0.2014,  0.1132,  0.0771])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.self.key.weight', Parameter containing:\n",
      "tensor([[ 0.0584, -0.0312,  0.2179,  ..., -0.0163,  0.0306,  0.0215],\n",
      "        [ 0.0736, -0.0643,  0.0657,  ...,  0.0124, -0.0495,  0.0072],\n",
      "        [-0.1159, -0.0253, -0.0377,  ..., -0.1288,  0.0432, -0.0252],\n",
      "        ...,\n",
      "        [ 0.0532,  0.0073,  0.0698,  ..., -0.0457,  0.0011,  0.0541],\n",
      "        [ 0.0204, -0.0007, -0.0652,  ..., -0.0271,  0.0352,  0.0350],\n",
      "        [-0.0142, -0.0168, -0.0170,  ...,  0.1071,  0.0038,  0.0629]])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.self.key.bias', Parameter containing:\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  3.7611e-05,  ...,  6.2500e-02,\n",
      "         1.4722e-05, -1.9526e-04])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.self.value.weight', Parameter containing:\n",
      "tensor([[-0.0672, -0.0675, -0.0098,  ..., -0.0340,  0.0020,  0.0076],\n",
      "        [ 0.0228,  0.0076,  0.0388,  ..., -0.0288,  0.0032, -0.0462],\n",
      "        [-0.0406,  0.0640, -0.0136,  ...,  0.0078, -0.0466, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0501,  0.0296, -0.0295,  ..., -0.0133, -0.0237,  0.0161],\n",
      "        [ 0.0028, -0.0348,  0.0274,  ..., -0.0147, -0.0001,  0.0126],\n",
      "        [ 0.0114, -0.0212,  0.0245,  ..., -0.0004,  0.0173,  0.0146]])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.self.value.bias', Parameter containing:\n",
      "tensor([-0.0155,  0.0180,  0.0120,  ...,  0.0075,  0.0072,  0.0132])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.output.dense.weight', Parameter containing:\n",
      "tensor([[-0.0207, -0.0080, -0.0011,  ...,  0.0384, -0.0315,  0.0331],\n",
      "        [-0.0367, -0.0047,  0.0260,  ...,  0.0552, -0.0101, -0.0481],\n",
      "        [-0.0257,  0.0097,  0.0090,  ..., -0.0109,  0.0160, -0.0322],\n",
      "        ...,\n",
      "        [-0.0017, -0.0469, -0.0107,  ..., -0.0216, -0.0013, -0.0090],\n",
      "        [ 0.0173,  0.0432,  0.0164,  ...,  0.0112,  0.0047,  0.0042],\n",
      "        [ 0.0380, -0.0132, -0.0239,  ..., -0.0108,  0.0131,  0.0138]])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.output.dense.bias', Parameter containing:\n",
      "tensor([ 0.0640,  0.0726, -0.0046,  ...,  0.0315,  0.0211,  0.1617])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9619, 0.9805, 0.9937,  ..., 0.9810, 0.9917, 0.9644])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.attention.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.1349,  0.0312, -0.1586,  ..., -0.0664, -0.1432, -0.0922])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.intermediate.dense.weight', Parameter containing:\n",
      "tensor([[-0.0112, -0.0869, -0.0142,  ..., -0.0096,  0.0169, -0.0261],\n",
      "        [-0.0146, -0.0241, -0.0116,  ..., -0.0115,  0.0173, -0.0637],\n",
      "        [ 0.0008, -0.0923, -0.0033,  ...,  0.0326,  0.0343, -0.0215],\n",
      "        ...,\n",
      "        [-0.0409,  0.0268,  0.0119,  ..., -0.0029, -0.0405, -0.0081],\n",
      "        [ 0.0434,  0.0025, -0.0067,  ..., -0.0121,  0.0637,  0.0384],\n",
      "        [ 0.0348, -0.0151, -0.0295,  ...,  0.0524,  0.0290,  0.0393]])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.intermediate.dense.bias', Parameter containing:\n",
      "tensor([ 0.0085, -0.0684, -0.1255,  ..., -0.0403, -0.0952, -0.0114])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.output.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0229, -0.0396, -0.0189,  ...,  0.0182,  0.0236, -0.0075],\n",
      "        [ 0.0120,  0.0641, -0.0241,  ..., -0.0137, -0.0395,  0.0267],\n",
      "        [-0.0031, -0.0097,  0.0091,  ...,  0.0130, -0.0261, -0.0145],\n",
      "        ...,\n",
      "        [ 0.0150, -0.0355,  0.0850,  ...,  0.0314, -0.0135,  0.0417],\n",
      "        [ 0.0082, -0.0177,  0.0474,  ...,  0.0078,  0.0259, -0.0064],\n",
      "        [-0.0393,  0.0069,  0.0051,  ...,  0.0363,  0.0073, -0.0131]])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.output.dense.bias', Parameter containing:\n",
      "tensor([-0.0757, -0.0241, -0.0956,  ...,  0.0764, -0.1582,  0.0786])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.output.LayerNorm.weight', Parameter containing:\n",
      "tensor([0.9780, 0.9946, 0.9976,  ..., 0.9819, 1.0000, 0.9883])) weights are unchanged.\n",
      "Layer ('encoder.layer.23.output.LayerNorm.bias', Parameter containing:\n",
      "tensor([-0.0241, -0.0406, -0.0134,  ..., -0.0475,  0.0083, -0.0058])) weights are unchanged.\n",
      "Newly Added: encoder.layer.23.down_layer.weight Parameter containing:\n",
      "tensor([[ 0.0011, -0.0346,  0.0179,  ...,  0.0374, -0.0238,  0.0112],\n",
      "        [-0.0350, -0.0574,  0.0078,  ...,  0.0051, -0.0287,  0.0213],\n",
      "        [-0.0072, -0.0381,  0.0537,  ..., -0.0094, -0.0388,  0.0272],\n",
      "        ...,\n",
      "        [-0.0078, -0.0534, -0.0361,  ..., -0.0128, -0.0172,  0.0014],\n",
      "        [ 0.0119, -0.0560, -0.0267,  ..., -0.0097,  0.0625, -0.0041],\n",
      "        [ 0.0456, -0.0593,  0.0403,  ..., -0.0091,  0.0575,  0.0125]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.23.down_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.23.up_layer.weight Parameter containing:\n",
      "tensor([[-0.0536,  0.0309, -0.0611,  ..., -0.0549, -0.0370,  0.0280],\n",
      "        [ 0.0391, -0.0286,  0.0615,  ..., -0.0518, -0.0028, -0.0448],\n",
      "        [-0.0285,  0.0450, -0.0003,  ..., -0.0354, -0.0624, -0.0300],\n",
      "        ...,\n",
      "        [ 0.0410,  0.0192, -0.0208,  ...,  0.0465, -0.0504, -0.0146],\n",
      "        [-0.0241, -0.0318,  0.0617,  ...,  0.0611,  0.0110, -0.0339],\n",
      "        [ 0.0254,  0.0266,  0.0031,  ...,  0.0064, -0.0588, -0.0602]],\n",
      "       requires_grad=True)\n",
      "Newly Added: encoder.layer.23.up_layer.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Newly Added: encoder.layer.23.layer_norm.weight Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "Newly Added: encoder.layer.23.layer_norm.bias Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Layer ('pooler.dense.weight', Parameter containing:\n",
      "tensor([[ 0.0486, -0.0293,  0.0037,  ..., -0.0056, -0.0124, -0.0057],\n",
      "        [-0.0208,  0.0317,  0.0529,  ..., -0.0154, -0.0035, -0.0085],\n",
      "        [ 0.0276, -0.0152,  0.0252,  ...,  0.0219, -0.0103,  0.0315],\n",
      "        ...,\n",
      "        [ 0.0027,  0.0056, -0.0050,  ..., -0.0413, -0.0195, -0.0189],\n",
      "        [ 0.0007,  0.0088,  0.0242,  ..., -0.0043, -0.0285,  0.0232],\n",
      "        [-0.0258,  0.0251, -0.0135,  ..., -0.0104,  0.0105, -0.0127]])) weights are unchanged.\n",
      "Layer ('pooler.dense.bias', Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])) weights are unchanged.\n",
      "Newly Added: classifier.0.weight Parameter containing:\n",
      "tensor([[-0.0124,  0.0309,  0.0218,  ..., -0.0037, -0.0181,  0.0265],\n",
      "        [ 0.0154,  0.0165, -0.0191,  ...,  0.0269,  0.0026, -0.0236],\n",
      "        [ 0.0274, -0.0298,  0.0115,  ...,  0.0260, -0.0030,  0.0038],\n",
      "        ...,\n",
      "        [ 0.0089,  0.0302,  0.0260,  ...,  0.0218, -0.0049,  0.0243],\n",
      "        [-0.0070, -0.0209, -0.0085,  ...,  0.0310,  0.0197,  0.0228],\n",
      "        [-0.0232,  0.0190,  0.0100,  ..., -0.0037, -0.0091,  0.0246]],\n",
      "       requires_grad=True)\n",
      "Newly Added: classifier.0.bias Parameter containing:\n",
      "tensor([ 0.0102, -0.0259, -0.0084,  ..., -0.0154,  0.0282,  0.0282],\n",
      "       requires_grad=True)\n",
      "Newly Added: classifier.2.weight Parameter containing:\n",
      "tensor([[-0.0071, -0.0019, -0.0125,  ..., -0.0142,  0.0283,  0.0124],\n",
      "        [ 0.0086,  0.0041, -0.0045,  ...,  0.0121,  0.0101, -0.0031]],\n",
      "       requires_grad=True)\n",
      "Newly Added: classifier.2.bias Parameter containing:\n",
      "tensor([ 0.0308, -0.0158], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# Load configuration\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomRobertaModel(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_model = RobertaModel.from_pretrained(model_name)\n",
    "original_weights = pretrained_model.state_dict()\n",
    "\n",
    "custom_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear) and getattr(module, \"_is_new\", False):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "custom_model.apply(initialize_weights)\n",
    "\n",
    "# Compare weights\n",
    "for name, param in custom_model.named_parameters():\n",
    "    if name in original_weights:\n",
    "        if not torch.equal(param, original_weights[name]):\n",
    "            print(f\"Layer {name} weights were modified.\")\n",
    "        else:\n",
    "            print(f\"Layer {name, param} weights are unchanged.\")\n",
    "    else:\n",
    "        print(\"Newly Added:\", name, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1297337-4c18-4b46-b825-842e5730f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Evaluate periodically during training\n",
    "    #eval_steps=100,               # Frequency of evaluation (adjust as needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training for GPU\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    ")\n",
    "\n",
    "# Train base model\n",
    "trainer_base = Trainer(\n",
    "    model=custom_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a0201b-f123-4f38-a933-3cb8a1049b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Custom Adapter Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1895' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1895/4689 1:25:24 < 2:06:03, 0.37 it/s, Epoch 1.21/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695700</td>\n",
       "      <td>0.694092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Custom Adapter Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Resize model embeddings after adding new special tokens\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# custom_model.resize_token_embeddings(len(tokenizer))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "custom_model.to(device)\n",
    "print(\"\\nTraining Custom Adapter Model...\")\n",
    "# Resize model embeddings after adding new special tokens\n",
    "# custom_model.resize_token_embeddings(len(tokenizer))\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd4db22a-cadc-4172-9e9b-1de52f9be2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06891806088924982"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26303490/381663234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea296e78-5e51-4a77-ab05-5addf8c1e5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable Parameters in Base Model:\n",
      "Total Parameters: 381663234\n",
      "Trainable Parameters: 26303490\n",
      "  - encoder.layer.0.down_layer.weight: 524288 params\n",
      "  - encoder.layer.0.down_layer.bias: 512 params\n",
      "  - encoder.layer.0.up_layer.weight: 524288 params\n",
      "  - encoder.layer.0.up_layer.bias: 1024 params\n",
      "  - encoder.layer.0.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.0.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.1.down_layer.weight: 524288 params\n",
      "  - encoder.layer.1.down_layer.bias: 512 params\n",
      "  - encoder.layer.1.up_layer.weight: 524288 params\n",
      "  - encoder.layer.1.up_layer.bias: 1024 params\n",
      "  - encoder.layer.1.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.1.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.2.down_layer.weight: 524288 params\n",
      "  - encoder.layer.2.down_layer.bias: 512 params\n",
      "  - encoder.layer.2.up_layer.weight: 524288 params\n",
      "  - encoder.layer.2.up_layer.bias: 1024 params\n",
      "  - encoder.layer.2.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.2.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.3.down_layer.weight: 524288 params\n",
      "  - encoder.layer.3.down_layer.bias: 512 params\n",
      "  - encoder.layer.3.up_layer.weight: 524288 params\n",
      "  - encoder.layer.3.up_layer.bias: 1024 params\n",
      "  - encoder.layer.3.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.3.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.4.down_layer.weight: 524288 params\n",
      "  - encoder.layer.4.down_layer.bias: 512 params\n",
      "  - encoder.layer.4.up_layer.weight: 524288 params\n",
      "  - encoder.layer.4.up_layer.bias: 1024 params\n",
      "  - encoder.layer.4.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.4.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.5.down_layer.weight: 524288 params\n",
      "  - encoder.layer.5.down_layer.bias: 512 params\n",
      "  - encoder.layer.5.up_layer.weight: 524288 params\n",
      "  - encoder.layer.5.up_layer.bias: 1024 params\n",
      "  - encoder.layer.5.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.5.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.6.down_layer.weight: 524288 params\n",
      "  - encoder.layer.6.down_layer.bias: 512 params\n",
      "  - encoder.layer.6.up_layer.weight: 524288 params\n",
      "  - encoder.layer.6.up_layer.bias: 1024 params\n",
      "  - encoder.layer.6.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.6.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.7.down_layer.weight: 524288 params\n",
      "  - encoder.layer.7.down_layer.bias: 512 params\n",
      "  - encoder.layer.7.up_layer.weight: 524288 params\n",
      "  - encoder.layer.7.up_layer.bias: 1024 params\n",
      "  - encoder.layer.7.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.7.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.8.down_layer.weight: 524288 params\n",
      "  - encoder.layer.8.down_layer.bias: 512 params\n",
      "  - encoder.layer.8.up_layer.weight: 524288 params\n",
      "  - encoder.layer.8.up_layer.bias: 1024 params\n",
      "  - encoder.layer.8.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.8.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.9.down_layer.weight: 524288 params\n",
      "  - encoder.layer.9.down_layer.bias: 512 params\n",
      "  - encoder.layer.9.up_layer.weight: 524288 params\n",
      "  - encoder.layer.9.up_layer.bias: 1024 params\n",
      "  - encoder.layer.9.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.9.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.10.down_layer.weight: 524288 params\n",
      "  - encoder.layer.10.down_layer.bias: 512 params\n",
      "  - encoder.layer.10.up_layer.weight: 524288 params\n",
      "  - encoder.layer.10.up_layer.bias: 1024 params\n",
      "  - encoder.layer.10.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.10.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.11.down_layer.weight: 524288 params\n",
      "  - encoder.layer.11.down_layer.bias: 512 params\n",
      "  - encoder.layer.11.up_layer.weight: 524288 params\n",
      "  - encoder.layer.11.up_layer.bias: 1024 params\n",
      "  - encoder.layer.11.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.11.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.12.down_layer.weight: 524288 params\n",
      "  - encoder.layer.12.down_layer.bias: 512 params\n",
      "  - encoder.layer.12.up_layer.weight: 524288 params\n",
      "  - encoder.layer.12.up_layer.bias: 1024 params\n",
      "  - encoder.layer.12.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.12.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.13.down_layer.weight: 524288 params\n",
      "  - encoder.layer.13.down_layer.bias: 512 params\n",
      "  - encoder.layer.13.up_layer.weight: 524288 params\n",
      "  - encoder.layer.13.up_layer.bias: 1024 params\n",
      "  - encoder.layer.13.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.13.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.14.down_layer.weight: 524288 params\n",
      "  - encoder.layer.14.down_layer.bias: 512 params\n",
      "  - encoder.layer.14.up_layer.weight: 524288 params\n",
      "  - encoder.layer.14.up_layer.bias: 1024 params\n",
      "  - encoder.layer.14.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.14.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.15.down_layer.weight: 524288 params\n",
      "  - encoder.layer.15.down_layer.bias: 512 params\n",
      "  - encoder.layer.15.up_layer.weight: 524288 params\n",
      "  - encoder.layer.15.up_layer.bias: 1024 params\n",
      "  - encoder.layer.15.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.15.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.16.down_layer.weight: 524288 params\n",
      "  - encoder.layer.16.down_layer.bias: 512 params\n",
      "  - encoder.layer.16.up_layer.weight: 524288 params\n",
      "  - encoder.layer.16.up_layer.bias: 1024 params\n",
      "  - encoder.layer.16.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.16.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.17.down_layer.weight: 524288 params\n",
      "  - encoder.layer.17.down_layer.bias: 512 params\n",
      "  - encoder.layer.17.up_layer.weight: 524288 params\n",
      "  - encoder.layer.17.up_layer.bias: 1024 params\n",
      "  - encoder.layer.17.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.17.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.18.down_layer.weight: 524288 params\n",
      "  - encoder.layer.18.down_layer.bias: 512 params\n",
      "  - encoder.layer.18.up_layer.weight: 524288 params\n",
      "  - encoder.layer.18.up_layer.bias: 1024 params\n",
      "  - encoder.layer.18.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.18.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.19.down_layer.weight: 524288 params\n",
      "  - encoder.layer.19.down_layer.bias: 512 params\n",
      "  - encoder.layer.19.up_layer.weight: 524288 params\n",
      "  - encoder.layer.19.up_layer.bias: 1024 params\n",
      "  - encoder.layer.19.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.19.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.20.down_layer.weight: 524288 params\n",
      "  - encoder.layer.20.down_layer.bias: 512 params\n",
      "  - encoder.layer.20.up_layer.weight: 524288 params\n",
      "  - encoder.layer.20.up_layer.bias: 1024 params\n",
      "  - encoder.layer.20.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.20.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.21.down_layer.weight: 524288 params\n",
      "  - encoder.layer.21.down_layer.bias: 512 params\n",
      "  - encoder.layer.21.up_layer.weight: 524288 params\n",
      "  - encoder.layer.21.up_layer.bias: 1024 params\n",
      "  - encoder.layer.21.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.21.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.22.down_layer.weight: 524288 params\n",
      "  - encoder.layer.22.down_layer.bias: 512 params\n",
      "  - encoder.layer.22.up_layer.weight: 524288 params\n",
      "  - encoder.layer.22.up_layer.bias: 1024 params\n",
      "  - encoder.layer.22.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.22.layer_norm.bias: 1024 params\n",
      "  - encoder.layer.23.down_layer.weight: 524288 params\n",
      "  - encoder.layer.23.down_layer.bias: 512 params\n",
      "  - encoder.layer.23.up_layer.weight: 524288 params\n",
      "  - encoder.layer.23.up_layer.bias: 1024 params\n",
      "  - encoder.layer.23.layer_norm.weight: 1024 params\n",
      "  - encoder.layer.23.layer_norm.bias: 1024 params\n",
      "  - classifier.0.weight: 1048576 params\n",
      "  - classifier.0.bias: 1024 params\n",
      "  - classifier.2.weight: 2048 params\n",
      "  - classifier.2.bias: 2 params\n"
     ]
    }
   ],
   "source": [
    "print_trainable_params(custom_model, stage_name=\"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad630c-6c8b-49c0-b3be-9bd460c1639f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat_nlp",
   "language": "python",
   "name": "stat_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
