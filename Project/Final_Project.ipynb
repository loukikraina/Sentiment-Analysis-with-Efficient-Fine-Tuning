{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db05b641-b477-4788-afc8-677dafd7b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Making sure it downloads models on my D drive, as no space in defualt file location\n",
    "os.environ['HF_HOME'] = 'D:\\\\Download\\\\UCSD\\\\cache'\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_scheduler, EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaModel,\n",
    "    RobertaEncoder,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be59cec3-cf96-475a-bef8-489986d5d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Using Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9578774-1faf-4122-84bb-d47e6b8957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directories\n",
    "BASE_MODEL_DIR = \"./base_model\"\n",
    "LORA_MODEL_DIR = \"./lora_model\"\n",
    "ADAPTER_MODEL_DIR = \"./adapter_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1a869-bf1a-486d-ba4e-4b2492b524e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base train output dir\n",
    "TRAIN_OUTPUT_DIR = \"./train_output\"\n",
    "def output_file_name(model_name):\n",
    "    return f'{TRAIN_OUTPUT_DIR}/training_results_{model_name}.json'\n",
    "# Base directory names\n",
    "from datetime import datetime\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_results_dir = f\"./results/base\" #_{timestamp}\"\n",
    "lora_results_dir = f\"./results/lora\" #_{timestamp}\"\n",
    "adapter_results_dir = f\"./results/adapter\" #_{timestamp}\"\n",
    "\n",
    "# Ensure directories exist\n",
    "Path(base_results_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(lora_results_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(adapter_results_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15155e3-8309-48c4-a9d9-e5469c361a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import TrainerCallback\n",
    "import os\n",
    "\n",
    "class CSVLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, log_file=\"./logs/training_log.csv\"):\n",
    "        self.log_file = log_file\n",
    "        if not os.path.exists(log_file):\n",
    "            # Initialize the CSV with headers if it doesn't exist\n",
    "            with open(log_file, \"w\") as f:\n",
    "                f.write(\"step,loss,lr,gradient_norm\\n\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, model=None, **kwargs):\n",
    "        if logs is not None and model is not None:\n",
    "            step = state.global_step\n",
    "            loss = logs.get(\"loss\", None)\n",
    "            lr = logs.get(\"learning_rate\", None)\n",
    "\n",
    "            # Calculate the gradient norm\n",
    "            gradient_norm = logs.get(\"grad_norm\", None)\n",
    "\n",
    "            # Append to CSV\n",
    "            if loss is not None:\n",
    "                with open(self.log_file, \"a\") as f:\n",
    "                    f.write(f\"{step},{loss},{lr},{gradient_norm}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c65a20-11f5-41cf-9f9e-029968b31ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model, stage_name=\"Model\"):\n",
    "    print(f\"\\nTrainable Parameters in {stage_name}:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    print(f\"%\\\\age of trainable params: {(trainable_params/total_params) * 100}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  - {name}: {param.numel()} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b00c47-e79c-44d0-af3b-9145d50c986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of epochs for all models\n",
    "num_epochs = 7\n",
    "# Base TrainingArguments configuration\n",
    "base_args = {\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"num_train_epochs\": num_epochs,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_steps\": 100,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"fp16\": True,\n",
    "    \"report_to\": \"none\",\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1967b-a7a8-4134-bf67-9cfa1ad40b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create dynamic result directory\n",
    "def create_training_args(output_dir, lr):\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        learning_rate = lr,\n",
    "        **base_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6786e0aa-af95-4583-afe0-6aef67376e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Llama 1B and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # Using LLama 1B as base model\n",
    "\n",
    "# Couldn't train Llama because of lower mem GPUs so shifting to roberta\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "\n",
    "# Step 1: Load or initialize tokenizer\n",
    "if os.path.exists(BASE_MODEL_DIR):\n",
    "    print(\"\\nTokenizer already exists. Loading from base model directory...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
    "else:\n",
    "    print(\"\\nInitializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Ensure tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc5539-0614-4bb4-864a-2225573c8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)  # Use full training dataset\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)    # Use full testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068915-0dc2-404e-b8a2-6cb8ff75c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, training_args, name):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(f\"\\nEvaluating {name} Model...\")\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"{name} Model Results:\", results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a59e493e-5941-4a6c-b968-6db0a2ba5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train or load the base model\n",
    "base_training_args = create_training_args(output_dir=base_results_dir, lr=2e-5)\n",
    "\n",
    "if os.path.exists(BASE_MODEL_DIR):\n",
    "    print(\"\\nBase model already exists. Loading base model...\")\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_DIR)\n",
    "else:\n",
    "    print(\"\\nTraining Base Model...\")\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    base_model.config.pad_token_id = base_model.config.eos_token_id\n",
    "    trainer_base = Trainer(\n",
    "        model=base_model,\n",
    "        args=base_training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), CSVLoggerCallback(\"./logs/training_log_base.csv\")],\n",
    "    )\n",
    "    print_trainable_params(base_model, stage_name=\"Base Model\")\n",
    "    base_model.to(device)\n",
    "    start_time = time.time()\n",
    "    train_output = trainer_base.train()\n",
    "    print(f\"Base Model training time: {time.time() - start_time}s\")\n",
    "    base_model.save_pretrained(BASE_MODEL_DIR)\n",
    "    tokenizer.save_pretrained(BASE_MODEL_DIR)\n",
    "    output_file = output_file_name('base')\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(train_output.metrics, f, indent=4)\n",
    "    print(\"\\nBase model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99124809-bb4f-4c16-8f0b-2d3d2eab2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train or load the LoRA model\n",
    "lora_training_args = create_training_args(output_dir=lora_results_dir, lr=1e-4)\n",
    "if os.path.exists(LORA_MODEL_DIR):\n",
    "    print(\"\\nLoRA model already exists. Loading LoRA model...\")\n",
    "    lora_model = AutoModelForSequenceClassification.from_pretrained(LORA_MODEL_DIR)\n",
    "else:\n",
    "    print(\"\\nTraining LoRA Model...\")\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        #target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\", \n",
    "        inference_mode=False,\n",
    "    )\n",
    "    # Apply LoRA to model\n",
    "    base_model_lora = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    lora_model = get_peft_model(base_model_lora, lora_config).to(device)\n",
    "    trainer_lora = Trainer(\n",
    "        model=lora_model,\n",
    "        args=lora_training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), CSVLoggerCallback(\"./logs/training_log_lora.csv\")],\n",
    "    )\n",
    "    print_trainable_params(lora_model, stage_name=\"LoRA Model\")\n",
    "    start_time = time.time()\n",
    "    train_output = trainer_lora.train()\n",
    "    print(f\"LoRA model training time: {time.time() - start_time}s\")\n",
    "    lora_model.save_pretrained(LORA_MODEL_DIR)\n",
    "    tokenizer.save_pretrained(LORA_MODEL_DIR)\n",
    "    output_file = output_file_name('lora')\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(train_output.metrics, f, indent=4)\n",
    "    print(\"\\nLoRA model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c624b-1100-4420-a0c3-b4b7eea258a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaModel,\n",
    "    RobertaEncoder,\n",
    "    RobertaLayer,\n",
    "    RobertaEmbeddings,\n",
    "    RobertaConfig,\n",
    ")\n",
    "\n",
    "class CustomRobertaLayer(RobertaLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.down_layer = nn.Linear(config.hidden_size, config.hidden_size // 2)  # Down-projection\n",
    "        self.up_layer = nn.Linear(config.hidden_size // 2, config.hidden_size)    # Up-projection\n",
    "        self.activation = nn.ReLU()                                              # Activation function\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)                       # LayerNorm after Up-projection\n",
    "        # intializing all as new layers\n",
    "        self.down_layer._is_new = True\n",
    "        self.up_layer._is_new = True\n",
    "        self.activation._is_new = True\n",
    "        self.layer_norm._is_new = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Ensure the attention mask matches the required dimensions\n",
    "        if attention_mask is not None:\n",
    "            # Expand dimensions for multi-head attention\n",
    "            attention_mask = attention_mask[:, None, None, :]  # Shape: [batch_size, 1, 1, seq_len]\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # Match precision (e.g., float16)\n",
    "\n",
    "\n",
    "        # Attention sub-layer\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output = attention_outputs[0]\n",
    "\n",
    "        # Feed-forward sub-layer\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(hidden_states=intermediate_output, input_tensor=attention_output)\n",
    "\n",
    "        # Down-projection, activation, up-projection, and LayerNorm\n",
    "        down_projected = self.activation(self.down_layer(layer_output))\n",
    "        up_projected = self.activation(self.up_layer(down_projected))\n",
    "        norm_output = self.layer_norm(up_projected + layer_output)  # Residual connection\n",
    "\n",
    "        return (norm_output,) + attention_outputs[1:]  # Return outputs\n",
    "\n",
    "# Custom Encoder\n",
    "class CustomRobertaEncoder(RobertaEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([CustomRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "# Custom Model\n",
    "class CustomRobertaModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the encoder with the custom encoder\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = CustomRobertaEncoder(config)\n",
    "\n",
    "        # Add the classification head at the end\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, config.num_labels),\n",
    "        )\n",
    "        self.classifier._is_new = True\n",
    "\n",
    "        # Freeze existing layers if needed\n",
    "        self.freeze_pretrained_layers()\n",
    "\n",
    "    def freeze_pretrained_layers(self):\n",
    "        # Freeze all layers except the classifier and custom layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"classifier\" in name or \"down_layer\" in name or \"up_layer\" in name or \"layer_norm\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # Embedding layer\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        # Extract the [CLS] token representation\n",
    "        cls_token_output = sequence_output[:, 0, :]\n",
    "\n",
    "       # Classification head\n",
    "        logits = self.classifier(cls_token_output)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Return loss if available, otherwise logits\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# Initialize new weights\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear) and getattr(module, \"_is_new\", False):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19177c-1d6e-48b6-8abc-d841eed169d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train or load the Adapter model\n",
    "adapter_training_args = create_training_args(output_dir=adapter_results_dir, lr=2e-5)\n",
    "if os.path.exists(ADAPTER_MODEL_DIR):\n",
    "    print(\"\\nAdapter model already exists. Loading Adapter model...\")\n",
    "    adapter_model = RobertaModel.from_pretrained(ADAPTER_MODEL_DIR)\n",
    "else:\n",
    "    print(\"\\nTraining Adapter Model...\")\n",
    "    config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # Create the custom model\n",
    "    adapter_model = CustomRobertaModel(config)\n",
    "\n",
    "    # Load pretrained weights\n",
    "    pretrained_model = RobertaModel.from_pretrained(model_name)\n",
    "    adapter_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "    adapter_model.apply(initialize_weights)\n",
    "    \n",
    "    # Optimizer with layer-wise learning rate decay\n",
    "    # optimizer_grouped_parameters = [\n",
    "    #     {\"params\": [p for n, p in adapter_model.named_parameters() if any(keyword in n for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm\",])], \"lr\": 1e-4},\n",
    "    #     {\"params\": [p for n, p in adapter_model.named_parameters() if all(keyword not in n for keyword in [\"classifier\", \"down_layer\", \"up_layer\", \"layer_norm\",])], \"lr\": 2e-5},\n",
    "    # ]\n",
    "    \n",
    "    # optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "    # # Scheduler\n",
    "    # num_training_steps = len(train_dataset) // 16 * num_epochs  # Example for 3 epochs\n",
    "    # scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "\n",
    "    \n",
    "    trainer_adapter = Trainer(\n",
    "        model=adapter_model,\n",
    "        args=adapter_training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3), CSVLoggerCallback(\"./logs/training_log_adapter.csv\")],\n",
    "    )\n",
    "    \n",
    "    print_trainable_params(adapter_model, stage_name=\"Adapter Model\")\n",
    "    start_time = time.time()\n",
    "    train_output = trainer_adapter.train()\n",
    "    print(f\"Adapter model training time: {time.time() - start_time}s\")\n",
    "    adapter_model.save_pretrained(ADAPTER_MODEL_DIR)\n",
    "    tokenizer.save_pretrained(ADAPTER_MODEL_DIR)\n",
    "    output_file = output_file_name('adapter')\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(train_output.metrics, f, indent=4)\n",
    "    print(\"\\nAdapter model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738b4c-5eac-4474-b8a7-f1059757b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate all models\n",
    "print(\"\\nEvaluating all models...\")\n",
    "base_results = evaluate_model(base_model, base_training_args, \"Base\"\n",
    "                              )\n",
    "lora_results = evaluate_model(lora_model, lora_training_args, \"LoRA\")\n",
    "adapter_results = evaluate_model(adapter_model, adapter_training_args, \"Adapter\")\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(\"Base Model:\", base_results)\n",
    "print(\"LoRA Model:\", lora_results)\n",
    "print(\"Adapter Model:\", adapter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5a3fa-6bb6-4a2f-ae6b-94e67912053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "\n",
    "def evaluate_model(trainer, model, name, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the given model and compute various metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {name} Model...\")\n",
    "    # Standard evaluation using Trainer\n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    # Extract predictions and labels\n",
    "    predictions, labels, _ = trainer.predict(test_dataset)\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute additional metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_recall_metric = evaluate.load(\"precision\", \"recall\", \"f1\")\n",
    "\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    precision_recall_f1 = precision_recall_metric.compute(predictions=preds, references=labels)\n",
    "    confusion_mat = confusion_matrix(labels, preds)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision_recall_f1['precision']:.4f}\")\n",
    "    print(f\"Recall: {precision_recall_f1['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {precision_recall_f1['f1']:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_mat)\n",
    "    \n",
    "    # Visualize confusion matrix\n",
    "    visualize_confusion_matrix(confusion_mat, name)\n",
    "    \n",
    "    # Add results to a dictionary\n",
    "    metrics = {\n",
    "        \"eval_loss\": results[\"eval_loss\"],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision_recall_f1[\"precision\"],\n",
    "        \"recall\": precision_recall_f1[\"recall\"],\n",
    "        \"f1_score\": precision_recall_f1[\"f1\"],\n",
    "        \"confusion_matrix\": confusion_mat\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_confusion_matrix(confusion_mat, model_name):\n",
    "    \"\"\"\n",
    "    Visualize the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name} Model\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "def calculate_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculate total and trainable parameters of a model.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Main comparison code\n",
    "def compare_models(base_model, lora_model, adapter_model, training_args_list, test_dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Compare Base, LoRA, and Adapter models on various metrics.\n",
    "    \"\"\"\n",
    "    metrics_summary = {}\n",
    "\n",
    "    # Evaluate Base Model\n",
    "    base_trainer = Trainer(\n",
    "        model=base_model,\n",
    "        args=training_args_list[0],\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    base_results = evaluate_model(base_trainer, base_model, \"Base\", test_dataset)\n",
    "    metrics_summary[\"Base\"] = base_results\n",
    "\n",
    "    # Evaluate LoRA Model\n",
    "    lora_trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args_list[1],\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    lora_results = evaluate_model(lora_trainer, lora_model, \"LoRA\", test_dataset)\n",
    "    metrics_summary[\"LoRA\"] = lora_results\n",
    "\n",
    "    # Evaluate Adapter Model\n",
    "    adapter_trainer = Trainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args_list[2],\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    adapter_results = evaluate_model(adapter_trainer, adapter_model, \"Adapter\", test_dataset)\n",
    "    metrics_summary[\"Adapter\"] = adapter_results\n",
    "\n",
    "    # Parameter size comparison\n",
    "    print(\"\\nParameter Size Comparison:\")\n",
    "    for name, model in zip([\"Base\", \"LoRA\", \"Adapter\"], [base_model, lora_model, adapter_model]):\n",
    "        total_params, trainable_params = calculate_model_size(model)\n",
    "        print(f\"{name} Model - Total Params: {total_params:,}, Trainable Params: {trainable_params:,}\")\n",
    "\n",
    "    # Summarize all results\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    for model_name, metrics in metrics_summary.items():\n",
    "        print(f\"{model_name} Model:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name != \"confusion_matrix\":\n",
    "                print(f\"  {metric_name}: {value}\")\n",
    "        print()  # Blank line for readability\n",
    "    \n",
    "    return metrics_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13dd039-ef10-4813-86bf-3ae35c7a2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_list = [base_training_args, lora_training_args, adapter_training_args]\n",
    "\n",
    "# Calculating metrics here for overall comparison\n",
    "metrics = compare_models(base_model, lora_model, adapter_model, training_args_list, test_dataset, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e38754b-57ab-48ec-8ddf-58af4d37ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\stat_nlp\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV logs for all three models\n",
    "log_model_1 = pd.read_csv('./logs/training_log_base.csv')\n",
    "log_model_2 = pd.read_csv('./logs/training_log_lora.csv')\n",
    "log_model_3 = pd.read_csv('./logs/training_log_adapter.csv')\n",
    "\n",
    "# Plot training loss and gradient norm for all models\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Training Loss Plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(log_model_1['step'], log_model_1['loss'], label=\"Base model Loss\")\n",
    "plt.plot(log_model_2['step'], log_model_2['loss'], label=\"LoRA model Loss\")\n",
    "plt.plot(log_model_3['step'], log_model_3['loss'], label=\"Adapter model Loss\")\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Progression')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Gradient Norm Plot\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(log_model_1['step'], log_model_1['gradient_norm'], label=\"Base model Gradient Norm\", linestyle='dashed')\n",
    "plt.plot(log_model_2['step'], log_model_2['gradient_norm'], label=\"LoRA model Gradient Norm\", linestyle='dotted')\n",
    "plt.plot(log_model_3['step'], log_model_3['gradient_norm'], label=\"Adapter model Gradient Norm\", linestyle='dashdot')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Gradient Norm Progression')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat_nlp",
   "language": "python",
   "name": "stat_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
